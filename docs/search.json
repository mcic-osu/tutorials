[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MCIC Bioinformatics Tutorials",
    "section": "",
    "text": "Welcome to the MCIC bioinformatics tutorials website!\nUse the top navigation bars to browse through the available tutorials.\n\n\n\n Back to top"
  },
  {
    "objectID": "git/02_git1.html#an-introduction-to-version-control",
    "href": "git/02_git1.html#an-introduction-to-version-control",
    "title": "Getting started with Git",
    "section": "1 An introduction to version control",
    "text": "1 An introduction to version control\n\n1.1 Why use a Version Control System (VCS)?\nHere are some “versioning”- and backup-related challenges for your research project files that you may run into when not using a formal Version Control System (VCS):\n\nWhat to save periodic copies of?\n\nDo you only save versions of individual files?\nSpace-efficient, but doesn’t allow you to go back to the state of other project files at the same point in time.\nDo you save a copy of the full project periodically?\nBetter than the above option, but can become prohibitive in terms of disk storage.\n\nHow to know what changes were made between saved versions?\nHow to collaborate, especially when working simultaneously?\nHow to restore an accidentally deleted, modified, or overwritten file? This can especially be an issue at OSC where there is no recycle bin or undo button.\nHow to manage simultaneous variants of files, such as when making experimental changes?\n\nA formal VCS can help you with these challenges. With a VCS:\n\nYou can easily see your history of changes.\nYou have a time machine: you can go back to past states of your project (and not just of individual files!).\nYou can do simultaneous collaborative work — you can always track down who made which changes.\nSharing your code and other aspects of your project is easy.\nYou can make experimental changes without affecting current functionality.\n\n\nOr, as the CSB book puts it:\n\nVersion control is a way to keep your scientific projects tidily organized, collaborate on science, and have the whole history of each project at your fingertips.\n— CSB Chapter 2\n\n\n\n\n1.2 How Git roughly works\nGit is the most widely used Version Control System1. With Git, you save “snapshots” of your project with every minor piece of progress. Git manages this cleverly without having to create full copies of the project for every snapshot:\n\n\n\nThe boxes with dashed lines depict files that have not changed: these will not be saved repeatedly.Figure from https://git-scm.com.\n\n\n\nAs illustrated above, files that haven’t changed between snapshots are not saved again and again with every snapshot. But Git doesn’t even save full copies of files that have changed: it tracks changes on a line-by-line basis, and saves changed lines (!).\nNote that one Git database (repository) manages files inside a single directory structure, so to use Git, it’s important that your projects are properly organized or at least kept in separate dirs, as discussed in this tutorial.\n\n\nKey Git term 1: Repository (repo)\nA Git “repository” (or “repo”) is the version-control database for a project. Note that:\n\nYou can start a Git repository in any dir on your computer.\nThe Git database is saved in a hidden dir .git in the dir in which you started the repo.\nIt is typical (& recommended) that you should have one Git repository for each research project.\nYou can also download any public online Git repository.\n\n\n\n\n\n\n\nHidden files and dirs\n\n\n\nWhen a file or dir name has a leading ., it’s “hidden”. These don’t show up in file browsers by default, nor in ls file listings unless you use the -a (“all”) option. Hidden files and dirs are often generated automatically by software.\n\n\n\n\nKey Git term 2: Commit\nA Git “commit” is a saved snapshot of the project. For now, note that:\n\nIt is always possible to go the exact state of the entire project or individual files for any commit.\nWhenever you create a commit, you also include a message describing the changes you made.\n\n\n\n\n\n1.3 What do I put under version control?\nThe primary files to put under version control are:\n\nScripts2.\nProject documentation files.\nMetadata.\nManuscripts, if you write them in a plain text format.\n\nWhat about data and results?\n\nRaw data may or may not be included — for omics data, this is generally not feasible due to large file sizes.\nResults from analyses should generally not be included.\n\n\n\nSource versus derived files\nThe general idea behind what you should and should not include is that you should version-control the source, but not derived files. For instance:\n\nVersion-control your Markdown file, not the HTML it produces.\nVersion-control your script, not the output it produces.\n\n\n\n\n\n\n\nDerived files\n\n\n\nRecall that results and other derived files are (or should be) dispensable, because they can be regenerated using the raw data and the scripts.\n\n\n\n\n\nFile limitations\nThere are some limitations to the types and sizes of files that can be committed with Git:\n\nFile type: binary (non-text) files, such a Word or Excel files, or compiled software, can be included but can’t be tracked in quite the same way as plain-text files3.\nRepository size: for performance reasons, it’s best to keep individual repositories under about 1 GB.\nFile size: while you can have them in your Git repo, GitHub will not allow you to upload files &gt;100 MB.\n\nAs such, omics data is usually too large to be version-controlled. To make your data available to others, you can use dedicated repositories like the NCBI’s Sequence Read Archive (SRA).\n\n\n\n\n1.4 User Interfaces for Git\n\n\n\nBy xkcd\n\n\nYou can work with Git in several different ways — using:\n\nThe native command-line interface (CLI).\nThird-party graphical user interfaces (GUIs) such as Git Kraken.\nIDEs/editors with Git integration like RStudio and VS Code.\n\nIn this course, we will mainly focus on the CLI because it’s the most universal and powerful interface. But it’s absolutely fine to switch to GUI usage later, which will not be hard if you’ve learned the basics with the CLI.\nGit takes some getting used to, regardless of the interface. Many people have one or more “false starts” with it. I hope that being “forced” to use it in a course4 will take you past that!"
  },
  {
    "objectID": "git/02_git1.html#the-basic-git-workflow",
    "href": "git/02_git1.html#the-basic-git-workflow",
    "title": "Getting started with Git",
    "section": "2 The basic Git workflow",
    "text": "2 The basic Git workflow\nGit commands always start with git followed by a second command/subcommand or “verb”: git add, git commit, etc. Only three commands tend to make up the vast majority of your Git work:\n\ngit add does two things:\n\n\nStart “tracking” files (i.e., files in your directory structure are not automatically included in the repo).\nMark changed/new files as ready to be committed, which is called “staging” files.\n\ngit commit\nCreate a new snapshot of the project by commiting all currently staged files (changes).\ngit status\nGet the status of your repo: which files have changed, which new files are present, tips on next steps, etc.\n\n\n\n\nAdding and committing changes with Git commands.The Git database, which is in a hidden folder .git, is depicted with a gray background.\n\n\n\n\n\n\nAnother way of visualizing the adding and committing of changes in Git.Note that git add has a dual function: it starts tracking files and stages them."
  },
  {
    "objectID": "git/02_git1.html#getting-set-up",
    "href": "git/02_git1.html#getting-set-up",
    "title": "Getting started with Git",
    "section": "3 Getting set up",
    "text": "3 Getting set up\nWe will start with loading Git at OSC5 and then do some one-time personal Git configuration:\n\nLaunch VS Code at https://ondemand.osc.edu as before, at the dir /fs/ess/PAS2700/users/$USER, and open a terminal in VS Code.\n“Load” the most recent version of Git that is available at OSC with the module load command6:\nmodule load git/2.39.0\nUse git config to make your (actual, not user) name known to Git:\ngit config --global user.name 'John Doe'\nUse git config to make your email address known (use the same email address you signed up for GitHub with):\ngit config --global user.email 'doe.391@osu.edu'\nUse git config to set a default text editor for Git.\n(Occasionally7, Git will open up a text editor for you. Even though we’re using VS Code, here it is better to select a text editor that runs directly in the shell, like nano.)\ngit config --global core.editor \"nano -w\"\nActivate Git output with colors:\ngit config --global color.ui true\nChange the default “branch” name to main:\ngit config --global init.defaultbranch main\nCheck whether you successfully changed the settings:\ngit config --global --list\n# user.name=John Doe\n# user.email=doe.39@osu.edu\n# colour.ui=true\n# core.editor=nano -w\n# init.defaultbranch=main"
  },
  {
    "objectID": "git/02_git1.html#your-first-git-repository",
    "href": "git/02_git1.html#your-first-git-repository",
    "title": "Getting started with Git",
    "section": "4 Your first Git repository",
    "text": "4 Your first Git repository\nYou’ll create a Git repository for a mock book project: writing Charles Darwin’s “On the Origin of Species”.\n\n4.1 Start a new Git repository\nCreate a new dir for a mock project that we will version-control with Git, and move there:\n# Before starting, you should be in /fs/PAS2700/users/$USER, cd there first if needed\nmkdir -p week03/originspecies\ncd week03/originspecies\nThe command to initialize a new Git repository is git init — use that to start a repo for the originspecies dir:\ngit init\nInitialized empty Git repository in /fs/ess/PAS2700/users/jelmer/week03/originspecies/.git/\nCan we confirm that the Git repo dir is there?\n# The -a option to ls will also show hidden files\nls -a\n.  ..  .git\nNext, check the status of your new repository with git status:\ngit status\nOn branch main\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\nGit reports that you:\n\nAre on a “branch” that is called ‘main’. We won’t talk about Git branches in class, but this is discussed in the optional self-study material and CSB Chapter 2.6. Basically, these are “parallel versions” of your repository.\nHave not created any commits yet.\nHave “nothing to commit” because there are no files in this dir.\n\n\n\n\n4.2 Your first Git commit\nYou will start writing the book (😉) by echo-ing some text into a new file called origin.txt:\necho \"An Abstract of an Essay on ...\" &gt; origin.txt\nNow, check the status of the repository again:\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        origin.txt\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nGit has clearly detected the new file. But as mentioned, Git does not automatically start “tracking” files, which is to say it won’t automatically include files in the repository. Instead, it tells you the file is “Untracked” and gives a hint on how to add it to the repository.\nSo, start tracking the file and stage it all at once with git add:\n# (Note that tab-completion on file names will work here, too)\ngit add origin.txt\nCheck the status of the repo again:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   origin.txt\nNow, your file has been added to the staging area (also called the Index) and is listed as a “change to be committed”8. This means that if you now run git commit, the file would be included in that commit.\nSo, with your file tracked & staged, let’s make your first commit. Note that you must add the option -m followed by a “commit message”: a short description of the changes you are including in the current commit.\n# We use the commit message (option '-m') \"Started the book\" to describe our commit\ngit commit -m \"Started the book\"\n[main (root-commit) 3df4361] Started the book\n 1 file changed, 1 insertion(+)\n create mode 100644 origin.txt\nNow that you’ve made your first Git commit, check the status of the repo again:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\n\n\n\n\n\nTry to get used to using git status a lot — as a sanity check before and after other git actions.\n\n\n\n\n\n\nAlso look at the commit history of the repo with git log:\ngit log\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21 (HEAD -&gt; main)\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book\nNote the “hexadecimal code” (using numbers and the letters a-f) on the first line — this is a unique identifier for each commit, called the SHA-1 checksum. You can reference and access each past commit with these checksums.\n\n\n\n4.3 Your second commit\nStart by modifying the book file — you’ll actually overwrite the earlier content:\necho \"On the Origin of Species\" &gt; origin.txt\nCheck the status of the repo:\ngit status\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   origin.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nGit has noticed the changes, because the file is being tracked: origin.txt is listed as “modified”. But changes to tracked files aren’t automatically staged — use git add to stage the file as a first step to committing these changes:\ngit add origin.txt\nNow, make your second commit:\ngit commit -m \"Changed the title as suggested by Murray\"\n[main f106353] Changed the title as suggested by Murray\n 1 file changed, 1 insertion(+), 1 deletion(-)\nGit gives a brief summary of the changes that were made: you changed 1 file (origin.txt), and since you replaced the line of text in that file, it is interpreting that as 1 insertion (the new line) and 1 deletion (the removed/replace line).\nCheck the history of the repo again — you’ll see that there are now 2 commits:\ngit log\ncommit f1063537b6a1e0d87d2d52c9e96c38694959997a (HEAD -&gt; main)\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:01:49 2024 -0400\n\n    Changed the title as suggested by Murray\n\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book\n\n\n\n\n\n\n\nOne-line commit log\n\n\n\nAs you start accumulating commits, you might prefer git log --oneline for a one-line-per-commit summary:\ngit log --oneline\n1e2bba4 Changed the title as suggested by Murray\n4fd04af Started the book\n\n\n\n\n\n\n\n\n\nStaging files efficiently\n\n\n\nWhen you have multiple files that you would like to stage, you don’t need to add them one-by-one:\n# NOTE: Don't run any of this - these are hypothetical examples\n\n# Stage all files in the project (either option works):\ngit add --all\ngit add *\n\n# Stage all files in a specific dir (here: 'scripts') in the project:\ngit add scripts/*\n\n# Stage all shell scripts *anywhere* in the project:\ngit add *sh   \nFinally, you can use the -a option for git commit as a shortcut to stage and commit all changes with a single command (but note that this will not add untracked files):\n# Stage & commit all tracked files:\ngit commit -am \"My commit message\"\n\n\n\n\n\n4.4 What to include in individual commits\nThe last example in the box above showed the -a option to git commit, which allows you to at once stage & commit all changes since the last commit. That seems more convenient than separately git adding files before committing.\nHowever, it’s good practice not to simply and only commit, say, at the end of each day, but instead to try and create commits for units of progress worth saving and as such create separate commits for distinct changes.\nFor example, let’s say that you use git status to check which files you’ve changed since your last commit, and you find that you have:\n\nUpdated a README file to include more information about your samples.\nWorked on a script to run quality control of sequence files.\n\nThese are completely unrelated changes, and it would not be recommended to include both in a single commit.\n\n\n Exercise (CSB Intermezzo 2.1)\n\nCreate a new file todo.txt containing the line: “June 18, 1858: read essay from Wallace”.\n\n\n\nClick to see the solution\n\necho \"June 18, 1858: read essay from Wallace\" &gt; todo.txt\n\n\nUse a Git command to stage the file.\n\n\n\nClick to see the solution\n\ngit add todo.txt\n\n\nCreate a Git commit with the commit message “Added to-do list”.\n\n\n\nClick to see the solution\n\ngit commit -m \"Added to-do list\""
  },
  {
    "objectID": "git/02_git1.html#file-states-and-showing-changes",
    "href": "git/02_git1.html#file-states-and-showing-changes",
    "title": "Getting started with Git",
    "section": "5 File states and showing changes",
    "text": "5 File states and showing changes\n\n5.1 File states (and Git’s three “trees”)\nTracked files can be in one of three states:\n\nUnchanged since the last commit: committed (latest version is in the repo/commits).\nModified and staged since the last commit: staged (latest version is in the Index).\nModified but not staged since the last commit: modified (latest version is in the working dir).\n\n\n\n\n\n\n\n\nThe three trees of Git (Click to expand)\n\n\n\n\n\nThese three states correspond to the three “trees” of Git:\n\nHEAD: State of the project in the most recent commit9.\nIndex (Stage): State of the project ready to be committed.\nWorking directory (Working Tree): State of the project as currently on your computer.\n\n\n\n\n\nThe three “trees” of Git: HEAD, the index, and the working dir.The hexadecimals in the Commits rectangles are abbreviated checksums for each commit.\n\n\n\nOr consider this table for a hypothetical example in which HEAD, the Index, and the working dir all differ with regards to the the version of file 1, and there also is an untracked file in the working dir:\n\n\n\n\n\n\n\n\nFile state\nVersion\nWhich tree\n\n\n\n\nCommitted\nfile 1 version X\nHEAD\n\n\nStaged\nfile 1 version Y\nIndex (stage)\n\n\nModified\nfile 1 version Z\nWorking dir\n\n\nUntracked\nfile 2 version X\nWorking dir\n\n\n\n\n\n\n\n\n\n\n\n\nWays to refer to past commits (Click to expand)\n\n\n\n\n\nTo refer to specific past commits, you can:\n\nUsing the hexadecimal checksum (either the full ID or the 7-character abbreviation)\nUse HEAD notation: HEAD is the most recent commit, and there are two ways of indicating ancestors of HEAD:\n\n\n\n\n\nTo refer to past commits, you can use checksums (e.g. dab0dc4 for the second-to-last commit)or HEAD notation (HEAD^^ or HEAD~2 for the second-to-last commit).\n\n\n\n\n\n\n\n\n5.2 Showing changes\nYou can use the git diff command to show changes that you have made. By default, it will show all changes between the working dir and:\n\nThe last commit if nothing has been staged.\nThe stage (Index) if something has been staged.\n\n\n\n\n\n\n\n“Working dir” in the context of Git\n\n\n\nNote that when I talk about the “working dir” in the context of Git, I mean not just your top-level project/repository directory, or any specific dir within there that you may have cd-ed into, but the entire project/repository directory hierarchy.\nIt is mainly used to distinguish the state of your project on your computer (“working dir”) versus that in the repository (“index” and “commits”), and should technically be referred to as the “working dir tree”.\n\n\nRight now, there are no differences to report in our originspecies repository, because our working dir, the stage/Index, and the last commit are all the same:\n# Git diff will not have output if there are no changes to report\ngit diff\nChange the to-do list (note: for this to work, you should have done the exercise above!), and check again:\necho \"June 20, 1858: Send first draft to Huxley\" &gt;&gt; todo.txt\n\ngit diff\ndiff --git a/todo.txt b/todo.txt\nindex e3b5e55..9aca508 100644\n--- a/todo.txt\n+++ b/todo.txt\n@@ -1 +1,2 @@\n June 18, 1858: read essay from Wallace\n+June 20, 1858: Send first draft to Huxley\nWe won’t go into the details of the above “diff format”, but at the bottom of the output above, you can see some specific changes: the line “Send first draft to Huxley” was added (hence the + sign) in our latest version of the file.\n\n\n\n\n\n\n\nVS Code can show file differences in a nicer way\n\n\n\n\nClick on the Git symbol in the narrow side bar (below the search icon) to open the Source Control side bar.\nIn the source control sidebar, you should see not just the originspecies repository listed, but also the CSB repo10. If needed, click on originspecies to expand it:\n\n\n\n\n\n\n\nWithin the originspecies listing, you should see todo.txt: click on the M next to the file todo.txt, and the following should appear in your editor pane:\n\n\n\n\n\n\nThat’s a much more intuitive overview that makes it clear which line was added.\n\n\n\n\n\n\n\n\nMore git diff (Click to expand)\n\n\n\n\n\n\nTo show changes between the Index (stage) and the last commit, use the --staged option to git diff.\nIf you have changed multiple files, but just want to see differences for one of them, you can specify the filename — in our case here, that will give the same output as the plain git diff command above, since we only changed one file:\ngit diff todo.txt\n# Output not shown, same as above\nYou can also compare your repo or individual files between any two arbitrary commits (for the HEAD notation, see the boxes on the “three trees” of Git above.):\n# Last commit vs second-to-last commit - full repo:\ngit diff HEAD HEAD^\n\n# Last commit vs a specified commit - specific file: \ngit diff HEAD d715c54 todo.txt \n\n\n\n\n\n Exercise: another commit\nStage and commit the changes to todo.txt, then check what you have done.\n\n\nClick to see the solution\n\n\nStage the file:\ngit add todo.txt\nCommit:\ngit commit -m \"Update the TODO list\"\n[main 8ec8103] Update the TODO list\n1 file changed, 1 insertion(+)\nCheck the log:\ngit log\ncommit 8ec8103e8d01b342f9470908b87f0649be53edd5\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 12:30:35 2024 -0400\n\n    Update the TODO list\n\ncommit 9715ab5325429526a90ea49e9d40a923c93ccb72\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:37:32 2024 -0400\n\n    Added a gitignore file\n\ncommit 603d1792619bf628d66cd91a45cd7114e3d6b95b\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:21:36 2024 -0400\n\n    Added to-do list\n\ncommit f1063537b6a1e0d87d2d52c9e96c38694959997a\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 11:01:49 2024 -0400\n\n    Changed the title as suggested by Murray\n\ncommit 3df4361c1de9b71e08bf6e050105d53097acec21\nAuthor: Jelmer Poelstra &lt;jelmerpoelstra@gmail.com&gt;\nDate:   Mon Mar 11 10:55:35 2024 -0400\n\n    Started the book"
  },
  {
    "objectID": "git/02_git1.html#ignoring-files-and-directories",
    "href": "git/02_git1.html#ignoring-files-and-directories",
    "title": "Getting started with Git",
    "section": "6 Ignoring files and directories",
    "text": "6 Ignoring files and directories\nAs discussed above, it’s best not to track some files, such as very bulky data files, temporary files, and results.\nWe’ve seen that Git will notice and report any “untracked” files in your project whenever you run git status. This can get annoying and can make it harder to spot changes and untracked files that you do want to add — and you might even accidentally start tracking these files such as with git add --all.\nTo deal with this, you can tell Git not to pay attention to certain files by adding file names and wildcard selections to a .gitignore file. This way, these files won’t be listed as untracked files when you run git status, and they wouldn’t be added even when you use git add --all.\nTo see this in action, let’s start by adding some content that we don’t want to commit to our repository: a dir data, and a file ending in a ~ (a temporary file type that e.g. text editors can produce):\nmkdir data\ntouch data/drawings_1855-{01..12} todo.txt~\nWhen we check the status of the repo, we can see that Git has noticed these files:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        todo.txt~\nIf we don’t do anything about this, Git will keep reporting these untracked files whenever we run git status. To prevent this, we will we create a .gitignore file:\n\nThis file should be in the project’s root dir and should be called .gitignore.\n.gitignore is a plain text file that contains dir and file names/patterns, all of which will be ignored by Git.\nAs soon as such a file exists, Git will automatically check and process its contents.\nIt’s a good idea add and commit this file to the repo.\n\nWe will create our .gitignore file and add the following to it to instruct Git to ignore everything in the data/ dir, and any file that ends in a ~:\necho \"data/\" &gt; .gitignore\necho \"*~\" &gt;&gt; .gitignore\ncat .gitignore\ndata/\n*~\nWhen we check the status again, Git will have automatically processed the contents of the .gitignore file, and the files we want to ignore should no longer be listed as untracked files:\ngit status\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .gitignore\nHowever, we do now have an untracked .gitignore file, and we should track and commit this file:\ngit add .gitignore\ngit commit -m \"Added a gitignore file\"\n[main 9715ab5] Added a gitignore file\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\n\n\n\n\n\n\n\nGood project file organization helps with version control\n\n\n\nGood project file organization can make your life with Git a lot easier. This is especially true when it comes to files that you want to ignore.\nSince you’ll generally want to ignore data and results files, if you keep all of those in their own top-level directories, it will be easy and not error-prone to tell Git to ignore them. But if you were -for example- mixing scripts and either results or data within dirs, it would be much harder to keep this straight."
  },
  {
    "objectID": "git/02_git1.html#moving-and-removing-tracked-files",
    "href": "git/02_git1.html#moving-and-removing-tracked-files",
    "title": "Getting started with Git",
    "section": "7 Moving and removing tracked files",
    "text": "7 Moving and removing tracked files\nWhen wanting to remove, move, or rename files that are tracked by Git, it is good practice to preface regular rm and mv commands with git: so, git rm &lt;file&gt; and git mv &lt;source&gt; &lt;dest&gt;.\nWhen removing or moving/renaming a tracked file with git rm / git mv, changes will be made to your working dir just like with a regular rm/mv, and the operation will also be staged. For example:\n# (NOTE: Don't run this, hypothetical examples)\ngit rm file-to-remove.txt\ngit mv myoldname.txt mynewname.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    myoldname.txt -&gt; mynewname.txt\n        deleted:    file-to-remove.txt\n\n\n\n\n\n\nWhat if I forget to use git rm/git mv? (Click to expand)\n\n\n\n\n\nIt is inevitable that you will occasionally forget about this and e.g. use rm instead of git rm. Fortunately, Git will eventually figure out what happened. For example:\n\nFor a renamed file, Git will first be confused and register both a removed file and an added file:\n# (Don't run this, this is a hypothetical example)\nmv myoldname.txt mynewname.txt\n\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add/rm &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        deleted:    myoldname.txt\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        mynewname.txt\nBut after you stage both changes (the new file and the deleted file), Git realizes it was renamed instead:\ngit add myoldname.txt\ngit add mynewname.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        renamed:    myoldname.txt -&gt; mynewname.txt\n\nSo, there is no need to stress if you forget this, but when you remember, use git mv and git rm.\n\n\n\n\n\n Exercises: .gitignore and git rm\nA) Create a new directory results with files Galapagos.txt and Ascencion.txt. Add a line to the .gitignore file to ignore these results, and commit the changes to the .gitignore file.\n\n\nClick to see the solution\n\n\nCreate the dir and files:\nmkdir results\ntouch results/Galapagos.txt results/Ascencion.txt\nOptional - check that they are detected by Git (note: only the dir will be shown, not its contents):\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        results/\n\nnothing added to commit but untracked files present (use \"git add\" to track\nAdd the string “results/” to the .gitignore file:\necho \"results/\" &gt;&gt; .gitignore\nOptional - check the status again:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   .gitignore\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nLooks good, results/ is no longer listed. But we do need to commit the changes to .gitignore.\nCommit the changes to .gitignore:\ngit add .gitignore\ngit commit -m \"Add results dir to gitignore\"\n[main 33b6576] Add results dir to gitignore\n1 file changed, 1 insertion(+)\n\n\nB) Create and commit an empty new file notes.txt. Then, remove it with git rm and commit your file removal.\n\n\nClick to see the solution\n\n\nCreate the file and add and commit it:\ntouch notes.txt\ngit add notes.txt\ngit commit -m \"Add notes\"\n[main 44a37f9] Add notes\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 notes.txt\nOptional - check that the file is there:\nls\ndata  notes.txt  origin.txt  README.md  todo.txt  todo.txt~\nRemove the file with git rm and commit the removal:\ngit rm notes.txt\ngit commit -m \"These notes were made in error\"\n[main 058fd47] These notes were made in error\n 1 file changed, 0 insertions(+), 0 deletions(-)\n delete mode 100644 notes.txt\nOptional - check that the file is no longer there:\nls\ndata  origin.txt  README.md  todo.txt  todo.txt~"
  },
  {
    "objectID": "git/02_git1.html#undoing-changes-that-have-not-been-committed",
    "href": "git/02_git1.html#undoing-changes-that-have-not-been-committed",
    "title": "Getting started with Git",
    "section": "8 Undoing changes that have not been committed",
    "text": "8 Undoing changes that have not been committed\nHere, you’ll learn how to undo changes that have not been committed, like undoing an accidental file removal or overwrite. (In the optional self-study Git material, there is a section on undoing changes that have been committed.)\n\n8.1 Recovering a version from the repo\nWe’ll practice with undoing changes to your working dir (that have not been staged) by recovering a version from the repo: in other words, using Git as an “undo button” after accidental file changes or removal.\n\nLet’s say you accidentally overwrote instead of appended to a file:\necho \"Finish the taxidermy of the finches from Galapagos\" &gt; todo.txt\nAlways start by checking the status:\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   todo.txt\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nYou’ll want to “discard changes in working directory”, and Git told you how to do this — with git restore:\ngit restore todo.txt\n\n\nIf you accidentally deleted a file, you can similarly retrieve it with git checkout:\n\nAccidental removal of todo.txt\nrm todo.txt\nUse git restore to get the file back!\ngit restore todo.txt\n\n\n\n\n\n\n\n\nAlternative, older command used in the CSB book: git checkout (Click to expand)\n\n\n\n\n\nUntil recently, this action used to be done with with the git checkout command, for example:\ngit checkout -- README.md\ngit restore is a relatively new command designed to avoid confusion with the git checkout and git reset commands, which have multiple functions. The CSB book still uses the git checkout command for a similar example11.\n\n\n\n\n\n\n8.2 Unstaging a file\ngit restore can also unstage a file, which is most often needed when you added a file that was not supposed to be part of the next commit. For example:\n\nYou modify two files and use git add --all:\necho \"Variation under domestication\" &gt;&gt; origin.txt\necho \"Prepare for the next journey\" &gt;&gt; todo.txt\n\ngit add --all\nThen you realize that those two file changes should be part of separate commits. Again, check the status first:\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   origin.txt\n        modified:   todo.txt\nAnd use git restore --staged as suggested by Git:\ngit restore --staged todo.txt\n\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   origin.txt\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   todo.txt\n\nNow, you can go ahead and add these changes to separate commits: see the exercise below.\n(Finally: in case you merely staged a file prematurely, you can just continue editing the file and re-add it.)\n\n\n\n\n\n\nAlternative, older command used in the CSB book: git reset (Click to expand)\n\n\n\n\n\nLike with discarding changes in the working dir, this action used to be done with another command, this time git reset. For example:\ngit reset HEAD README.md\nThe CSB book uses this but note that there is a mistake in the book: git reset will only unstage and not revert the file back to its state at the last commit. (git reset --hard does revert things back to the state of a desired commit, but only works on commits and not individual files.)\n\n\n\n\n\n Exercise: Commit the changes 1-by-1\n\nCommit the currently staged changes to origin.txt.\nStage and commit the changes to todo.txt.\n\n\n\nClick for the solution\n\n\nCommit the currently staged changes to origin.txt.\ngit commit -m \"Start writing about artificial selection\"\nStage and commit the changes to todo.txt.\ngit add todo.txt\ngit commit -m \"Update the TODO file\"\n\n\n\n\n\n\n\n\n\n\nUndoing staged changes\n\n\n\nWhat if you had made mistaken changes (like an accidental deletion) and also staged those changes? You can simply follow both of the two steps described above in order:\n\nFirst unstage the file with git restore --staged &lt;file&gt;.\nThen discard changes in the working dir with git restore &lt;file&gt;.\n\nFor instance, you overwrote the contents of the book and then staged the misshapen file:\necho \"Instincts of the Cuckoo\" &gt; origin.txt\ngit add origin.txt\n\ncat origin.txt\nInstincts of the Cuckoo\nYou can undo all of this as follows:\ngit restore --staged origin.txt\ngit restore origin.txt\n\ncat origin.txt\nOn the Origin of Species\nVariation under domestication"
  },
  {
    "objectID": "git/02_git1.html#some-git-best-practices",
    "href": "git/02_git1.html#some-git-best-practices",
    "title": "Getting started with Git",
    "section": "9 Some Git best-practices",
    "text": "9 Some Git best-practices\n\nWrite informative commit messages.\nImagine looking back at your project in a few months, after finding an error that you introduced a while ago.\n\nNot-so-good commit message: “Updated file”\nGood commit message: “In file x, updated function y to include z”\n\n\n\n\n\nImage source\n\n\n\n\n\n\n\n\nCommit messages for the truly committed\n\n\n\nIt is often argued that commit messages should preferably be in the form of completing the sentence “This commit will…”: When adhering to this, the above commit message would instead say “In file x, update function y to include z.”.\n\n\n\nCommit often, using small commits.\nThis will also help to keep commit messages informative!\nBefore committing, check what you’ve changed.\nUse git diff [--staged] or VS Code functionality.\nAvoid including unrelated changes in commits.\nSeparate commits if your working dir contains work from disparate edits: use git add + git commit separately for two sets of files.\nDon’t commit unnecessary files.\nThese can also lead to conflicts — especially automatically generated, temporary files.\n\n\n\n\n\n\n\n\nA more advanced tip: tags\n\n\n\nIf you have a repo with general scripts, which you continue to develop and use in multiple projects, and you publish a paper in which you use these scripts, it is a good idea to add a “tag” to a commit to mark the version of the scripts used in your analysis:\ngit tag -a v1.2.0 -m \"Clever release title\"\ngit push --follow-tags"
  },
  {
    "objectID": "git/02_git1.html#footnotes",
    "href": "git/02_git1.html#footnotes",
    "title": "Getting started with Git",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOthers include SVN and Mercurial.↩︎\nAnd if you’re writing software, all its source code.↩︎\nGit will just save an entirely new version whenever there’s been a change rather than tracking changes in individual lines.↩︎\nE.g., you’ll have to use Git for you final project.↩︎\n It is available by default, but that’s a very ancient version.↩︎\n You can learn more about “loading” (and installing) software at OSC in this tutorial↩︎\nWhen you need to provide Git with a “commit message” to Git and you haven’t entered one on the command line.↩︎\n You also get a hint on how to “unstage” the file: i.e., reverting what you just did with git add and leaving the file untracked once again↩︎\nOn the current “branch” – see the optional self-study page or CSB chapter 2.6 to learn about branches.↩︎\n This is because our VS Code working dir is not originspecies but two levels up from there. Typically, your VS Code working dir should be your project dir which would be the same as the repo dir.↩︎\n In that example, the CSB book example omits the dashes --. These indicate that the checkout command should operate on a file, but since the file name is provided too, this is not strictly necessary.↩︎"
  },
  {
    "objectID": "git/04_git3.html",
    "href": "git/04_git3.html",
    "title": "Git: Branching, collaborating, and undoing",
    "section": "",
    "text": "This page contains optional self-study material if you want to dig deeper into Git. Some of it may also be useful as a reference in case you run into problems while trying to use Git."
  },
  {
    "objectID": "git/04_git3.html#branching-merging",
    "href": "git/04_git3.html#branching-merging",
    "title": "Git: Branching, collaborating, and undoing",
    "section": "1 Branching & merging",
    "text": "1 Branching & merging\nIn this section, you’ll learn about using so-called “branches” in Git. Branches are basically parallel versions of your repository, which allow you or your collaborators to experiment or create variants without affecting existing functionality or others’ work.\n\n\n1.1 A repo with a couple of commits\nFirst, you’ll create a dummy repo with a few commits by running a script (following CSB).\ncd /fs/ess/PAS2700/users/$USER/CSB/git/sandbox\nTake a look at the script you will run to create your repo:\ncat ../data/create_repository.sh\n#!/bin/bash\n\n# function of the script:\n# sets up a repository and\n# immitates workflow of\n# creating and commiting two text files\n\nmkdir branching_example\ncd branching_example\ngit init\necho \"Some great code here\" &gt; code.txt\ngit add .\ngit commit -m \"Code ready\"\necho \"If everything would be that easy!\" &gt; manuscript.txt \ngit add .\ngit commit -m \"Drafted paper\"\nRun the script:\nbash ../data/create_repository.sh\nInitialized empty Git repository in /fs/ess/PAS2700/users/jelmer/CSB/git/sandbox/branching_example/.git/\n[main (root-commit) 3c59d8a] Code ready\n 1 file changed, 1 insertion(+)\n create mode 100644 code.txt\n[main 7ba8ca4] Drafted paper\n 1 file changed, 1 insertion(+)\n create mode 100644 manuscript.txt\nAnd move into the repository’s dir:\ncd branching_example\nLet’s see what has been done in this repo:\ngit log --oneline\n7ba8ca4 (HEAD -&gt; main) Drafted paper\n3c59d8a Code ready\nWe will later modify the file code.txt — let’s see what it contains now:\ncat code.txt\nSome great code here\n\n\n\n1.2 Using branches in Git\nYou now want to improve the code, but these changes are experimental, and you want to retain your previous version that you know works. This is where branching comes in. With a new branch, you can make changes that don’t affect the main branch, and can also keep working on the main branch:\n\n\n\nFigure modified after Allesino & Wilmes (2019).(Note that the main branch is here called “master”.)\n\n\n\nCreating a new branch\nFirst, create a new branch as follows, naming it fastercode:\ngit branch fastercode\nList the branches:\n# Without args, git branch will list the branches\ngit branch\n  fastercode\n* main\nIt turns out that you created a new branch but are still on the main branch, as the * indicates.\nYou can switch branches with git checkout:\ngit checkout fastercode\nSwitched to branch 'fastercode'\nAnd confirm your switch with git branch:\ngit branch\n* fastercode\n  main\nNote that you can also tell from the git status output on which branch you are:\ngit status\nOn branch fastercode\nnothing to commit, working tree clean\n\n\n\nMaking experimental changes on the new branch\nYou edit the code, stage and commit the changes:\necho \"Yeah, faster code\" &gt;&gt; code.txt\ncat code.txt\nSome great code here\nYeah, faster code\ngit add code.txt\ngit commit -m \"Managed to make code faster\"\n[fastercode 21f1828] Managed to make code faster\n 1 file changed, 1 insertion(+)\nLet’s check the log again, which tells you that the last commit was made on the fastercode branch:\ngit log --oneline\n21f1828 (HEAD -&gt; fastercode) Managed to make code faster\n7ba8ca4 (main) Drafted paper\n3c59d8a Code ready\n\n\n\nMoving back to the main branch\nYou need to switch gears and add references to the paper draft. Since this has nothing to do with your attempt at faster code, you should make these changes back on the main branch:\n# Move back to the 'main' branch\ngit checkout main\nSwitched to branch 'main'\nWhat does code.txt, which we edited on fastercode, now look like?\ncat code.txt\nSome great code here\nSo, by switching between branches, your working dir contents has changed!\nNow, while still on the main branch, add the reference, stage and commit:\necho \"Marra et al. 2014\" &gt; references.txt\ngit add references.txt\ngit commit -m \"Fixed the references\"\n[main 1bf123f] Fixed the references\n 1 file changed, 1 insertion(+)\n create mode 100644 references.txt\nNow that you’ve made changes to both branches, let’s see the log in “graph” format with --graph, also listing all branches with --all — note how it tries to depict these branches:\ngit log --oneline --graph --all\n* 1bf123f (HEAD -&gt; main) Fixed the references\n| * 21f1828 (fastercode) Managed to make code faster\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nFinishing up on the experimental branch\nEarlier, you finished speeding up the code in the fastercode branch, but you still need to document your changes. So, you go back:\ngit checkout fastercode\nSwitched to branch 'fastercode'\nDo you still have the references.txt file from the main branch?\nls\ncode.txt  manuscript.txt\nNope, your working dir has changed again.\nThen, add the “documentation” to the code, and stage and commit:\necho \"# My documentation\" &gt;&gt; code.txt\ngit add code.txt\ngit commit -m \"Added comments to the code\"\n[fastercode d09f611] Added comments to the code\n 1 file changed, 1 insertion(+)\nCheck the log graph:\ngit log --oneline --all --graph\n* d09f611 (HEAD -&gt; fastercode) Added comments to the code\n* 21f1828 Managed to make code faster\n| * 1bf123f (main) Fixed the references\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nMerging the branches\nYou’re happy with the changes to the code, and want to make the fastercode version the default version of the code. This means you should merge the fastercode branch back into main. To do so, you first have to move back to main:\ngit checkout main\nSwitched to branch 'main'\nNow you are ready to merge with the git merge command. You’ll also have to provide a commit message, because a merge is always accompanied by a commit:\ngit merge fastercode -m \"Much faster version of code\"\nMerge made by the 'ort' strategy.\n code.txt | 2 ++\n 1 file changed, 2 insertions(+)\nOnce again, check the log graph, which depicts the branches coming back together:\ngit log --oneline --all --graph\n*   5bb84cd (HEAD -&gt; main) Much faster version of code\n|\\  \n| * d09f611 (fastercode) Added comments to the code\n| * 21f1828 Managed to make code faster\n* | 1bf123f Fixed the references\n|/  \n* 7ba8ca4 Drafted paper\n* 3c59d8a Code ready\n\n\n\nCleaning up\nYou no longer need the fastercode branch, so you can delete it as follows:\ngit branch -d fastercode\nDeleted branch fastercode (was d09f611).\n\n\n\n\n1.3 Branching and merging – Workflow summary\n\n\n\nFigure from after Allesino & Wilmes (2019)\n\n\n\nOverview of commands used in the branching workflow\n# (NOTE: Don't run this)\n\n# Create a new branch:\ngit branch mybranch\n\n# Move to new branch:\ngit checkout mybranch\n\n# Add and commit changes:\ngit add --all\ngit commit -m \"my message\"\n\n# Done with branch - move back to main trunk and merge\ngit checkout main\ngit merge mybranch -m \"Message for merge\"\n\n# And [optionally] delete the branch:\ngit -d mybranch\n\n\n\n Exercise (Intermezzo 2.2)\n\n(a) Move to the directory CSB/git/sandbox.\n\n\n\nSolution\n\ncd /fs/ess/PAS2700/users/$USER/CSB/git/sandbox\n\n\n(b) Create a directory thesis and turn it into a Git repository.\n\n\n\nSolution\n\nmkdir thesis\ncd thesis\ngit init\n\n\n(c) Create the file introduction.txt with the line “Best introduction ever.”\n\n\n\nSolution\n\necho \"The best introduction ever\" &gt; introduction.txt\n\n\n(d) Stage introduction.txt and commit with the message “Started introduction.”\n\n\n\nSolution\n\ngit add introduction.txt\ngit commit -m \"Started introduction\"\n\n\n\n(e) Create the branch newintro and change into it.\n\n\n\nSolution\n\ngit branch newintro\ngit checkout newintro\n\n\n(f) Overwrite the contents of introduction.txt, create a new file methods.txt, stage, and commit.\n\n\n\nSolution\n\necho \"A much better introduction\" &gt; introduction.txt\ntouch methods.txt\ngit add --all\ngit commit -m \"A new introduction and methods file\"\n\n\n(g) Move back to main. What does your working directory look like now?\n\n\n\nSolution\n\ngit checkout main\nls     # Changes made on the other branch are not visible here!\ncat introduction.txt\n\n\n(h) Merge in the newintro branch, and confirm that the changes you made there are now in your working dir.\n\n\n\nSolution\n\ngit merge newintro -m \"New introduction\"\nls\ncat introduction.txt\n\n\n(i) Bonus: Delete the branch newintro.\n\n\n\nSolution\n\ngit branch -d newintro"
  },
  {
    "objectID": "git/04_git3.html#collaboration-with-git-multi-user-remote-workflows",
    "href": "git/04_git3.html#collaboration-with-git-multi-user-remote-workflows",
    "title": "Git: Branching, collaborating, and undoing",
    "section": "2 Collaboration with Git: multi-user remote workflows",
    "text": "2 Collaboration with Git: multi-user remote workflows\nIn a multi-user workflow, your collaborator can make changes to the repository (committing to local, then pushing to remote), and you need to make sure that you stay up-to-date with these changes.\nSynchronization between your and your collaborator’s repository happens via the remote, so now you will need a way to download changes from the remote that your collaborator made. This happens with the git pull command.\n\n\n\n\n\nAFirst, a second user, your collaborator, downloads (clones) the online repo. They should also receive admin rights on the repo (not shown - done on GitHub).\n\n\n\n\n\n\n\n\nBThen, your collaborator commits changes to their local copy of the repository.\n\n\n\n\n\n\n\n\n\n\nCBefore you can receive these changes, your collaborator will need to push their changes to the remote, which you can access too.\n\n\n\n\n\n\n\n\nDTo update your local repo with the changes made by your collaborator, you pull in the changes from the remote. Now all 3 copies of the repo are in sync again!\n\n\n\n\n\nIn a multi-user workflow, changes made by different users are shared via the online copy of the repo. But syncing is not automatic:\n\nChanges to your local repo remain local-only until you push to remote.\nSomeone else’s changes to the remote repo do not make it into your local repo until you pull from remote.\n\nHowever, when your collaborator has made changes, Git will tell you about “divergence” between your local repository and the remote when you run git status:\n# (Don't run this)\ngit status\n\n\n\n\n\nIn a multi-user workflow, you should use use git pull often, since staying up-to-date with your collaborator’s changes will reduce the chances of merge conflicts.\n\n\n2.1 Add a collaborator in GitHub\nYou can add a collaborator to a repository on GitHub as follows:\n\nGo to the repository’s settings:\n\n\n\n\n\n\n\nFind and click Manage access:\n\n\n\n\n\n\n\nClick Invite a collaborator:\n\n\n\n\n\n\n\n\n\n2.2 Merge conflicts\nA so-called merge conflict means that Git is not able to automatically merge two branches, which occurs when all three of the following conditions are met:\n\nYou try to merge two branches (including when pulling from remote: a pull includes a merge)\nOne or more file changes have been committed on both of these branches since their divergence.\nSome of these changes were made in the same part(s) of file(s).\n\nWhen this occurs, Git has no way of knowing which changes to keep, and it will report a merge conflict as follows:\n\n\n\n\n\n\nResolving a merge conflict\nWhen Git reports a merge conflict, follow these steps:\n\nUse git status to find the conflicting file(s).\n\n\n\n\n\n\n\nOpen and edit those file(s) manually to a version that fixes the conflict (!).\nNote below that Git will have changed these file(s) to add the conflicting lines from both versions of the file, and to add marks that indicate which lines conflict.\nYou have to manually change the contents in your text editor to keep the conflicting content that you want, and to remove the indicator marks that Git made.\nOn the Origin of Species       # Line preceding conflicting line\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD                   # GIT MARK 1: Next line = current branch\nLine 2 - from main             # Conflict line: current branch\n=======                        # GIT MARK 2: Dividing line\nLine 2 - from conflict-branch  # Conflict line: incoming branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; conflict-branch        # GIT MARK 3: Prev line = incoming branch\nUse git add to tell Git you’ve resolved the conflict in a particular file:\ngit add origin.txt\n\n\n\n\n\n\n\nOnce all conflicts are resolved, use git status to check that all changes have been staged. Then, use git commit to finish the merge commit:\ngit commit -m \"Solved the merge conflict\"\n\n\n\n\n\n\n\n\nVS Code functionality for resolving Merge Conflicts\n\n\n\nVS Code has some nice functionality to make Step 2 (resolving the conflict) easier:\ncode &lt;conflicting-file&gt;  # Open the file in VS Code\n\n\n\n\n\nIf you click on “Accept Current Change” or “Accept Incoming Change”, etc., it will keep the desired lines and remove the Git indicator marks. Then, save and exit."
  },
  {
    "objectID": "git/04_git3.html#contributing-to-repositories-forking-pull-requests",
    "href": "git/04_git3.html#contributing-to-repositories-forking-pull-requests",
    "title": "Git: Branching, collaborating, and undoing",
    "section": "3 Contributing to repositories: Forking & Pull Requests",
    "text": "3 Contributing to repositories: Forking & Pull Requests\n\n3.1 What can you do with someone else’s GitHub repository?\nIn some cases, you may be interested in working in some way with someone else’s repository that you found on GitHub. If you do not have rights to push, you can:\n\nClone the repo and make changes locally (as we have been doing with the CSB repo). When you do this, you can also periodically pull to remain up-to-date with changes in the original repo.\nFork the repository on GitHub and develop it independently. Forking creates a new personal GitHub repo, to which you can push.\nUsing a forked repo, you can also submit a Pull Request with proposed changes to the original repo: for example, if you’ve fixed a bug in someone else’s program.\n\nIf you’re actually collaborating on a project, though, you should ask your collaborator to give you admin rights for the repo, which makes things a lot easier than working via Pull Requests.\n\n\n\n\n\n\nForking, Pull Requests, and Issues are GitHub functionality, and not part of Git.\n\n\n\n\n\n\n\nForking a GitHub repository\nYou can follow along by e.g. forking my originspecies repo.\n\nGo to a GitHub repository, and click the “Fork” button in the top-right:\n\n\n\n\n\n\n\nYou may be asked which account to fork to: select your account.\nNow, you have your own version of the repository, and it is labeled explicitly as a fork:\n\n\n\n\n\n\n\n\nForking workflow\nYou can’t directly modify the original repository, but you can:\n\nFirst, modify your fork (with local edits and pushing).\nThen, submit a so-called Pull Request to the owner of the original repo to pull in your changes.\nAlso, you can also easily keep your fork up-to-date with changes to the original repository.\n\n\n\n\nFigure from Happy Git and GitHub for the useR\n\n\n\n\n\nEditing the forked repository\nTo clone your forked GitHub repository to a dir at OSC, start by creating a dir there — for example:\nmkdir /fs/ess/PAS2700/users/$USER/week03/fork_test\ncd /fs/ess/PAS2700/users/$USER/week03/fork_test\nThen, find the URL for your forked GitHub repository by clicking the green Code button. Make sure you get the SSH URL (rather than the HTTPS URL), and click the clipboard button next to the URL to copy it:\n\n\n\n\n\nThen, type git clone and a space, and paste the URL, e.g.:\ngit clone git@github.com:jelmerp/originspecies.git\nCloning into 'originspecies'...\nremote: Enumerating objects: 31, done.\nremote: Counting objects: 100% (31/31), done.\nremote: Compressing objects: 100% (19/19), done.\nremote: Total 31 (delta 4), reused 30 (delta 3), pack-reused 0\nReceiving objects: 100% (31/31), done.\nResolving deltas: 100% (4/4), done.\nNow, you can make changes to the repository in the familiar way, for example:\necho \"# Chapter 1. Variation under domestication\" &gt; origin.txt\ngit add origin.txt\ngit commit -m \"Suggested title for first chapter.\"\nAnd note that you can push without any setup — because you cloned the repository, the remote setup is already done (and you have permission to push because its your own repo on GitHub and you have set up GitHub authentication):\ngit push\n\n\n\nCreating a Pull Request\nIf you then go back to GitHub, you’ll see that your forked repo is “x commit(s) ahead” of the original repo:\n\n\n\n\n\nClick Pull request, and check whether the right repositories and branches are being compared (and here you can also see the changes that were made in the commits):\n\n\n\n\n\nIf it looks good, click the green Create Pull Request button:\n\n\n\n\n\nGive your Pull Request a title, and write a brief description of your changes:\n\n\n\n\n\n\n\n\nKeeping your fork up-to-date\nAs you saw, you can’t directly push to original repo but instead have to submit a Pull Request (yes, this terminology is confusing!).\nBut, you can create an ongoing connection to the original repo, which you can use to periodically pull to keep your fork up-to-date. This works similarly to connecting your own GitHub repo, but you should give the remote a different nickname than origin — the convention is upstream:\n# Add the \"upstream\" connection\ngit remote add upstream git@github.com:jelmerp/originspecies.git\n\n# List the remotes:\ngit remote -v\norigin   git@github.com:pallass-boszanger/originspecies.git  (fetch)\norigin   git@github.com:pallass-boszanger/originspecies.git  (push)\nupstream   git@github.com:jelmerp/originspecies.git  (fetch)\nupstream   git@github.com:jelmerp/originspecies.git  (push)\n# Pull from the upstream repository:\ngit pull upstream main\n\n\n\n\n\n\n“upstream” is an arbitrary but convential name, compare with “origin” which is used for your own version of the online repo."
  },
  {
    "objectID": "git/04_git3.html#undoing-viewing-changes-that-have-been-committed",
    "href": "git/04_git3.html#undoing-viewing-changes-that-have-been-committed",
    "title": "Git: Branching, collaborating, and undoing",
    "section": "4 Undoing (& viewing) changes that have been committed",
    "text": "4 Undoing (& viewing) changes that have been committed\nWhereas on the first Git page, we learned about undoing changes that have not been committed, here you’ll see how you can undo changes that have been committed.\n\n\n4.1 Viewing past versions of the repository\nBefore undoing committed changes, you may want to look at earlier states of your repo, e.g. to know what to revert to:\n\nFirst, print an overview of past commits and their messages:\n# (NOTE: example code in this and the next few boxes - don't run as-is)\ngit log --oneline\nFind a commit you want to go back to, and look around in the past:\ngit checkout &lt;sha-id&gt; # Replace &lt;sha-id&gt; by an actual hash\n\nless myfile.txt       # Etc. ...\nThen, you can go back to where you were originally as follows:\ngit checkout main\n\nThe next section will talk about strategies to move your repo back to an earlier state that you found this way.\n\n\n\n\n\n\nJust need to retrieve an older version of a single file?\n\n\n\nIf you just want to retrieve/restore an older version of a single file that you found while browsing around in the past, then a quick way can be: simply copy the file to a location outside of your repo, move yourself back to the “present”, and move the file back into your repo, now in the present.\n\n\n\n\n\nA visual of using git checkout to view files from older versions of your repo.Figure from https://software-carpentry.org.\n\n\n\n\n\n\n\n\nThe multiple uses of git checkout\n\n\n\nNote the confusing re-use of git checkout! We have now seen git checkout being used to:\n\nMove between branches\nMove to previous commits to explore (figure below)\n(Revert files back to previous states — as an alternative to git restore)\n\n\n\n\n\n\n4.2 Undoing entire commits\nTo undo commits, i.e. move the state of your repository back to how it was before the commit you want to undo, there are two main commands:\n\ngit revert: Undo the changes made by commits by reverting them in a new commit.\ngit reset: Delete commits as if they were never made.\n\n\nUndoing commits with git revert\nA couple of examples of creating a new commit that will revert all changes made in the specified commit:\n# Undo changes by the most recent commit:\ngit revert HEAD\n  \n# Undo changes by the second-to-last commit:\ngit revert HEAD^\n\n# Undo changes by a commit identified by its checksum:\ngit revert e1c5739\n\n\nUndoing commits with git reset\ngit reset is quite complicated as it has three modes (--hard, --mixed (default), and --soft) and can act either on individual files and on entire commits. To undo a commit, and:\n\nStage all changes made by that commit:\n# Resetting to the 2nd-to-last commit (HEAD^) =&gt; undoing the last commit\ngit reset --soft HEAD^\nPut all changes made by that commit as uncomitted working-dir changes:\n# Note that '--mixed' is the default, so you could omit that\ngit reset --mixed HEAD^\nCompletely discard all changes made by that commit:\ngit reset --hard HEAD^ \n\n\n\n\n\n\n\ngit reset erases history\n\n\n\nUndoing with git revert is much safer than with git reset, because git revert does not erase any history.\nFor this reason, some argue you should not use git reset on commits altogether. At any rate, you should never use git reset for commits that have already been pushed online.\n\n\n\n\n\n\n4.3 Viewing & reverting to earlier versions of files\nAbove, you learned to undo at a project/commit-wide level. But you can also undo things for specific files:\n\nGet a specific version of a file from a past commit:\n# Retrieve the version of README.md from the second-to-last commit\ngit checkout HEAD^^ -- README.md\n# Or: Retrieve the version of README.md from a commit IDed by the checksum\ngit checkout e1c5739 -- README.md\nNow, your have the old version in the working dir & staged, which you can optionally check with:\n# Optional: check the file at the earlier state\ncat README.md\ngit status\nYou can go on to commit this version from the past, or go back to the current version, as we will do below:\ngit checkout HEAD -- README.md\n\n\n\n\n\n\n\nBe careful with git checkout\n\n\n\nBe careful with git checkout: any uncommitted changes to this file would be overwritten by the past version you retrieve!\n\n\n\nAn alternative method to view and revert to older versions of specific files is to use git show.\n\nView a file from any commit as follows:\n# Retrieve the version of README.md from the last commit\ngit show HEAD:README.md\n# Or: Retrieve the version of README.md from a commit IDed by the checksum\ngit show ad4ca74:README.md\nRevert a file to a previous version:\ngit show ad4ca74:README.md &gt; README.md"
  },
  {
    "objectID": "git/04_git3.html#miscellaneous-git",
    "href": "git/04_git3.html#miscellaneous-git",
    "title": "Git: Branching, collaborating, and undoing",
    "section": "5 Miscellaneous Git",
    "text": "5 Miscellaneous Git\n\n5.1 Amending commits\nLet’s say you forgot to add a file to a commit, or notice a silly typo in something we just committed. Creating a separate commit for this seems “wasteful” or even confusing, and including these changes along with others in a next commit is also likely to be inappropriate. In such cases, you can amend the previous commit.\nFirst, stage the forgotten or fixed file:\n# (NOTE: don't run this)\ngit add myfile.txt\nThen, amend the commit, adding --no-edit to indicate that you do not want change the commit message:\n# (NOTE: don't run this)\ngit commit --amend --no-edit\n\n\n\n\n\n\nAmending commits is a way of “changing history”\n\n\n\nBecause amending a commit “changes history”, some recommend avoiding this altogether. For sure, do not amend commits that have been published in (pushed to) the online counterpart of the repo.\n\n\n\n\n\n5.2 git stash\nGit stash can be useful when you need to pull from the remote, but have changes in your working dir that:\n\nAre not appropriate for a separate commit\nAre not worth starting a new branch for\n\nHere is an example of the sequence of commands you can use in such cases.\n\nStash changes to tracked files with git stash:\n# (Note: add option '-u' if you need to include untracked files) \ngit stash\nPull from the remote repository:\ngit pull\n“Apply” (recover) the stashed changes back to your working dir:\ngit stash apply\n\n\n\n\n5.3 A few more tips\n\nGit will not pay attention to empty directories in your working dir.\nYou can create a new branch and move to it in one go using:\ngit checkout -b &lt;new-branch-name&gt;\nTo show commits in which a specific file was changed, you can simply use:\ngit log &lt;filename&gt;\n“Aliases” (command shortcuts) can be useful with Git, and can be added in two ways:\n\nBy adding lines like the below to the ~/.gitconfig file:\n[alias]\n  hist = log --graph --pretty=format:'%h %ad | %s%d [%an]' --date=short\n  last = log -1 HEAD  # Just show the last commit\nWith the git config command:\ngit config --global alias.last \"log -1 HEAD\""
  },
  {
    "objectID": "nf-run/01_pipelines.html#overview",
    "href": "nf-run/01_pipelines.html#overview",
    "title": "Runner scripts and pipelines",
    "section": "Overview",
    "text": "Overview\nIn this tutorial, you will see different ways to write and run analysis pipelines. W’ll talk about “runner scripts” and you’ll get introduced to formal pipelines. In the next tutorial, we’ll run a best-practice Nextflow pipeline for RNA-seq analysis.\nWhile thinking about different ways to organize and run your analyses, let’s have an example in mind with a simple RNA-seq analysis that:\n\nTrims reads in FASTQ files (independently for each sample)\nMaps (aligns) reads to a reference genome (independently for each sample)\nCreates a gene expression count table from the alignments (for all samples together)\n\n\n\n\nA simple 3-step RNA-seq analysis, with arrows showing the connectionsbetween the steps: the output of one step is the input of the next.The example only has 3 samples, and the first two steps are executed separatelyfor each sample, while the last step is executed once for all samples."
  },
  {
    "objectID": "nf-run/01_pipelines.html#more-on-runner-scripts",
    "href": "nf-run/01_pipelines.html#more-on-runner-scripts",
    "title": "Runner scripts and pipelines",
    "section": "1 More on runner scripts",
    "text": "1 More on runner scripts\nI will refer to “Runner scripts” as digital notebook-like scripts that contain your analysis workflow, and that you run line-by-line to submit your “primary scripts” as batch jobs.\nThese runner scripts are useful to store that code instead of typing it directly in the terminal, and benefit efficiency and reproducibility to make it easier to rerun your analysis.\nIn the context of a research project that would include running a series of steps with different bioinformatics tools (and perhaps custom scripts), the idea would be to include all these steps in such a runner script. For example, for the above-mentioned RNA-seq analysis, such a runner script could look like so:\n# [hypothetical example - don't run this]\n# Define the inputs\nfastq_dir=data/fastq\nref_assembly=data/ref/assembly.fna\nref_annotation=data/ref/annotation.gtf\n\n# Trim the reads:\nfor R1 in \"$fastq_dir\"/*_R1.fastq.gz; do\n    # (The trim.sh script takes 2 arguments: R1 FASTQ and output dir)\n    sbatch scripts/trim.sh \"$R1\" results/trim\ndone\n\n# Align (map) the reads to a reference genome assembly:\nfor R1 in results/trim/*_R1.fastq.gz; do\n    # (The map.sh script takes 3 arguments: R1 FASTQ, ref. assembly, and output dir)\n    sbatch scripts/map.sh \"$R1\" \"$ref_assembly\" results/map\ndone\n\n# Count alignments per sample per gene using the annotation:\n# (The count.sh script takes 3 arguments: input dir, ref. annotation, and output dir)\nsbatch scripts/count.sh results/map \"$ref_annotation\" results/count_table.txt\nThe code above runs a primary shell script for each of the three steps (trim.sh, map.sh, andcount.sh): each of these takes arguments and runs a bioinformatics tool to perform that step (for example, TrimGalore for trimming).\nHere are some advantages of using such a script structure with flexible (i.e., argument-accepting) primary scripts, and an overarching runner script:\n\nRerunning everything, including with a modified sample set, or tool settings, is relatively straightforward — both for yourself and others, improving reproducibility.\nRe-applying the same set of analyses in a different project is straightforward.\nThe runner script is a form of documentation of all steps taken.\nIt (more or less) ensures you are including all necessary steps."
  },
  {
    "objectID": "nf-run/01_pipelines.html#pipelines",
    "href": "nf-run/01_pipelines.html#pipelines",
    "title": "Runner scripts and pipelines",
    "section": "2 Pipelines",
    "text": "2 Pipelines\nWhat exactly do we mean by a “pipeline”? We may informally refer to any consecutive series of analysis steps as a pipeline. The above runner script, in particular, can informally be called a pipeline. But here, I am using pipeline in a stricter sense to mean a series of steps that can be executed from start to finish with a single command.\nAn advantage of a true pipeline is increased automation, as well as “supercharging” all the above-mentioned advantages of runner script. For example, a pipeline truly ensures that you are including all necessary steps.\n\n\n\nBut wait, can we not just run our runner script with a single command: bash run/run.sh? (Click to expand)\n\nThis doesn’t work because we are submitting batch jobs in each step. Because the script would continue to the next line/submission immediately after the previous lines, all jobs would effectively be submitted at the same time, and e.g. the mapping script would fail because the trimmed reads it needs are not yet there. (Below, we’ll briefly talk about ways around this problem.)\n\n\nTo turn our runner script into a pipeline, we would need to overcome the problem of simultaneous batch job submission. Additionally, a pipeline worth its salt should also be able to detect and stop upon failure, and to rerun parts of the pipeline flexibly. The latter may be necessary after, e.g.:\n\nSome scripts failed for all or some samples\nYou added or removed a sample\nYou had to modify a script or settings somewhere halfway the pipeline.\n\nSo how could we implement all of that?\n\nPush the limits of the Bash and Slurm tool set\nUse if statements, many script arguments, and Slurm “job dependencies” (see the box below) — but this is hard to manage for more complex workflows. Alternatively, if you only want to solve the simultaneous batch job problem, you can put all steps in a single script, but this would make for a very inefficient pipeline.\nUse a formal workflow management system.\nWe’ll talk about these some more below.\n\n\n\n\n\n\n\n\nPushing the limits of the Bash and Slurm tool set (Click to expand)\n\n\n\n\n\nFirst, here are some low-tech, ad-hoc solutions to rerunning parts of the workflow:\n\nComment out part of the workflow — e.g., to skip a step:\n# Trim:\n#for R1 in data/fastq/*_R1.fastq.gz; do\n#    sbatch scripts/trim.sh \"$R1\" results/trim\n#done\n\n# Align (map):\nfor R1 in results/trim/*_R1.fastq.gz; do\n    sbatch scripts/map.sh \"$R1\" results/map\ndone\n\n# Count alignments per sample per gene:\nsbatch scripts/count.sh results/map results/count_table.txt\nMake temporary changes — e.g., to only run a single added sample:\n# Trim:\n#for R1 in data/fastq/*_R1.fastq.gz; do\n    R1=data/fastq/newsample_R1.fastq.gz\n    sbatch scripts/trim.sh \"$R1\" results/trim\n#done\n\n# Align (map):\n#for R1 in results/trim/*_R1.fastq.gz; do\n    R1=results/trim/newsample_R1.fastq.gz\n    sbatch scripts/map.sh \"$R1\" results/map\n#done\n\n# Count - NOTE, this steps should be rerun as a whole:\nsbatch scripts/count.sh results/map results/count_table.txt\n\n\nSecond, here are some more bespoke code-based solutions:\n\nCommand-line options and if-statements to flexibly run part of the pipeline (and perhaps change settings):\ntrim=$1   # true or false\nmap=$2    # true or false\ncount=$3  # true or false\n\nif [[ \"$trim\" == true ]]; then\n    for R1 in data/fastq/*_R1.fastq.gz; do\n        bash scripts/trim.sh \"$R1\" results/trim\n    done\nfi\n\nif [[ \"$map\" == true ]]; then\n    for R1 in results/trim/*_R1.fastq.gz; do\n        bash scripts/map.sh \"$R1\" results/map\n    done\nfi\n\nif [[ \"$count\" == true ]]; then\n    bash scripts/count.sh results/map results/count_table.txt\nfi\nSlurm job dependencies — in the example below, jobs will only start after their “dependencies” (jobs whose outputs they need) have finished:\nfor R1 in data/fastq/*_R1.fastq.gz; do\n    # Submit the trimming job and store its job number:\n    submit_line=$(sbatch scripts/trim.sh \"$R1\" results/trim)\n    trim_id=$(echo \"$submit_line\" | sed 's/Submitted batch job //')\n\n    # Submit the mapping job with the condition that it only starts when the\n    # trimming job is done, using '--dependency=afterok:':\n    R1_trimmed=results/trim/$(basename \"$R1\")\n    sbatch --dependency=afterok:$trim_id scripts/map.sh \"$R1_trimmed\" results/map\ndone\n\n# If you give the mapping and counting jobs the same name with `#SBATCH --job-name=`,\n# then you can use '--dependency=singleton': the counting job will only start\n# when ALL the mapping jobs are done:\nsbatch --dependency=singleton scripts/count.sh results/map results/count_table.txt\n\n\n\n\n\n\n\n\n\n\nBackground reading on the need for pipelines (Click to expand)\n\n\n\n\n\n\n\n\nPerkel 2019 - https://www.nature.com/articles/d41586-019-02619-z\n\n\nTwo quotes from this article:\n\nTypically, researchers codify workflows using general scripting languages such as Python or Bash. But these often lack the necessary flexibility.  Workflows can involve hundreds to thousands of data files; a pipeline must be able to monitor their progress and exit gracefully if any step fails. And pipelines must be smart enough to work out which tasks need to be re-executed and which do not."
  },
  {
    "objectID": "nf-run/01_pipelines.html#workflow-management-systems",
    "href": "nf-run/01_pipelines.html#workflow-management-systems",
    "title": "Runner scripts and pipelines",
    "section": "3 Workflow management systems",
    "text": "3 Workflow management systems\nPipeline/workflow tools, often called “workflow management systems” in full, provide ways to formally describe and execute pipelines. Advantages of these tools are improved automation, flexibility, portability, and scalability1.\n\nAutomation\n\nDetect & rerun upon changes in input files and failed steps.\nAutomate Slurm job submissions.\nIntegration with software management.\nEasily run for other data sets.\n\n\n\n\nFlexibility, portability, and scalability\nThis is due to these tools separating generic pipeline nuts-and-bolts from the following two aspects:\n\nRun-specific configuration — samples, directories, settings/parameters.\nThings specific to the run-time environment (laptop vs. cluster vs. cloud).\n\n\nThe two most commonly used command-line based options in bioinformatics are Nextflow and Snakemake. Both have their pros and cons, but we’ll focus on Nextflow here.\n\nLearn to write pipelines?\nMost workflow tools are small “domain-specific” languages (DSLs), often a sort of extension of a more general language: for example, Python for Snakemake, and Groovy/Java for Nextflow.\nLearning one of these tools is harder than it should be, in my opinion — a truly excellent workflow tool does not yet exist, and may not appear in the near-future either because existing options have become entrenched. Therefore, learning to write your own pipelines with one of them is probably only worth it if you plan to regularly work on genomics/bioinformatics projects.\nIf you decide not to do this, I recommend that you instead use runner scripts.\nEither way, for many kinds of omics data, it is also possible (and a great idea) to use publicly available pipelines written with one of these workflow tools, and we’ll practice with that next."
  },
  {
    "objectID": "nf-run/01_pipelines.html#nf-core-pipelines",
    "href": "nf-run/01_pipelines.html#nf-core-pipelines",
    "title": "Runner scripts and pipelines",
    "section": "4 nf-core pipelines",
    "text": "4 nf-core pipelines\nAmong workflow tools, Nextflow has by far the best ecosystem of publicly available pipelines. The “nf-core” initiative (https://nf-co.re, Ewels et al. 2020) curates a set of best-practice, flexible, and well-documented pipelines written in Nextflow:\n\n\n\n\n\n\nFor many common omics analysis types, nf-core has a pipeline. It currently has 58 complete pipelines — these are the four most popular ones:\n\n\n\n\n\nLet’s take a closer look at the most widely used one, the rnaseq pipeline, which we’ll run in the next session:\n\n\n\n\n\nThere is often a bewildering array of bioinformatics programs for a given type of analysis, and it can be very hard and time-consuming to figure out what you should use. An additional advantage of using an nf-core (or similar) pipeline is that you can be confident that it uses a good if not optimal combination of tools and tool settings, since most of these pipelines have been developed over years by many experts in the field, and are also continuously updated."
  },
  {
    "objectID": "nf-run/01_pipelines.html#footnotes",
    "href": "nf-run/01_pipelines.html#footnotes",
    "title": "Runner scripts and pipelines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThat’s a lot of big words!↩︎"
  },
  {
    "objectID": "rnaseq/04_fastqc.html",
    "href": "rnaseq/04_fastqc.html",
    "title": "Read QC with FastQC",
    "section": "",
    "text": "Under construction\n\n\n\nThis page is still under construction."
  },
  {
    "objectID": "rnaseq/04_fastqc.html#overview-setting-up",
    "href": "rnaseq/04_fastqc.html#overview-setting-up",
    "title": "Read QC with FastQC",
    "section": "Overview & setting up",
    "text": "Overview & setting up\n\n\n\n\n\n\nVS Code improvements\n\n\n\nThese two settings will make life easier when writing shell scripts in VS Code.\nFirst, we’ll add a keyboard shortcut to send code from your editor to the terminal. This is the same type of behavior that you may be familiar with from RStudio, and will prevent you from having to copy-and-paste code into the terminal:\n\nClick the  (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter. (Don’t worry about the warning that other bindings exist for this shortcut.)\n\nIn VS Code’s editor pane, the entire line that your cursor is on is always selected by default. As such, your keyboard shortcut will by default send the line that your cursor is in to the terminal; you can also send multiple lines to the terminal after selecting them.\nSecond, we’ll add the ShellCheck VS Code extension. This extension will check your shell scripts for errors like referencing variables that have not been assigned, and not using variables that have been assigned. Potential problems will show up as colored squiggly lines below the words or lines in question. You can also click on the links that will appear when you hover over a problematic piece of code, and find information about how to fix this mistake and improve your code. All in all, this extension is incredibly useful!\n\nClick on the Extensions icon in the far left (narrow) sidebar in VS Code.\nType “shellcheck” and click the small purple “Install” button next to the entry of this name (the description should include “Timon Wong”, who is the author).\n\n\n\n\nFastQC: A program for quality control of FASTQ files\nFastQC is one the most ubiquitous pieces of genomics software. It allows you to assess the overall quality of, and potential problems with, the reads in your FASTQ files. It produces visualizations and assessments of for statistics such as per-base quality (below) and adapter content. Running FastQC or an equivalent program should always be the first analysis step after you receive your sequences.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser and which has about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: A FastQC per-base quality score graph for files with fairly good (left) and very poor (right) quality reads. The y-axis shows Phred quality scores (higher is better, see also the color-coding of the graph) and the x-axis shows the position along the read.\n\n\n\n\n\nStart VS Code and open your folder\nAs always, we’ll be working in VS Code — if you don’t already have a session open, see below how to do so.\nMake sure to open your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir, either by using the Open Folder menu item, or by clicking on this dir when it appears in the Welcome tab.\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect an appropriate OSC project (here: PAS0471)\nFor this session, select /fs/ess/PAS0471 as the starting directory\nMake sure that Number of hours is at least 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\n\n\n\n\n\n\n\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal. (Or use one of the keyboard shortcuts: Ctrl+` (backtick) or Ctrl+Shift+C.)\nIn the Welcome tab under Recent, you should see your /fs/ess/PAS0471/&lt;user&gt;/rnaseq_intro dir listed: click on that to open it. Alternatively, use      =&gt;   File   =&gt;   Open Folder to open that dir in VS Code.\n\n\n\n\n\n\n\n\n\n\nDon’t have your own dir with the data? (Click to expand)\n\n\n\n\n\nIf you missed the last session, or deleted your rnaseq_intro dir entirely, run these commands to get a (fresh) copy of all files you should have so far:\nmkdir -p /fs/ess/PAS0471/$USER/rnaseq_intro\ncp -r /fs/ess/PAS0471/demo/202307_rnaseq /fs/ess/PAS0471/$USER/rnaseq_intro\nAnd if you do have an rnaseq_intro dir, but you want to start over because you moved or removed some of the files while practicing, then delete the dir before your run the commands above:\nrm -r /fs/ess/PAS0471/$USER/rnaseq_intro\nYou should have at least the following files in this dir:\n/fs/ess/PAS0471/demo/202307_rnaseq\n├── data\n│   └── fastq\n│       ├── ASPC1_A178V_R1.fastq.gz\n│       ├── ASPC1_A178V_R2.fastq.gz\n│       ├── ASPC1_G31V_R1.fastq.gz\n│       ├── ASPC1_G31V_R2.fastq.gz\n│       ├── md5sums.txt\n│       ├── Miapaca2_A178V_R1.fastq.gz\n│       ├── Miapaca2_A178V_R2.fastq.gz\n│       ├── Miapaca2_G31V_R1.fastq.gz\n│       └── Miapaca2_G31V_R2.fastq.gz\n├── metadata\n│   └── meta.tsv\n└── README.md\n│   └── ref\n│       ├── GCF_000001405.40.fna\n│       ├── GCF_000001405.40.gtf"
  },
  {
    "objectID": "rnaseq/04_fastqc.html#a-script-to-run-fastqc",
    "href": "rnaseq/04_fastqc.html#a-script-to-run-fastqc",
    "title": "Read QC with FastQC",
    "section": "1 A script to run FastQC",
    "text": "1 A script to run FastQC\n\n1.1 FastQC syntax\nTo analyze one (optionally gzipped) FASTQ file with FastQC, the syntax can be as simple as:\n\nfastqc &lt;fastq-file&gt;\n\nAbove, &lt;fastq-file&gt; should be replaced by the path to an actual FASTQ file. We’ll also always want to specify the output directory, though, because the unfortunate default for FastQC is to put them in the directory that contains the FASTQ files themselves1. We can tell FastQC about our desired output directory as follows:\n\nfastqc --outdir &lt;output-dir&gt; &lt;fastq-file&gt;\n\nFor instance, if we wanted output files to go to the directory results/fastqc and wanted the program to analyze the file data/fastq/ASPC1_A178V_R1.fastq.gz, a functional command would be:\n\nfastqc --outdir results/fastqc data/fastq/ASPC1_A178V_R1.fastq.gz\n\n\n\n\n\n\n\nFastQC’s output file names are automatically determined\n\n\n\nFastQC allows us to specify the output directory, but not the output file names, which will be automatically determined based on the input file name.\nFor one FASTQ file, FastQC will output one HTML file and one ZIP archive. The latter contains files with the summary statistics that were computed and on which the figures are based — we generally don’t need to look at that.\n\n\n\n\n\n1.2 A basic script to run FastQC\nInstead of running FastQC interactively, we’ll want to write a FastQC script that we can submit as a batch job.\nSpecifically, our script will deliberately run FastQC for only one FASTQ file. Alternative approaches would be to include multiple FASTQ files in our FastQC command (this is possible), or even to loop over FASTQ files inside the FastQC script. However, given that we have access to OSC’s compute cluster, it will be much more efficient to submit a separate batch job for each FASTQ file.\nThis approach means that our script needs to accept an argument with a file name (of the focal FASTQ file), something that we have practiced with quite a bit in the previous sessions. So here is what a basic script along these lines could look like:\n#!/bin/bash\n\n# Strict Bash settings\nset -euo pipefail\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\n\n# (Don't run this in your terminal, this is an example script)\n\n\n\n1.3 A more well-developed FastQC script\nWe should add a few things to this script to e.g. make it run it smoothly as a batch job at OSC:\n\nA line to load the relevant OSC software module:\n\nmodule load fastqc/0.11.8\n\nA few sbatch options (we’ll keep the time limit and number of cores at their default values of 1 hour and 1 core, respectively):\n\n#SBATCH --account=PAS0471\n#SBATCH --mail-type=FAIL\n#SBATCH --output=slurm-fastqc-%j.out\n\nSeveral echo statements to report what’s going on\nA line to create the output directory if it doesn’t yet exist:\nmkdir -p \"$outdir\"\n\n\n\n\n\n\n\nRefresher: the -p option to mkdir (Click to expand)\n\n\n\n\n\nUsing the -p option does two things at once for us, both of which are necessary for a foolproof inclusion of this command in a script:\n\nIt will enable mkdir to create multiple levels of directories at once (i.e., to act recursively): by default, mkdir errors out if the parent directory/ies of the specified directory don’t yet exist.\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: No such file or directory\nmkdir -p newdir1/newdir2    # This successfully creates both directories\nIf the directory already exists, it won’t do anything and won’t return an error (by default, mkdir would return an error in this case, which would in turn lead the script to abort at that point with our set settings):\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: File exists\nmkdir -p newdir1/newdir2   # This does nothing since the dirs already exist\n\n\n\n\nHere is what our script looks like with those additions:\n\n#!/bin/bash\n#SBATCH --account=PAS2250\n#SBATCH --mail-type=FAIL\n#SBATCH --output=slurm-fastqc-%j.out\n  \n# Strict Bash settings\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=\"$1\"\noutdir=\"$2\" \n\n# Initial reporting\necho \"# Starting script fastqc.ch\"\ndate\necho \"# Input FASTQ file:   $fastq_file\"\necho \"# Output dir:         $outdir\"\necho\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir=\"$outdir\" \"$fastq_file\"\n\n# Final reporting\necho\necho \"# Listing the output files:\"\nls -lh \"$outdir\"\n\necho\necho \"# Done with script fastqc.sh\"\ndate\n\n# (Don't run this in your terminal, but copy it into a .sh text file)\n\n Open a new file in VS Code (     =&gt;   File   =&gt;   New File) and save it as fastqc.sh within your scripts/ directory. Paste in the code above and save the file.\nNotice that this script is very similar to our toy scripts from the previous sessions: mostly standard (“boilerplate”) code with just a single command to run our program of interest. Therefore, you can adopt this script as a template for scripts that run other command-line programs, and will generally only need minor modifications!\n\n\nOn Your Own: Use multiple threads\nMost bioinformatics programs, including FastQC, can make use of multiple threads/CPUs/cores (which we can all treat as the same unit below the node level, for our purposes), and this can speed things up tremendously.\nTo run FastQC with multiple threads, we need to take two steps — below, n is the number of threads that we would like to use.\n\nAdd the #SBATCH --cpus-per-task=n option to the script.\nTell FastQC that it can use n threads.\n\nInclude both of these options in your fastqc.sh script so as to run FastQC with 8 cores.\n(To find out the name of the FastQC option for the number of threads, run fastqc --help and search for the relevant option.)\n\n\nHint (click here)\n\nThe FastQC option in question is -t (short form) or --threads (long form). For clarity, I would suggest to use the long form option in your script.\n\n\n\nSolution (click here)\n\n\nYou should add the following #SBATCH line at the top of the script:\n\n#SBATCH --cpus-per-task=8\n\nYour FastQC command in the script should now be as follows (though the order of the --threads and --outdir options does not matter, as long as the input file positional argument comes last):\n\nfastqc --threads 8 --outdir \"$outdir\" \"$fastq_file\""
  },
  {
    "objectID": "rnaseq/04_fastqc.html#a-master-runner-script",
    "href": "rnaseq/04_fastqc.html#a-master-runner-script",
    "title": "Read QC with FastQC",
    "section": "2 A master / runner “script”",
    "text": "2 A master / runner “script”\nAbove, we created a fastqc.sh script, which we’ll eventually want to submit a bunch of times with a for loop. The code with that loop and the sbatch command could be directly typed in the terminal. But it’s better to save the commands used for job submission in a file/script as well.\nWe will now create such a file, which has the overall purpose of documenting the steps we took and the batch jobs we submitted. You can think of this file as your analysis lab notebook, or perhaps more accurately, your notebook entry that contains the final protocol you followed.\nThis kind of script is sometimes called a “master” or “runner” script. Because it will contain shell code, we will save it as a shell script (.sh) just like the script to run fastqc.sh and other individual analysis steps. However, it is important to realize that the runner script is conceptually different from the scripts that run individual steps of your analysis. The latter are meant to be run/submitted in their entirety by the runner script, whereas a basic runner script that contains sbatch compute job commands for multiple steps has to be run step-by-step (see the box below).\n\n\n\n\n\n\nThe runner script can’t itself be run at once in its entirety (Click to expand)\n\n\n\n\n\nOnce we’ve added multiple batch job steps, and the input of a later step uses the output of an earlier step, we won’t be able to just run the script as is. This is because the runner script would then submit jobs from different steps all at once, and that later step would start running before the earlier step has finished.\nFor example, consider the following series of two steps, in which the second step uses the output of the first step:\n# This script would create a genome \"index\" for STAR, that will be used in the next step\n# ('my_genome.fa' = input genome FASTA, 'results/star_index' = output index dir)\nsbatch scripts/star_index.sh my_genome.fa results/star_index\n\n# This script would align a FASTQ file to the genome index created in the previous step\n# ('results/star_index' = input index dir, 'sampleA.fastq.gz' = input FASTQ file,\n# 'results/star_align' = output dir)\nsbatch scripts/star_align.sh results/star_index sampleA.fastq.gz results/star_align \nIf these two lines were included in your runner script, and you would run that script in its entirety all at once, the script in the second step would be submitted just a split-second after the first one (recall: when using sbatch, you get your prompt back immediately – there is no waiting). As such, it would fail because of the missing output from the first step.\nIt is possible to make sbatch batch jobs wait for earlier steps to finish (e.g. with the --dependency option), but this quickly gets tricky. If you want to create a workflow/pipeline that can run from start to finish in an automated way, you should consider using a workflow management system like Snakemake or NextFlow.\n\n\n\nTo summarize, we’ll separate our code into two hierarchical levels of scripts, which we’ll also save in separate dirs to make this division clear:\n\nThe scripts that run individual steps of your analysis, like fastqc.sh. We’ll save these in a directory called scripts.\nAn overarching “runner” script that orchestrates the batch job submission of these individual steps. We’ll save this script in a directory called run.\n\n Let’s go ahead and open a new text file, and save it as run/run.sh (VS Code should create that directory on the fly as needed).\n\n\n\n\n\n\nKeep the scripts for individual steps simple\n\n\n\nIt is a good idea to keep the shell scripts you will submit (e.g., fastqc.sh) simple in the sense that they should generally just run one program, and not a sequence of programs.\nOnce you get the hang of writing these scripts, it may seem appealing to string a series of programs/steps together in a single script, so that it’s easier to rerun everything at once — but in practice, that will often end up leading to more difficulties than convenience. Once again, if you do want to develop a workflow that can run from start to finish, you’ll have to bite the bullet and learn a workflow management system like Snakemake or Nextflow."
  },
  {
    "objectID": "rnaseq/04_fastqc.html#running-fastqc-using-batch-jobs",
    "href": "rnaseq/04_fastqc.html#running-fastqc-using-batch-jobs",
    "title": "Read QC with FastQC",
    "section": "3 Running FastQC using batch jobs",
    "text": "3 Running FastQC using batch jobs\n\n3.1 Submitting the script for one FASTQ file\nLet’s submit our fastqc.sh script to the Slurm queue with sbatch:\nsbatch scripts/fastqc.sh data/fastq/ASPC1_A178V_R1.fastq.gz results/fastqc\nSubmitted batch job 12521308\n\n\n\n\n\n\nOnce again: Where does our output go? (Click to expand)\n\n\n\n\n\n\nOutput that would have been printed to screen if we had run the script directly, such as our echo statements and FastQC’s progress logging, will go into the Slurm log file slurm-fastqc-&lt;job-nr&gt;.out in our working dir.\nFastQC’s main output files (HTML and zip) will end up in the output directory we specified, in this case results/fastqc.\n\n\n\n\n\nIf we take a look at the queue, you may catch the job while it’s still pending (note below that the job’s NAME will by default be the filename of the script):\nFri Aug 25 12:07:48 2023\n    JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n  23666218 serial-40 fastqc.s   jelmer  PENDING       0:00   1:00:00      1 (None)\n…and then it should start running:\nFri Aug 25 12:07:54 2023\n    JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n  23666218 condo-osu fastqc.s   jelmer  RUNNING       0:06   1:00:00      1 p0133\nThe job will be finished within 10 seconds, though (recall that we are working with small subsets of the full FASTQ files), and you might miss its listing in the squeue output entirely: as soon as it’s done, it will be removed from the list.\n\nOf course, just because a job has finished does not mean that it has ran successfully, and we should always check this. Let’s start by taking a look at the Slurm log file:\n\ncat slurm-fastqc-23666218.out    # You'll have a different job number in the filename\n\n\n\n\n\n\n\nClick to see the contents of the Slurm log file\n\n\n\n\n\n# Starting script fastqc.ch\nFri Aug 25 12:07:50 EDT 2023\n# Input FASTQ file:   data/fastq/ASPC1_A178V_R1.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ASPC1_A178V_R1.fastq.gz\nApprox 5% complete for ASPC1_A178V_R1.fastq.gz\nApprox 10% complete for ASPC1_A178V_R1.fastq.gz\nApprox 15% complete for ASPC1_A178V_R1.fastq.gz\nApprox 20% complete for ASPC1_A178V_R1.fastq.gz\nApprox 25% complete for ASPC1_A178V_R1.fastq.gz\nApprox 30% complete for ASPC1_A178V_R1.fastq.gz\nApprox 35% complete for ASPC1_A178V_R1.fastq.gz\nApprox 40% complete for ASPC1_A178V_R1.fastq.gz\nApprox 45% complete for ASPC1_A178V_R1.fastq.gz\nApprox 50% complete for ASPC1_A178V_R1.fastq.gz\nApprox 55% complete for ASPC1_A178V_R1.fastq.gz\nApprox 60% complete for ASPC1_A178V_R1.fastq.gz\nApprox 65% complete for ASPC1_A178V_R1.fastq.gz\nApprox 70% complete for ASPC1_A178V_R1.fastq.gz\nApprox 75% complete for ASPC1_A178V_R1.fastq.gz\nApprox 80% complete for ASPC1_A178V_R1.fastq.gz\nApprox 85% complete for ASPC1_A178V_R1.fastq.gz\nApprox 90% complete for ASPC1_A178V_R1.fastq.gz\nApprox 95% complete for ASPC1_A178V_R1.fastq.gz\nApprox 100% complete for ASPC1_A178V_R1.fastq.gz\nAnalysis complete for ASPC1_A178V_R1.fastq.gz\n\n# Listing the output files:\ntotal 5.1M\n-rw-r--r-- 1 jelmer PAS0471 266K Aug 25 12:07 ASPC1_A178V_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 456K Aug 25 12:07 ASPC1_A178V_R1_fastqc.zip\n\n# Done with script fastqc.sh\nFri Aug 25 12:07:56 EDT 2023\n\n\n\nThe Slurm log file shown in the box above looks good, we can see that FastQC ran and finished and that there are no errors. Make sure your log file looks similar.\nAt the end of the log file, our script listed FastQC’s output files (a ZIP file and an HTML file), so we could see their file names and sizes: another useful check that everything went well.\n\n\n\n3.2 Submitting the script many times with a loop\nThe script that we wrote above will run FastQC for a single FASTQ file. Now, we will write a loop that iterates over all of our FASTQ files (only 8 files in our case, but this could be 100s of files just the same), and submits a batch job for each of them.\nIn our run.sh script, let’s start by writing a loop that iterates over our FASTQ files and simply prints their names (note: the -e option to echo will allow us to insert an extra new line with \\n, resulting in an empty line):\nfor fastq_file in data/fastq/*fastq.gz; do\n    echo -e \"\\nFASTQ file: $fastq_file\"\ndone\n\nFASTQ file: data/fastq/ASPC1_A178V_R1.fastq.gz\n\nFASTQ file: data/fastq/ASPC1_A178V_R2.fastq.gz\n\nFASTQ file: data/fastq/ASPC1_G31V_R1.fastq.gz\n\nFASTQ file: data/fastq/ASPC1_G31V_R2.fastq.gz\n\nFASTQ file: data/fastq/Miapaca2_A178V_R1.fastq.gz\n\nFASTQ file: data/fastq/Miapaca2_A178V_R2.fastq.gz\n\nFASTQ file: data/fastq/Miapaca2_G31V_R1.fastq.gz\n\nFASTQ file: data/fastq/Miapaca2_G31V_R2.fastq.gz\nNow that we’ve confirmed that we are succesfully looping over our files, let’s add the code to submit a batch job in every iteration:\nfor fastq_file in data/fastq/*fastq.gz; do\n    echo -e \"\\nFASTQ file: $fastq_file\"\n    sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\nFASTQ file: data/fastq/ASPC1_A178V_R1.fastq.gz\nSubmitted batch job 24048031\n\nFASTQ file: data/fastq/ASPC1_A178V_R2.fastq.gz\nSubmitted batch job 24048032\n\nFASTQ file: data/fastq/ASPC1_G31V_R1.fastq.gz\nSubmitted batch job 24048033\n\nFASTQ file: data/fastq/ASPC1_G31V_R2.fastq.gz\nSubmitted batch job 24048034\n\nFASTQ file: data/fastq/Miapaca2_A178V_R1.fastq.gz\nSubmitted batch job 24048035\n\nFASTQ file: data/fastq/Miapaca2_A178V_R2.fastq.gz\nSubmitted batch job 24048036\n\nFASTQ file: data/fastq/Miapaca2_G31V_R1.fastq.gz\nSubmitted batch job 24048037\n\nFASTQ file: data/fastq/Miapaca2_G31V_R2.fastq.gz\nSubmitted batch job 24048038\n\n\nOn Your Own: Check if everything went well\n\nUse squeue to monitor your jobs.\nTake a look at the Slurm log files while the jobs are running and/or after the jobs are finished.\nA nice trick when you have many log files is to check the last few lines of all of them using tail with a wildcard. This is useful because recall that with our strict Bash settings, a script should only run until the end if it did not encounter errors. And tail will helpfully include file name markers when you run it on multiple files as follows:\ntail slurm-fastqc*\nTake a look at FastQC’s output files: are you seeing 8 HTML files?\nCheck your email to see that you didn’t receive any emails from Slurm: any emails would mean that the job(s) in question failed."
  },
  {
    "objectID": "rnaseq/04_fastqc.html#interpreting-the-fastqc-output",
    "href": "rnaseq/04_fastqc.html#interpreting-the-fastqc-output",
    "title": "Read QC with FastQC",
    "section": "4 Interpreting the FastQC output",
    "text": "4 Interpreting the FastQC output\n\n4.1 Downloading the HTML files\nIn the older version of VS Code that’s installed at OSC, we unfortunately can’t view HTML files. So we’ll have to download FastQC’s HTML output files to our own computers and then take a look at them.\nFind the results/fastqc dir in VS Code’s file Explorer on the left-hand side of the screen, right-click on it, and find and use the “Download…” entry towards the bottom (the screenshot just has a single file selected, but please select the dir instead):\n\n\n\nOnce you’ve downloaded the files, go to the folder you’ve downloaded them to in your computer’s file explorer / Finder.\nWe will look at just two of the HTML files: those for the R1 and R2 files of the first sample, ASPC1_A178V. It would be a bit tedious to have to go through all HTML files, and this would especially be the case if we have dozens of files, which would be the case for most full datasets. After looking at these two FastQC HTML files, we will use MultiQC to summarize all FastQC outputs into a single file, and examine the MultiQC output to look for differences among samples.\nIn your file explorer, double-click on the first file (ASPC1_A178V_R1_fastqc.html) and it should open inside your browser.\n\n\n4.2 Interpreting the results\nTBA – for now, see these slides."
  },
  {
    "objectID": "rnaseq/04_fastqc.html#footnotes",
    "href": "rnaseq/04_fastqc.html#footnotes",
    "title": "Read QC with FastQC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd we’d like to separate our data from our results!↩︎"
  },
  {
    "objectID": "rnaseq/03_DE.html",
    "href": "rnaseq/03_DE.html",
    "title": "Gene count table analysis",
    "section": "",
    "text": "After running steps like read pre-processing, alignment, and quantification using the nf-core rnaseq workflow, or another method, you will have a gene count table. In this tutorial, with that gene count table, you will:"
  },
  {
    "objectID": "rnaseq/03_DE.html#getting-set-up",
    "href": "rnaseq/03_DE.html#getting-set-up",
    "title": "Gene count table analysis",
    "section": "1 Getting set up",
    "text": "1 Getting set up\n\n1.1 Start an RStudio session at OSC\n\nLog in to OSC at https://ondemand.osc.edu\nClick on Interactive Apps (top bar) and then RStudio Server (all the way at the bottom)\nFill out the form as follows:\n\nCluster: Pitzer\nR version: 4.3.0\nProject: PAS2658\nNumber of hours: 3\nNode type: any\nNumber of cores: 2\n\nClick the big blue Launch button at the bottom.\nNow, you should be sent to a new page with a box at the top for your RStudio Server “job”, which should initially be “Queued” (waiting to start).\n\n\n\nClick to see a screenshot\n\n\n\n\nYour job should start running very soon, with the top bar of the box turning green and saying “Running”.\nClick Connect to RStudio Server at the bottom of the box, and an RStudio Server instance will open in a new browser tab. You’re ready to go!\n\n\n\n\n\n\n\n\nOptional: change two RStudio settings (Click to expand)\n\n\n\n\n\nFirst, prevent R from saving your “Workspace”:\n\nClick Tools (top bar, below your browser’s address bar) &gt; Global Options\nIn the pop-up window (stay on the General tab), change the settings under the “Workspace” heading to:\n\n\n\n\n\n\nWhy are we doing this? In short, the default behavior of saving and restoring your “Workspace”, which are all the items (objects) that you create during an R session, is bad practice. Instead, you should recreate your environment from a script and/or saved files with individual pieces of data, as we’ll do today.\n\nSecond, “update” your pipe symbol from %&gt;% 1 to |&gt; 2:\n\nAgain click Tools &gt; Global Options (you may still be there)\nNow go to Code tab in the side panel on the left, and check the box for Use native pipe operator, |&gt; (requires R 4.1+)\nClick OK at the bottom of the pop-up window\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 Create a new RStudio Project\nUsing an “RStudio Project” will most of all help to make sure your working directory in R is correct. To create a new RStudio Project inside your personal dir in /fs/scratch/PAS2658/&lt;your-name&gt;/Lab9:\n\nClick File (top bar, below your browser’s address bar) &gt; New Project\nIn the popup window, click Existing Directory.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\n\nClick Browse... to select your personal dir.\n\n\n\nClick to see a screenshot\n\n\n\n\n\n\n\n\nIn the next window, you should be in your Home directory (abbreviated as ~), from which you can’t click your way to /fs/scratch! Instead, you’ll first have to click on the (very small!) ... highlighted in the screenshot below:\n\n\n\n\n\n\n\nType at least part of the path to your dir in /fs/scratch/PAS2658, e.g. as shown below, and click OK:\n\n\n\n\n(This doesn’t show the correct OSC project but you get the idea.)\n\n\n\nNow you should be able to browse/click the rest of the way to your Lab9 dir.\nClick Choose to pick your selected directory.\nClick Create Project.\n\n\n\n\n1.3 Create an R script\nWe’re going to write all our code in an R script instead of typing it in the console. This helps us to keep track of what we’ve been doing, and enables us to re-run our code after modifying input data or one of the lines of code.\nCreate and open a new R script by clicking File (top menu bar) &gt; New File &gt; R Script. Save this new script right away by clicking File &gt; Save As, and save it with a name like scripts/DE.R (inside the Lab9 dir which should be automatically selected).\n\n\n\n\n\n\nMake sure to type all the R code below inside your script, and then send it to the console from there.\n\n\n\nYou can send code to the console by pressing Ctrl + Enter on Windows, or Cmd + Return on a Mac.\n\n\n\n\n\n1.4 Load the necessary packages\nIn R, we need to install and then use “packages” (basically, add-ons) to perform specialized tasks like differential expression analysis3. Installing packages is quite straightforward in principle, but in RStudio Server at OSC, there can be some hiccups.\nI have therefore created a “library” (a directory with a collection of packages) for you — you can load the packages from that library, without needing to install them yourself. Copy the code below into your R script and then send it to the R console:\n\n# First, we define the dir that has the custom library:\ndyn.load(\"/fs/ess/PAS0471/jelmer/software/GLPK/lib/libglpk.so.40\", local = FALSE)\ncustom_library &lt;- \"/fs/ess/PAS0471/jelmer/R/rnaseq\"\n.libPaths(custom_library)\n\n# Then, we load all needed R packages from that library:\nlibrary(tidyverse)          # Misc. data manipulation and plotting\nlibrary(pheatmap)           # Heatmap plot\nlibrary(EnhancedVolcano)    # Volcano plot\nlibrary(DESeq2)             # Differential expression analysis\n\n\n\nThis will produce output in the R console (a lot when loading DESeq2), and some of it in orange, but all should be good unless you see explicit errors at the bottom (Click to see expected output)\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nlibrary(pheatmap)\n\n\nlibrary(EnhancedVolcano)\n\nLoading required package: ggrepel\n\n\n\nlibrary(DESeq2)\n\nLoading required package: S4Vectors\n\n\nLoading required package: stats4\n\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    intersect, setdiff, union\n\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, table,\n    tapply, union, unique, unsplit, which.max, which.min\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    second, second&lt;-\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, rename\n\n\nThe following object is masked from 'package:tidyr':\n\n    expand\n\n\nThe following object is masked from 'package:utils':\n\n    findMatches\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\n\nAttaching package: 'IRanges'\n\n\nThe following object is masked from 'package:lubridate':\n\n    %within%\n\n\nThe following objects are masked from 'package:dplyr':\n\n    collapse, desc, slice\n\n\nThe following object is masked from 'package:purrr':\n\n    reduce\n\n\nLoading required package: GenomicRanges\n\n\nLoading required package: GenomeInfoDb\n\n\nLoading required package: SummarizedExperiment\n\n\nLoading required package: MatrixGenerics\n\n\nLoading required package: matrixStats\n\n\n\nAttaching package: 'matrixStats'\n\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\n\n\nAttaching package: 'MatrixGenerics'\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n    colWeightedMeans, colWeightedMedians, colWeightedSds,\n    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n    rowWeightedSds, rowWeightedVars\n\n\nLoading required package: Biobase\n\n\nWelcome to Bioconductor\n\n    Vignettes contain introductory material; view with\n    'browseVignettes()'. To cite Bioconductor, see\n    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\n\n\nAttaching package: 'Biobase'\n\n\nThe following object is masked from 'package:MatrixGenerics':\n\n    rowMedians\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    anyMissing, rowMedians\n\n\n\n\n\n\n1.5 Define our input files\nFor the differential expression analysis, we have the following input files:\n\nMetadata table — Metadata for the study, linking sample IDs to treatments\nGene count table — Produced by the nf-core rnaseq workflow\n\n\n# NOTE: here I am providing the path to my gene count table,\n#       but if you ran the workflow to completion, you can use your own.\n# We'll save the paths to our input files for later use\ncount_table_file &lt;- \"/fs/scratch/PAS2658/jelmer/share/results/salmon.merged.gene_counts_length_scaled.tsv\"\nmetadata_file &lt;- \"data/meta/metadata.tsv\""
  },
  {
    "objectID": "rnaseq/03_DE.html#create-a-deseq2-object",
    "href": "rnaseq/03_DE.html#create-a-deseq2-object",
    "title": "Gene count table analysis",
    "section": "2 Create a DESeq2 object",
    "text": "2 Create a DESeq2 object\nLike in the Culex paper whose data we are working with, we will perform a Principal Component Analysis (PCA) and a Differential Expression (DE) analysis using the popular DESeq2 package (paper, website).\nThe DESeq2 package has its own “object type” (a specific R format type) and before we can do anything else, we need to create a DESeq2 object from three components:\n\nMetadata\nOur independent variables should be in the metadata, allowing DESeq2 to compare groups of samples.\nCount table\nA matrix (table) with one row per gene, and one column per sample.\nA statistical design\nA statistical design formula (basically, which groups to compare) will tell DESEq2 how to analyze the data\n\n\n\n2.1 Metadata\nFirst, we’ll load the metadata file and take a look at the resulting data frame:\n\n# Read in the count table\nmeta_raw &lt;- read_tsv(metadata_file, show_col_types = FALSE)\n\n\n# Take a look at the first 6 rows\nhead(meta_raw)\n\n# A tibble: 6 × 3\n  sample_id   time  treatment  \n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;      \n1 ERR10802882 10dpi cathemerium\n2 ERR10802875 10dpi cathemerium\n3 ERR10802879 10dpi cathemerium\n4 ERR10802883 10dpi cathemerium\n5 ERR10802878 10dpi control    \n6 ERR10802884 10dpi control    \n\n\nWe’ll make sure the data frame is sorted by sample ID, and that the sample IDs are contained in “row names”:\n\n# Prepare the metadata so it can be loaded into DESeq2\nmeta &lt;- meta_raw |&gt;\n  # 1. Sort by the 'sample_id' column\n  arrange(sample_id) |&gt;\n  # 2. Turn the 'sample_id' column into row names:\n  column_to_rownames(\"sample_id\") |&gt;\n  # 3. Turn the 'time' and 'treatment' columns into \"factors\":\n  mutate(time = factor(time, levels = c(\"24hpi\", \"10dpi\")),\n         treatment = factor(treatment, levels = c(\"control\", \"cathemerium\", \"relictum\")))\n\n\n# Take a look at the first 6 rows\nhead(meta)\n\n             time   treatment\nERR10802863 24hpi     control\nERR10802864 24hpi cathemerium\nERR10802865 24hpi    relictum\nERR10802866 24hpi     control\nERR10802867 24hpi cathemerium\nERR10802868 24hpi    relictum\n\n\n\n\n\n\n\n\nIn the two outputs above, note the difference between having the sample IDs as a separate column versus as row names.\n\n\n\n\n\n\n\n\n\nFactors are a common R data type for categorical variables (Click to expand)\n\n\n\n\n\nWe changed the two independent variable columns (time and treatment) into factors, because DESEq2 wants this — this also allowed us to use a custom, non-alphanumeric ordering where 24hpi comes before 10dpi:\n\nhead(meta$time)\n\n[1] 24hpi 24hpi 24hpi 24hpi 24hpi 24hpi\nLevels: 24hpi 10dpi\n\n\n\n\n\n\n\n\n2.2 Gene count table\nSecond, load the gene count table into R:\n\n# Read in the count table\ncount_df &lt;- read_tsv(count_table_file, show_col_types = FALSE)\n\n\n# Take a look at the first 6 rows\nhead(count_df)\n\n# A tibble: 6 × 24\n  gene_id gene_name ERR10802863 ERR10802864 ERR10802865 ERR10802866 ERR10802867\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 ATP6    ATP6         10275.       8255.       4103.      18615.      11625.  \n2 ATP8    ATP8             3.85        2.92        2.33        7.76        7.01\n3 COX1    COX1         88041.      83394.      36975.     136054.     130863.  \n4 COX2    COX2          8749.       7925.       2901.      16802.      10026.  \n5 COX3    COX3         55772.      50312.      35074.      80510.      69850.  \n6 CYTB    CYTB         38543.      36352.      22185.      62147.      57461.  \n# ℹ 17 more variables: ERR10802868 &lt;dbl&gt;, ERR10802869 &lt;dbl&gt;, ERR10802870 &lt;dbl&gt;,\n#   ERR10802871 &lt;dbl&gt;, ERR10802874 &lt;dbl&gt;, ERR10802875 &lt;dbl&gt;, ERR10802876 &lt;dbl&gt;,\n#   ERR10802877 &lt;dbl&gt;, ERR10802878 &lt;dbl&gt;, ERR10802879 &lt;dbl&gt;, ERR10802880 &lt;dbl&gt;,\n#   ERR10802881 &lt;dbl&gt;, ERR10802882 &lt;dbl&gt;, ERR10802883 &lt;dbl&gt;, ERR10802884 &lt;dbl&gt;,\n#   ERR10802885 &lt;dbl&gt;, ERR10802886 &lt;dbl&gt;\n\n\nAgain, we have to make several modifications before we can include it in the DESeq2 object. DESeq2 expects with whole numbers (integers) and with gene IDs as row names:\n\n# Prepare the count table so it can be loaded into DESeq2\ncount_mat &lt;- count_df |&gt;\n  # 1. Turn the 'gene_id' column into row names:\n  column_to_rownames(\"gene_id\") |&gt;\n  # 2. Remove a remaining non-numeric column (which has gene names):\n  select(-gene_name) |&gt;\n  # 3. Round everything to whole numbers:\n  round() |&gt;\n  # 4. Convert it to a formal 'matrix' format:\n  as.matrix()\n\n\n# Take a look at the first 6 rows\nhead(count_mat)\n\n     ERR10802863 ERR10802864 ERR10802865 ERR10802866 ERR10802867 ERR10802868\nATP6       10275        8255        4103       18615       11625        7967\nATP8           4           3           2           8           7           2\nCOX1       88041       83394       36975      136054      130863       62279\nCOX2        8749        7925        2901       16802       10026        6701\nCOX3       55772       50312       35074       80510       69850       42478\nCYTB       38543       36352       22185       62147       57461       28159\n     ERR10802869 ERR10802870 ERR10802871 ERR10802874 ERR10802875 ERR10802876\nATP6       12788        4408       13648       13834        1346       10032\nATP8           2           0           2           1           3           2\nCOX1      109596      106402      104394       77682       38276       78290\nCOX2       11494        6603       11151        9893        1473       13146\nCOX3       68228       71945       66900       52368       14665       37275\nCYTB       46219       52035       46090       35247       17449       38762\n     ERR10802877 ERR10802878 ERR10802879 ERR10802880 ERR10802881 ERR10802882\nATP6         987        1834        3337        5036        1983       11586\nATP8           0           0           0           3           0          27\nCOX1       17785       32099       64490       63960       50965       76113\nCOX2        1141        1907        3439        8334        2063       12752\nCOX3        8797       15948       26278       29997       17802       35419\nCYTB       11177       22262       34368       33401       25854       43912\n     ERR10802883 ERR10802884 ERR10802885 ERR10802886\nATP6       18821        2792       11749        6682\nATP8          40           0           8           1\nCOX1      108343       65829      107741       94682\nCOX2       19148        2713       17947       10656\nCOX3       51441       24915       50029       47750\nCYTB       57844       34616       50587       51198\n\n\n\nCheck that the sample IDs match\nWhen creating the DESeq2 object, DESeq2 assumes that sample IDs in both tables match and are provided in the same order. Let’s make sure this is indeed the case:\n\n# Check that sample IDs in the metadata and the count table match\nall(row.names(meta) == colnames(count_mat))\n\n[1] TRUE\n\n\n\n\n\n\n2.3 Create the DESeq2 object\nWe will create the DESeq2 object using the function DESeqDataSetFromMatrix(), which we will provide with three arguments corresponding to the components discussed above:\n\nThe metadata with argument colData.\nThe count data with argument countData.\nThe statistical design for the DE analysis with argument design. For now, we will specify ~1, which effectively means “no design” — we will change this before the actual DE analysis.\n\n\n# Create the DESeq2 object\n# (`dds` is a name commonly used for DESeq2 objects, short for \"DESeq Data Set\")\ndds &lt;- DESeqDataSetFromMatrix(\n  colData = meta,\n  countData = count_mat,\n  design = ~ 1\n  )\n\nconverting counts to integer mode\n\n\nBefore we will run the differential expression analysis, though, we will do a bit of exploratory data analysis using our dds object."
  },
  {
    "objectID": "rnaseq/03_DE.html#exploratory-data-analysis",
    "href": "rnaseq/03_DE.html#exploratory-data-analysis",
    "title": "Gene count table analysis",
    "section": "3 Exploratory Data Analysis",
    "text": "3 Exploratory Data Analysis\n\n3.1 Our count matrix\nWhat are the number of rows (=number of genes) and columns (=number of samples) of our count matrix?\n\ndim(count_mat)\n\n[1] 18855    22\n\n\nHow many genes have total (= across all samples) counts that are non-zero?\n\nnrow(count_mat[rowSums(count_mat) &gt; 0, ])\n\n[1] 17788\n\n\n\n\n Exercise: gene counts\n\nHow many genes have total counts of at least 10?\n\n\n\nClick to see the solution\n\n\nnrow(count_mat[rowSums(count_mat) &gt;= 10, ])\n\n[1] 16682\n\n\n\n\nBonus: How many genes have mean counts of at least 10?\n\n\n\nClick to see the solution\n\n\n# Now we need to divide by the number of samples, which is the number of columns,\n# which we can get with 'ncol'\nnrow(count_mat[rowSums(count_mat) / ncol(count_mat) &gt;= 10, ])\n\n[1] 12529\n\n\n\n\n\nHow do the “library sizes”, i.e. the summed per-sample gene counts, compare across samples?\n\ncolSums(count_mat)\n\nERR10802863 ERR10802864 ERR10802865 ERR10802866 ERR10802867 ERR10802868 \n   24297245    17177436    22745445    26849403    21471477    17506262 \nERR10802869 ERR10802870 ERR10802871 ERR10802874 ERR10802875 ERR10802876 \n   24299398    25490128    26534405    22194841    18927885    28804150 \nERR10802877 ERR10802878 ERR10802879 ERR10802880 ERR10802881 ERR10802882 \n    9498249    14807513    20667093    23107463    17545375    19088206 \nERR10802883 ERR10802884 ERR10802885 ERR10802886 \n   21418234    19420046    24367372    25452228 \n\n\n\n Bonus exercise: nicer counts\nThat’s not so easy to read / interpret. Can you instead get these numbers in millions, rounded to whole numbers, and sorted from low to high?\n\n\nClick to see the solution\n\n\nsort(round(colSums(count_mat) / 1000000))\n\nERR10802877 ERR10802878 ERR10802864 ERR10802868 ERR10802881 ERR10802875 \n          9          15          17          18          18          19 \nERR10802882 ERR10802884 ERR10802867 ERR10802879 ERR10802883 ERR10802874 \n         19          19          21          21          21          22 \nERR10802865 ERR10802880 ERR10802863 ERR10802869 ERR10802885 ERR10802870 \n         23          23          24          24          24          25 \nERR10802886 ERR10802866 ERR10802871 ERR10802876 \n         25          27          27          29 \n\n\n\n\n\n\n\n3.2 Principal Component Analysis (PCA)\nWe will run a PCA to examine overall patterns of (dis)similarity among samples, helping us answer questions like:\n\nDo the samples cluster by treatment (infection status) and/or time point?\nWhich of these two variables has a greater effect on overall patterns of gene expression?\nIs there an overall interaction between these two variables?\n\nFirst, normalize the count data to account for differences in library size among samples and “stabilize” the variance among genes4:\n\ndds_vst &lt;- varianceStabilizingTransformation(dds)\n\n\n\n\n\n\n\nThe authors of the study did this as well:\n\n\n\n\nWe carried out a Variance Stabilizing Transformation (VST) of the counts to represent the samples on a PCA plot.\n\n\n\n\nNext, run and plot the PCA with a single function call, plotPCA from DESeq2:\n\n# With 'intgroup' we specify the variables (columns) to color samples by\nplotPCA(dds_vst, intgroup = c(\"time\", \"treatment\"))\n\nusing ntop=500 top features by variance\n\n\n\n\n\n\n\n Exercise: PCA\n\nBased on your PCA plot, try to answer the three questions asked at the beginning of this PCA section.\nHow does our plot compare to the PCA plot in the paper (Figure 1), in terms of the conclusions you just drew in the previous exercise.\n\n\n\nClick to see the paper’s Figure 1\n\n\n\n\n\n\nBonus: Compare the PCA plot with different numbers of included genes (Hint: figure out how to do so by looking at the help by running ?plotPCA).\nBonus: Customize the PCA plot — e.g. can you “separate” treatment and time point (different shapes for one variable, and different colors for the other), like in Fig. 1 of the paper?\n\n\n\nClick to see some hints for PCA plot customization\n\nTo expand on the point of the exercise: in the plot we made above, each combination of time point and treatment has a distinct color — it would be better to use point color only to distinguish one of the variables, and point shape to distinguish the other variable (as was also done in the paper’s Fig. 1).\nTo be able to customize the plot properly, we best build it from scratch ourselves, rather than using the plotPCA function. But then how do we get the input data in the right shape?\nA nice trick is that we can use returnData = TRUE in the plotPCA function, to get plot-ready formatted data instead of an actual plot:\n\npca_df &lt;- plotPCA(dds_vst, ntop = 500,\n                  intgroup = c(\"time\", \"treatment\"), returnData = TRUE)\n\nusing ntop=500 top features by variance\n\n\nWith that pca_df dataframe in hand, it will be relatively straightforward to customize the plot, if you know some ggplot2.\n\n\n\nClick to see a possible solution\n\nFirst, we’ll get the data in the right format, as explained in the hint:\n\npca_df &lt;- plotPCA(dds_vst, ntop = 500,\n                  intgroup = c(\"time\", \"treatment\"), returnData = TRUE)\n\nusing ntop=500 top features by variance\n\n\nSecond, we’ll extract and store the percentage of variance explained by different principal components, so we can later add this information to the plot:\n\npct_var &lt;- round(100 * attr(pca_df, \"percentVar\"), 1)\npct_var\n\n[1] 85.3  3.1\n\n\nNow we can make the plot:\n\nggplot(pca_df,\n       aes(x = PC1, y = PC2, color = treatment, shape = time)) +\n  geom_point(size = 5) +\n  labs(x = paste0(\"PC1 (\", pct_var[1], \"%)\"),\n       y = paste0(\"PC2 (\", pct_var[2], \"%)\")) +\n  scale_color_brewer(palette = \"Dark2\", name = \"Infection status\") +\n  scale_shape(name = \"Time points\") +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())"
  },
  {
    "objectID": "rnaseq/03_DE.html#differential-expression-de-analysis",
    "href": "rnaseq/03_DE.html#differential-expression-de-analysis",
    "title": "Gene count table analysis",
    "section": "4 Differential Expression (DE) analysis",
    "text": "4 Differential Expression (DE) analysis\n\n4.1 Figuring out how to do the analysis\nFirst, let’s see how the DE analysis was done in the paper:\n\nThen, we used the DESeq2 package (Love et al., 2014) to perform the differential gene expression analysis comparing: (i) P. relictum-infected mosquitoes vs. controls, (ii) P. cathemerium-infected mosquitoes vs. controls, and (iii) P. relictum-infected mosquitoes vs. P. cathemerium-infected mosquitoes.\n\nThis is not terribly detailed and could be interpreted in a couple of different ways. For example, they may have compared infection statuses by ignoring time points or by controlling for time points (and there are different ways to do the latter).\nIgnoring time would mean analyzing the full dataset (all time points) while only using the infection status as an independent variable, i.e. the design ~treatment.\n\n\n Given the PCA results, do you think that ignoring the time variable is a good idea? (Click for the answer)\n\nNo: the time variable clearly has a large effect on overall patterns of gene expression, in fact more so than the treatment..\n\nControlling for time can be done in two ways:\n\nA two-factor analysis: ~ time + treatment.\nPairwise comparisons between each combination of time and treatment (we’ll see below how we can do that).\n\nIf we take a look at Table 1 with the DE results, it will become clearer how they did their analysis:\n\n\n\n\n\n\n\n How do you interpret this: did they run pairwise comparisons or a two-factor model? (Click for the answer)\n\nIt looks like they performed pairwise comparisons between each combination of time and treatment.\n\n\nThat brings us a step closer, but pairwise comparisons with &gt;1 independent variable can (also!) be done in two ways:\n\nAfter subsetting the dataset to each combination of time and treatment.\nAfter creating a single, combined independent variable that is a combination of time and treatment.\n\nThe latter method is the more common one, and is what we will do below5.\n\n\n\n4.2 Setting the statistical design\nWe will now create a new variable that is a combination of treatment and time, and call it group:\n\n# Create a combined variable called 'group':\ndds$group &lt;- factor(paste(dds$treatment, dds$time, sep = \"_\"))\n\n\n# Which unique values does 'group' have, and how many samples are in each?\ntable(dds$group)\n\n\ncathemerium_10dpi cathemerium_24hpi     control_10dpi     control_24hpi \n                4                 3                 4                 3 \n   relictum_10dpi    relictum_24hpi \n                4                 4 \n\n\nNext, we set the analysis design:\n\n# Set the statistical design (Note: the symbol before 'group' is a tilde, ~ )\ndesign(dds) &lt;- ~ group\n\nNow we’re ready to run the DE analysis!\n\n\n\n4.3 Running the DE analysis\nWhile we had to do a lot of prep to get to this stage, actually running the DE analysis is very simple:\n\n# Run the DE analysis\n# (We are assigning the output back to the same `dds` object - the DE results are added to it)\ndds &lt;- DESeq(dds)\n\nestimating size factors\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\nThe DESeq() function is a wrapper that performs three steps (functions) consecutively:\n\nestimateSizeFactors() — “Normalization” by library size and composition.\nestimateDispersions() — Estimate gene-wise dispersion (variance in counts).\nnbinomWaldTest(ddsObj) — Fit the negative binomial GLM and calculate test statistics\n\nA key thing to understand is that above, DESeq2 automatically performed pairwise comparisons between each of the (6) levels of the group variable. This means that for any individual gene, it tested whether the gene is differentially expressed separately for each of these pairwise comparisons."
  },
  {
    "objectID": "rnaseq/03_DE.html#extracting-the-de-results",
    "href": "rnaseq/03_DE.html#extracting-the-de-results",
    "title": "Gene count table analysis",
    "section": "5 Extracting the DE results",
    "text": "5 Extracting the DE results\nDESeq2 stores the results as a separate table for each pairwise comparison, and now, we’ll extract one of these.\n\n5.1 The results table\nWe can extract the results for one pairwise comparison (which DESeq2 refers to as a contrast) at a time, by specifying it with the contrast argument as a vector of length 3:\n\nThe focal independent variable (here, group)\nThe first (reference) level of the independent variable (in the example below, relictum_24hpi)\nThe second level of the independent variable (in the example below, control_24hpi)\n\n\n# Extract the DE results for one pairwise comparison\nfocal_contrast &lt;- c(\"group\", \"relictum_24hpi\", \"control_24hpi\")\nres_rc24 &lt;- results(dds, contrast = focal_contrast)\n\nhead(res_rc24)\n\nlog2 fold change (MLE): group relictum_24hpi vs control_24hpi \nWald test p-value: group relictum_24hpi vs control_24hpi \nDataFrame with 6 rows and 6 columns\n       baseMean log2FoldChange     lfcSE      stat    pvalue      padj\n      &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nATP6  7658.0445      -0.416305  0.609133 -0.683438 0.4943300  0.776172\nATP8     4.9196      -1.311116  1.388811 -0.944057 0.3451406        NA\nCOX1 75166.8670      -0.590935  0.282075 -2.094958 0.0361747  0.208045\nCOX2  7807.1848      -0.610152  0.578401 -1.054893 0.2914743  0.615249\nCOX3 41037.7359      -0.400173  0.251760 -1.589498 0.1119479  0.388880\nCYTB 36916.6130      -0.501653  0.261927 -1.915242 0.0554617  0.266528\n\n\nWhat do the columns in this table contain?\n\nbaseMean: Mean expression level across all samples.\nlog2FoldChange: The “log2-fold change” of gene counts between the compared levels.\nlfcSE: The uncertainty in terms of the standard error (SE) of the log2-fold change estimate.\nstat: The value for the Wald test’s test statistic.\npvalue: The uncorrected p-value from the Wald test.\npadj: The multiple-testing corrected p-value (i.e., adjusted p-value).\n\n\n\n\n\n\n\nMultiple testing correction\n\n\n\nBecause we are testing significance for many genes, we need to correct for multiple testing. DESeq2 uses the Benjamini-Hochberg False Discovery Rate (FDR) correction. For more info, see this StatQuest video.\n\n\n\n\n\n\n\n\nLog2-fold changes (LFCs)\n\n\n\nIn RNA-seq, log2-fold changes (LFCs) are the standard way of representing the magnitude (effect size) of expression level differences between two groups of interest. With A and B being the compared sample groups, the LFC is calculated as:\nlog2(mean of A / mean of B)\nDue the log-transformation, the LFC also increase more slowly than a raw fold-change:\n\nAn LFC of 1 indicates a 2-fold difference\nAn LFC of 2 indicates a 4-fold difference\nAn LFC of 3 indicates a 8-fold difference\n\nA nice property of LFC is that decreases and increases in expression are expressed symmetrically:\n\nAn LFC of 1 means that group A has a two-fold higher expression that group B\nAn LFC of -1 means that group A has a two-fold lower expression that group B\n\n\n\n\n\n Exercise: Log-fold changes\nBased on the above, or your knowledge of log-transformations, what do you expect the following to return:\n\n# In the context of a LFC, these 2 numbers would be mean expression levels in 2 groups\nlog2(8 / 2)\nlog2(2 / 8)\n\n\n\nClick to see the solution\n\n\nA fold-change of 4 (8/2) is a LFC of 2:\n\n\nlog2(8 / 2)\n\n[1] 2\n\n\n\nA fold-change of 0.25 (2/8) is a LFC of -2:\n\n\nlog2(2 / 8)\n\n[1] -2\n\n\n\n\n\n\n\n5.2 Numbers of Differentially Expressed Genes (DEGs)\nHow many adjusted p-values were less than 0.05 (i.e., significant)?\n\n# (We need 'na.rm = TRUE' because some p-values are 'NA')\n# (If we don't remove NAs from the calculation, sum() will just return NA)\nsum(res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 801\n\n\nSo, we have 801 Differentially Expressed Genes (DEGs) for this specific pairwise comparison.\n\n\n Exercise: DEGs\nThe paper’s Table 1 (which we saw above) reports the number of DEGs for a variety of comparisons.\n\nHow does the number of DEGs we just got compare to what they found in the paper for this comparison?\nThe table also reports numbers of up- and downregulated genes separately. Can you find this out for our DEGs?\n\n\n\nClick to see the solution\n\n\nSolution using tidyverse/dplyr:\n\n\n# First we need to convert the results table into a regular data frame\nas.data.frame(res_rc24) |&gt;\n  # Then we only select the rows/genes that are significant\n  filter(padj &lt; 0.05) |&gt;\n  # If we run count() on a logical test, we get the nrs. that are FALSE v. TRUE\n  dplyr::count(log2FoldChange &gt; 0)\n\n  log2FoldChange &gt; 0   n\n1              FALSE 616\n2               TRUE 185\n\n\n\nSolution using base R:\n\n\n# Down-regulated (relictum &lt; control):\nsum(res_rc24$log2FoldChange &lt; 0 & res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 616\n\n# Up-regulated (relictum &gt; control):\nsum(res_rc24$log2FoldChange &gt; 0 & res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 185\n\n\n\n\n\n\nBonus: The table also reports the number of DEGs with an absolute LFC &gt; 1. Can you find this out for our DEGs?\n\n\n\nClick to see the solution\n\n\nSolution using tidyverse/dplyr:\n\n\n# First we need to convert the results table into a regular data frame\nas.data.frame(res_rc24) |&gt;\n  # Then we only select the rows/genes that are significant\n  filter(padj &lt; 0.05, abs(log2FoldChange) &gt; 1) |&gt;\n  # If we run count() on a logical test, we get the nrs. that are FALSE v. TRUE\n  dplyr::count(log2FoldChange &gt; 0)\n\n  log2FoldChange &gt; 0   n\n1              FALSE 159\n2               TRUE  49\n\n\n\nSolution using base R:\n\n\n# Down-regulated (relictum &lt; control):\nsum(res_rc24$log2FoldChange &lt; -1 & res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 159\n\n# Up-regulated (relictum &gt; control):\nsum(res_rc24$log2FoldChange &gt; 1 & res_rc24$padj &lt; 0.05, na.rm = TRUE)\n\n[1] 49\n\n\n\n\nBonus: Extract the results for one or more other contrasts in the table, and compare the results."
  },
  {
    "objectID": "rnaseq/03_DE.html#visualizing-the-de-results",
    "href": "rnaseq/03_DE.html#visualizing-the-de-results",
    "title": "Gene count table analysis",
    "section": "6 Visualizing the DE results",
    "text": "6 Visualizing the DE results\nTo practice with visualization of the differential expression results, we will create a few plots for the results for the relictum_24hpi vs. control_24hpi comparison, which we extracted above.\n\n\n6.1 Volcano plot\nFor a nice overview of the results, we can create a so-called “volcano plot” using the EnhancedVolcano() function from the package of the same name (see here for a “vignette”/tutorial):\n\nEnhancedVolcano(\n  toptable = res_rc24,      # DESeq2 results to plot   \n  title = \"relictum vs. control at 24 hpi\",\n  x = \"log2FoldChange\",     # Plot the log2-fold change along the x-axis\n  y = \"padj\",               # Plot the p-value along the y-axis\n  lab = rownames(res_rc24), # Use the rownames for the gene labels (though see below)\n  labSize = 0               # Omit gene labels for now\n  )\n\n\n\n\n\n\n\n\n\n Bonus exercise: Volcano plots\nThe EnhancedVolcano() function by default adds gene IDs to highly significant genes, but above, we turned off gene name labeling by setting labSize = 0. I did this because the default p-value cut-off for point labeling is 1e-5 and in this case, that would make the plot quite busy with gene labels. We might want to try a plot with a stricter p-value cut-off that does show the gene labels.\n\nPlay around with the p-value cut-off and the labeling to create a plot you like.\nCheck the vignette, or the help page (accessed by running ?EnhancedVolcano) to see how you can do this.\n\n\n\nClick for an example\n\n\nEnhancedVolcano(\n  toptable = res_rc24,      \n  title = \"relictum vs. control at 24 hpi\",\n  x = \"log2FoldChange\",     \n  y = \"padj\",             \n  lab = rownames(res_rc24), \n  labSize = 4,               # Now we will show the gene labels\n  pCutoff = 10e-10,          # Modify the p-value cut-off\n  subtitle = NULL,           # I'll also remove the silly subtitle\n  caption = NULL,            # ... and the caption\n  )\n\n\n\n\n\n\n\n\n\n\nFigure out the identity of the above-mentioned log2-fold change outlier.\n(You can do so either by labeling it in the plot, or by filtering the res_rc24 table.)\n\n\n\nClick for the solution for how to lab it in the plot\n\n\n\nEnhancedVolcano(\n  toptable = res_rc24,      \n  title = \"relictum vs. control at 24 hpi\",\n  x = \"log2FoldChange\",     \n  y = \"padj\",             \n  lab = rownames(res_rc24), \n  labSize = 4,               \n  pCutoff = 0.05,            # Modify the p-value cut-off\n  FCcutoff = 20,             # Modify the LFC cut-off\n  )\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_vline()`).\n\n\n\n\n\n\n\n\n\n\n\n\nClick for the solution for how to find it in the results table\n\n\n\nas.data.frame(res_rc24) |&gt; filter(log2FoldChange &gt; 20)\n\n              baseMean log2FoldChange    lfcSE     stat       pvalue\nLOC120413430  7.540043       24.46898 5.397990 4.532979           NA\nLOC120431476 39.720375       23.01445 5.301369 4.341228 1.416886e-05\n                     padj\nLOC120413430           NA\nLOC120431476 0.0008584398\n\n\n(Interestingly, there’s a second gene with a LFC &gt; 20 that we hadn’t seen in the plot, because it has NA as the pvalue and padj. See the section “Extra info: NA values in the results table” in the Appendix for why p-values can be set to NA.)\n\n\n\n\n\n6.2 Plot specific genes\nWe can also create plots of expression levels for individual genes. That is especially interesting for genes with highly significant differential expression. So let’s plot the most highly significant DEG.\nFirst, let’s create a vector with most highly significant DEGs, which we’ll use again for the heatmap below.\n\ntop25_DE &lt;- row.names(res_rc24[order(res_rc24$padj)[1:25], ])\n\ntop25_DE\n\n [1] \"LOC120423768\" \"LOC120423767\" \"LOC120414587\" \"LOC128092307\" \"LOC120431154\"\n [6] \"LOC120427827\" \"LOC120415152\" \"LOC120422735\" \"LOC120431739\" \"LOC120431733\"\n[11] \"LOC120428214\" \"LOC120427588\" \"LOC120415540\" \"LOC120415522\" \"LOC120429000\"\n[16] \"LOC120414889\" \"LOC120413491\" \"LOC120414802\" \"LOC120423826\" \"LOC120429211\"\n[21] \"LOC120425480\" \"LOC120431003\" \"LOC120421894\" \"LOC120423819\" \"LOC128093166\"\n\n\nDESeq2 has a plotting function but the plot is not very good. We will still use that function but just to quickly extract the counts for our gene of interest in the right format for plotting, using returnData = TRUE:\n\nfocal_gene_counts &lt;- plotCounts(\n  dds,\n  gene = top25_DE[1],\n  intgroup = c(\"time\", \"treatment\"),\n  returnData = TRUE\n  )\n\nhead(focal_gene_counts)\n\n                 count  time   treatment\nERR10802863 1543.81532 24hpi     control\nERR10802864 2279.03704 24hpi cathemerium\nERR10802865   25.42295 24hpi    relictum\nERR10802866 1105.75009 24hpi     control\nERR10802867 1199.28425 24hpi cathemerium\nERR10802868   32.14394 24hpi    relictum\n\n\nNow, we can make the plot:\n\nggplot(focal_gene_counts,\n       # Treatment along the x-axis, gene counts along the y, color by treatment:\n       aes(x = treatment, y = count, fill = treatment)) +\n  # Plot separate \"facets\" with the different time points\n  facet_wrap(vars(time)) +\n  # Add a boxplot with a partly transparent (alpha) color:\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  # _And_ add individual points:\n  geom_point(size = 4, shape = 21,\n             position = position_jitter(w = 0.1, h = 0)) +\n  # Plot styling (e.g., we don't need a legend)\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n Exercise: Single-gene plots\n\nPlot one or a few more of the top-DE genes. Do they have similar expression patterns across treatment and time points as the first one?\n\n\n\nBonus: Plot the gene with the very high LFC value that we saw when making the volcano plot. How would you interpret this?\n\n\n\nClick for the solution\n\n\nfocal_gene_counts &lt;- plotCounts(\n  dds,\n  gene = \"LOC120431476\",\n  intgroup = c(\"time\", \"treatment\"),\n  returnData = TRUE\n  )\n\nggplot(focal_gene_counts, aes(x = treatment, y = count, fill = treatment)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_point(size = 4, shape = 21, position = position_jitter(w = 0.1, h = 0)) +\n  facet_wrap(vars(time)) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWow! It looks like in every single time + treatment combinations, all but one (or in one case, two) of the samples have zero expression, but there are several extreme outliers.\nOur focal comparison at 24hpi (left panel/facet), and comparing control vs relictum: so it looks like the difference between these two groups is solely due to the one outlier in relictum. Nevertheless, even the multiple-testing corrected p-value (padj) is significant for this gene:\n\nas.data.frame(res_rc24) |&gt;\n  rownames_to_column(\"gene\") |&gt;\n  filter(gene == \"LOC120431476\")\n\n          gene baseMean log2FoldChange    lfcSE     stat       pvalue\n1 LOC120431476 39.72038       23.01445 5.301369 4.341228 1.416886e-05\n          padj\n1 0.0008584398\n\n\nSo, we have to be careful with talking our statistical results at face value, and need to visualize important genes!\n\n\n\n\n\n\n\nOutliers!\n\n\n\nYou may want to check out the solution to the previous exercise, even if you don’t get around to doing it yourself."
  },
  {
    "objectID": "rnaseq/03_DE.html#in-closing",
    "href": "rnaseq/03_DE.html#in-closing",
    "title": "Gene count table analysis",
    "section": "7 In Closing",
    "text": "7 In Closing\nToday, you have performed several steps in the analysis of gene counts that result from a typical RNA-seq workflow. Specifically, you have:\n\nCreated a DESEq2 object from the gene count data and the experiment’s metadata\nPerformed exploratory data analysis including a PCA\nRan a Differential Expression (DE) analysis with DESeq2\nExtracted, interpreted, and visualized the DE results\n\n\nNext steps\nTypical next steps in such an analysis include:\n\nExtracting, comparing, and synthesizing DE results across all pairwise comparisons (this would for example allow us to make the upset plot in Figure 2 of the paper)\nFunctional enrichment analysis with Gene Ontology (GO) terms, as done in the paper, and/or with KEGG pathways and other functional gene grouping systems."
  },
  {
    "objectID": "rnaseq/03_DE.html#appendix",
    "href": "rnaseq/03_DE.html#appendix",
    "title": "Gene count table analysis",
    "section": "8 Appendix",
    "text": "8 Appendix\n\n8.1 Heatmaps\nRather than plotting expression levels for many individual genes, we can create “heatmap” plots to plot dozens (possibly even hundreds) of genes at once.\nWe will create heatmaps with the pheatmap function, and let’s make a heatmap for the top-25 most highly significant DEGs for our focal contrast.\nUnlike with some of the functions we used before, we unfortunately can’t directly use our DESeq2 object, but we have to extract and subset the count matrix, and also pass the metadata to the heatmap function:\n\n# We need a normalized count matrix, like for the PCA\n# We can simply extract the matrix from the normalized dds object we created for the PCA\nnorm_mat &lt;- assay(dds_vst)\n\n# In the normalized count matrix, select only the genes of interest\n# We'll reuse the 'top25_DE' vector that we created for the individual gene plots\nnorm_mat_sel &lt;- norm_mat[match(top25_DE, rownames(norm_mat)), ]\n\n# Sort the metadata\nmeta_sort &lt;- meta |&gt;\n  arrange(treatment, time) |&gt;\n  select(treatment, time)\n\nNow we can create the plot:\n\npheatmap(\n  norm_mat_sel,\n  annotation_col = meta_sort,  # Add the metadata\n  cluster_cols = FALSE,        # Don't cluster samples (=columns, cols)\n  show_rownames = FALSE,       # Don't show gene names\n  scale = \"row\",               # Perform z-scaling for each gene\n  )\n\n\n\n\n\n\n\n\nNotes on the code and plot above:\n\nThe z-scaling with scale = will make sure we can compare genes with very different expression levels: after all, we’re interested in relative expression levels across samples/sample groups.\npheatmap will by default perform hierarchical clustering both at the sample (col) and gene (row) level, such that more similar samples and genes will appear closer to each other. Above, we turned clustering off for samples, since we want to keep them in their by-group order.\n\n\n\n Bonus exercise: heatmaps\nMake a heatmap with the top-25 most-highly expressed genes (i.e., genes with the highest mean expression levels across all samples).\n\n\nClick for a hint: how to get that top-25\n\n\ntop25_hi &lt;- names(sort(rowMeans(norm_mat), decreasing = TRUE)[1:25])\n\n\n\n\nClick for the solution\n\n\n# In the normalized count matrix, select only the genes of interest\nnorm_mat_sel &lt;- norm_mat[match(top25_hi, rownames(norm_mat)), ]\n\n# Sort the metadata\nmeta_sort &lt;- meta |&gt;\n  arrange(treatment, time) |&gt;\n  select(treatment, time)\n\n# Create the heatmap\npheatmap(\n  norm_mat_sel,\n  annotation_col = meta_sort,\n  cluster_cols = FALSE,\n  show_rownames = FALSE,\n  scale = \"row\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.2 NA values in the DESeq2 results table\nSome values in the DESeq2 results table can be set to NA for one of the following reasons:\n\nIf a gene contains a sample with a count outlier, both the p-value and adjusted p-value will be set to NA. (DESeq2 performs outlier detection using Cook’s distance.)\nIf all samples have zero counts for a given gene, the baseMean column will be zero, and the log2-fold change estimates, p-value and adjusted p-value will all be set to NA.\nDESeq2 also automatically filters genes with a low mean count in the sense that it does not include them in the multiple testing correction. Therefore, in such cases, the p-value will not be NA, but the adjusted p-value will be.\nBecause we have very low power to detect differential expression for such low-count genes, it is beneficial to remove them prior to the multiple testing correction: that way, the correction becomes less severe for the remaining genes.\n\nLet’s see how many genes have NA p-values:\n\n# Number of genes with NA p-value:\nsum(is.na(res_rc24$pvalue))\n\n[1] 1124\n\n# As a proportion of the total number of genes in the test:\nsum(is.na(res_rc24$pvalue)) / nrow(res_rc24)\n\n[1] 0.05961283\n\n\nAnd NA adjusted p-values:\n\n# Number of genes with NA p-value:\nsum(is.na(res_rc24$padj))\n\n[1] 7283\n\n# As a proportion of the total number of genes in the test:\nsum(is.na(res_rc24$padj)) / nrow(res_rc24)\n\n[1] 0.3862636\n\n\n\n\n\n8.3 Exporting the results\nTo save the DE results tables, you can for example use the write_tsv() function. You could open the resulting file in Excel for further exploration.\n\n# Create the output directory, if necessary:\ndir.create(\"results/DE\", recursive = TRUE, showWarnings = FALSE)\n\n# Write the \nwrite_tsv(as.data.frame(res_rc24), \"results/DE/resultsres_rc24.tsv\")"
  },
  {
    "objectID": "rnaseq/03_DE.html#footnotes",
    "href": "rnaseq/03_DE.html#footnotes",
    "title": "Gene count table analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn older pipe, which requires loading an R package to work↩︎\nThe new base R pipe that does not require a package↩︎\nAnd even for more basic tasks, it is common to use packages that are preferred over the functionality that is by default available in R, like in the case of plotting.↩︎\nSpecifically, the point is to remove the dependence of the variance in expression level on its mean, among genes↩︎\nI can’t tell from the paper which method they used↩︎"
  },
  {
    "objectID": "rnaseq/02_nfc-rnaseq.html#introduction",
    "href": "rnaseq/02_nfc-rnaseq.html#introduction",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this tutorial, we will run the nf-core rnaseq pipeline. Using raw RNA-seq reads in FASTQ files and reference genomes files as inputs, this pipeline will generate a gene count table as its most important output. That gene count table can then be analyzed to examine, for example, differential expression, which is the topic of the self-study lab.\n\n\n\nAn overview of the steps in the nf-core rnaseq pipeline.\n\n\n\nWe will work with the data set from the paper “Two avian Plasmodium species trigger different transcriptional responses on their vector Culex pipiens”, published last year in Molecular Ecology (link):\n\n\n\n\n\nThis paper uses RNA-seq data to study gene expression in Culex pipiens mosquitoes infected with malaria-causing Plasmodium protozoans — specifically, it compares mosquito according to:\n\nInfection status: Plasmodium cathemerium vs. P. relictum vs. control\nTime after infection: 24 h vs. 10 days vs. 21 days"
  },
  {
    "objectID": "rnaseq/02_nfc-rnaseq.html#getting-started-with-vs-code",
    "href": "rnaseq/02_nfc-rnaseq.html#getting-started-with-vs-code",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "2 Getting started with VS Code",
    "text": "2 Getting started with VS Code\nWe will use the VS Code text editor to write a script to run the nf-core rnaseq pipeline. To emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. The RStudio program is another good example of an IDE. Just like RStudio is an IDE for R, VS Code will be our IDE for shell code today.\n\n2.1 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and near the bottom, click Code Server.\nVS Code runs on a compute node so we have to fill out a form to make a reservation for one:\n\nThe OSC “Project” that we want to bill for the compute node usage: PAS2658.\nThe “Number of hours” we want to make a reservation for: 2.\nThe “Working Directory” for the program: your personal folder in /fs/scratch/PAS2658 (e.g. /fs/scratch/PAS2658/jelmer).\nThe “Codeserver Version”: 4.8 (most recent).\nClick Launch.\n\nFirst, your job will be “Queued” — that is, waiting for the job scheduler to allocate compute node resources to it.\nYour job is typically granted resources within a few seconds (the card will then say “Starting”), and should be ready for usage (“Running”) in another couple of seconds. Once the job is running click on the blue Connect to VS Code button to open VS Code — it will open in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) — click “Yes” (and check the box) and “Don’t Show Again”, respectively:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll also get a Welcome/Get Started page — you don’t have to go through steps that may be suggested there.\n\n\n\n\n2.2 The VS Code User Interface\n\n\nClick to see an annotated screenshot\n\n\n\n\nSide bars\nThe narrow side bar on the far left has:\n\nA  (“hamburger menu”), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle between options for what to show in the wide side bar, e.g. a File Explorer (the default option).\n\n\n\n\nTerminal\nOpen a terminal with a Unix shell: click      =&gt; Terminal =&gt; New Terminal. In the terminal, create a dir for this lab, e.g.:\n# You should be in your personal dir in /fs/scratch/PAS2658\npwd\n/fs/scratch/PAS2658/jelmer\nmkdir -p Lab9 \ncd Lab9\nmkdir scripts run software\n\n\n\nEditor pane and Welcome document\nThe main part of the VS Code window is the editor pane. Here, you can open text files and images. Create two new files:\ntouch run/run.sh scripts/nfc-rnaseq.sh\nThen open the run.sh file in the editor – hold Ctrl/Cmd and click on the path in the command you just issued:\n\n\n\n\n\n\n\n\n\n\n\nOr create and open files using the menus (Click to expand)\n\n\n\n\n\n\nOpen a new file: Click the hamburger menu , then File &gt; New File.\nSave the file (Ctrl/⌘+S), inside one of the dirs you just created: Lab9/run/run.sh.\nRepeat steps 1 and 2 to create a file Lab9/scripts/nfc-rnaseq.sh.\nFind the run.sh file in the File Explorer in the left side bar, and click on it to open.\n\n\n\n\n\n\n\n\n\n\nSome VS Code tips and tricks (Click to expand)\n\n\n\n\n\n\nA folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nThis is why your terminal was “already” located in /fs/scratch/PAS2658/&lt;your-name&gt;.\n(If you need to switch folders, click      &gt;   File   &gt;   Open Folder.)\nResizing panes\nYou can resize panes (terminal, editor, side bar) by hovering your cursor over the borders and then dragging.\nHide the side bars\nIf you want to save some screen space while coding along in class, you may want to occasionally hide the side bars:\n\nIn  &gt; View &gt; Appearance you can toggle both the Activity Bar (narrow side bar) and the Primary Side Bar (wide side bar).\nOr use keyboard shortcuts:\n\nCtrl/⌘+B for the primary/wide side bar\nCtrl+Shift+B for the activity/narrow side bar\n\n\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for. To access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P). To use it, start typing something to look for an option.\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system:    =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.)\nInstall the Shellcheck extension\nClick the gear icon  and then Extensions, and search for and then install the shellcheck (by simonwong) extension, which will check your shell scripts for errors, and is extremely useful."
  },
  {
    "objectID": "rnaseq/02_nfc-rnaseq.html#setting-up",
    "href": "rnaseq/02_nfc-rnaseq.html#setting-up",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "3 Setting up",
    "text": "3 Setting up\n\n3.1 Getting your own copy of the data\nAs mentioned above, we will use the RNA-seq data from Garrigos et al. 2023. However, to keep things manageable for a lab like this, I have subset the data set we’ll be working with: I omitted the 21-day samples and only kept 500,000 reads per FASTQ file. All in all, our set of files consists of:\n\n44 paired-end Illumina RNA-seq FASTQ files for 22 samples.\nCulex pipiens reference genome files from NCBI: assembly in FASTA format and annotation in GTF format.\nA metadata file in TSV format matching sample IDs with treatment & time point info.\nA README file describing the data set.\n\nGo ahead and get yourself a copy of the data with cp command:\n# (Using the -r option for recursive copying, and -v to print what it's doing)\ncp -rv /fs/scratch/PAS2658/jelmer/share/* .\n‘/fs/scratch/PAS2658/jelmer/share/data’ -&gt; ‘./data’\n‘/fs/scratch/PAS2658/jelmer/share/data/meta’ -&gt; ‘./data/meta’\n‘/fs/scratch/PAS2658/jelmer/share/data/meta/metadata.tsv’ -&gt; ‘./data/meta/metadata.tsv’\n‘/fs/scratch/PAS2658/jelmer/share/data/ref’ -&gt; ‘./data/ref’\n‘/fs/scratch/PAS2658/jelmer/share/data/ref/GCF_016801865.2.gtf’ -&gt; ‘./data/ref/GCF_016801865.2.gtf’\n‘/fs/scratch/PAS2658/jelmer/share/data/ref/GCF_016801865.2.fna’ -&gt; ‘./data/ref/GCF_016801865.2.fna’\n‘/fs/scratch/PAS2658/jelmer/share/data/fastq’ -&gt; ‘./data/fastq’\n‘/fs/scratch/PAS2658/jelmer/share/data/fastq/ERR10802868_R2.fastq.gz’ -&gt; ‘./data/fastq/ERR10802868_R2.fastq.gz’\n‘/fs/scratch/PAS2658/jelmer/share/data/fastq/ERR10802863_R1.fastq.gz’ -&gt; ‘./data/fastq/ERR10802863_R1.fastq.gz’\n‘/fs/scratch/PAS2658/jelmer/share/data/fastq/ERR10802886_R2.fastq.gz’ -&gt; ‘./data/fastq/ERR10802886_R2.fastq.gz’\n# [...output truncated...]\nUse the tree command to get a nice overview of the files you copied:\n# '-C' will add colors to the output (not visible in the output below)\ntree -C data\ndata\n├── fastq\n│   ├── ERR10802863_R1.fastq.gz\n│   ├── ERR10802863_R2.fastq.gz\n│   ├── ERR10802864_R1.fastq.gz\n│   ├── ERR10802864_R2.fastq.gz\n│   ├── ERR10802865_R1.fastq.gz\n│   ├── ERR10802865_R2.fastq.gz\n    ├── [...truncated...]\n├── meta\n│   └── metadata.tsv\n├── README.md\n└── ref\n    ├── GCF_016801865.2.fna\n    └── GCF_016801865.2.gtf\n\n3 directories, 48 files\nWe’ll take a look at some of the files:\n\nThe metadata file:\ncat data/meta/metadata.tsv\nsample_id       time    treatment\nERR10802882     10dpi   cathemerium\nERR10802875     10dpi   cathemerium\nERR10802879     10dpi   cathemerium\nERR10802883     10dpi   cathemerium\nERR10802878     10dpi   control\nERR10802884     10dpi   control\nERR10802877     10dpi   control\nERR10802881     10dpi   control\nERR10802876     10dpi   relictum\nERR10802880     10dpi   relictum\nERR10802885     10dpi   relictum\nERR10802886     10dpi   relictum\nERR10802864     24hpi   cathemerium\nERR10802867     24hpi   cathemerium\nERR10802870     24hpi   cathemerium\nERR10802866     24hpi   control\nERR10802869     24hpi   control\nERR10802863     24hpi   control\nERR10802871     24hpi   relictum\nERR10802874     24hpi   relictum\nERR10802865     24hpi   relictum\nERR10802868     24hpi   relictum\nThe FASTQ files:\nls -lh data/fastq\ntotal 941M\n-rw-r--r-- 1 jelmer PAS2658 21M Mar 23 12:40 ERR10802863_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802863_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 21M Mar 23 12:40 ERR10802864_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802864_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802865_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802865_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 21M Mar 23 12:40 ERR10802866_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802866_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802867_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS2658 22M Mar 23 12:40 ERR10802867_R2.fastq.gz\n# [...output truncated...]\n\n\n\n\n3.2 How we’ll run the pipeline\nAs discussed in the lecture, the entire nf-core rnaseq pipeline can be run with a single command. That said, before we can do so, we’ll need to a bunch of prep, such as:\n\nActivating the software environment and downloading the pipeline files.\nDefining the pipeline’s inputs and outputs, which includes creating a “sample sheet”.\nCreating a small “config file” to run Nextflow pipelines at OSC.\n\nWe need the latter configuration because the pipeline will submit Slurm batch jobs for us for each step of the pipeline. And in most steps, programs are run independently for each sample, so the pipeline will submit a separate job for each sample for these steps — therefore, we’ll have many jobs altogether (typically 100s).\nThe main Nextflow process does not need much computing power (a single core with the default 4 GB of RAM will be sufficient) and even though our VS Code shell already runs on a compute and not a login node, we are still better off submitting the main process as a batch job as well, because:\n\nThis process can run for hours and we don’t want to risk it disconnecting.\nWe want to store all the standard output about pipeline progress and so on to a file — this will automatically end up in a Slurm log file if we submit it as a batch job.\n\n\n\n\n3.3 Conceptual overview of our script setup\nWe will be working with two scripts in this lab, both of which you already created an empty file for:\n\nA “runner” script that you can also think of as a digital lab notebook, containing commands that we run interactively.\nA script that we will submit as a Slurm batch job with sbatch, containing code to run the nf-core nextflow pipeline.\n\nTo give you an idea of what this will look like — the runner script will include code like this, which will submit the job script:\n# [Don't run or copy this]\nsbatch scripts/nfc_rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\"\nThe variables above (\"$samplesheet\" etc.) are the inputs and outputs of the pipeline, which we will have defined elsewhere in the runner script. Inside the job script, we will then use these variables to run the pipeline in a specific way.\n\n\n\n3.4 Activating the Conda environment\nTo save some time, you won’t do your own Conda installation of Nextflow or nf-core tools — I’ve installed both in an environment you can activate as follows:\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n\n# First load OSC's (mini)Conda module\nmodule load miniconda3\n# Then activate the Nextflow conda environment \nsource activate /fs/ess/PAS0471/jelmer/conda/nextflow\nCheck that Nextflow and nf-core tools can be run by printing the versions:\n# [Run this code directly in the terminal]\nnextflow -v\nnextflow version 23.10.1.5891\n# [Run this code directly in the terminal]\nnf-core --version\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.13.1 - https://nf-co.re\n\nnf-core, version 2.13.1\n\n\n\n3.5 Downloading the nf-core rnaseq pipeline\nWe’ll use the nf-core download command to download the rnaseq pipeline’s files.\nFirst, we need to set the environment variable NXF_SINGULARITY_CACHEDIR to tell Nextflow where to store the Singularity containers for all the tools the pipeline runs1. We will use a dir of mine that already has all containers, to save some downloading time2:\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n\n# Create an environment variable for the container dir\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS0471/containers\nNext, we’ll run the nf-core download command to download the currently latest version (3.14.0) of the rnaseq pipeline to software/rnaseq, and the associated container files to the previously specified dir:\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n\n# Download the nf-core rnaseq pipeline files\nnf-core download rnaseq \\\n    --revision 3.14.0 \\\n    --outdir software/nfc-rnaseq \\\n    --compress none \\\n    --container-system singularity \\\n    --container-cache-utilisation amend \\\n    --download-configuration\n\n\n\n\n\n\n\n\n\n\n\nExplanation of all options given to nf-core download (Click to expand)\n\n\n\n\n\n\n--revision: The version of the rnaseq pipeline.\n--outdir: The dir to save the pipeline definition files.\n--compress: Whether to compress the pipeline files — we chose not to.\n--container-system: The type of containers to download. This should always be singularity at OSC, because that’s the only supported type.\n--container-cache-utilisation: This is a little technical and not terribly interesting, but we used amend, which will make it check our $NXF_SINGULARITY_CACHEDIR dir for existing containers, and simply download any that aren’t already found there.\n--download-configuration: This will download some configuration files that we will actually not use, but if you don’t provide this option, it will ask you about it when you run the command.\n\nAlso, don’t worry about the following warning, this doesn’t impact the downloading:\n\nWARNING Could not find GitHub authentication token. Some API requests may fail.\n\n\n\n\n\nLet’s take a quick peek at the dirs and files we just downloaded:\n# [Run this code directly in the terminal]\nls software/nfc-rnaseq\n3_14_0  configs\n# [Run this code directly in the terminal]\nls software/nfc-rnaseq/3_14_0\nassets        CODE_OF_CONDUCT.md  LICENSE       nextflow.config       subworkflows\nbin           conf                main.nf       nextflow_schema.json  tower.yml\nCHANGELOG.md  docs                modules       pyproject.toml        workflows\nCITATIONS.md  lib                 modules.json  README.md\nThe dir and file structure here is unfortunately quite complicated, as are the individual pipeline definition files, so we won’t go into further detail about that here."
  },
  {
    "objectID": "rnaseq/02_nfc-rnaseq.html#writing-a-shell-script-to-run-the-pipeline",
    "href": "rnaseq/02_nfc-rnaseq.html#writing-a-shell-script-to-run-the-pipeline",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "4 Writing a shell script to run the pipeline",
    "text": "4 Writing a shell script to run the pipeline\nIn this section, we’ll go through the components of the scripts/nfc-rnaseq.sh script that we’ll later submit as a Slurm batch job. The most important part of this script is the nextflow command that will actually run the pipeline.\n\n4.1 Building our nextflow run command\nTo run the pipeline, we use the command nextflow run, followed by the path to the dir that we just downloaded:\n# [Partial shell script code, don't copy or run]\nnextflow run software/nfc-rnaseq/3_14_0\nAfter that, there are several required options (see the pipeline’s documentation), which represent the input and output files/dirs for the pipeline:\n\n--input: The path to a “sample sheet” with the paths to FASTQ files (more on that below).\n--fasta: The path to a reference genome assembly FASTA file — we’ll use the FASTA file we have in data/ref.\n--gtf: The path to a reference genome annotation file3 — we’ll use the GTF file we have in data/ref.\n--outdir: The path to the desired output dir for the final pipeline output — this can be whatever we like.\n\nAs discussed in the lecture, this pipeline has different options for e.g. alignment and quantification. We will stick close to the defaults, which includes alignment with STAR and quantification with Salmon, with one exception: we want to remove reads from ribosomal RNA (this step is skipped by default).\n\n Exercise: Finding the option to remove rRNA\nTake a look at the “Parameters” tab on the pipeline’s documentation website:\n\nBrowse through the options for a bit to get a feel for the extent to which you can customize the pipeline.\nTry to find the option to turn on removal of rRNA with SortMeRNA.\n\n\n\nClick for the solution\n\nThe option we want is --remove_ribo_rna.\n\n\n\nWe’ll also use several general Nextflow options (note the single dash - notation; pipeline-specific options have --):\n\n-profile: A so-called “profile” — should be singularity when running the pipeline with Singularity containers.\nwork-dir: The dir in which all the pipeline’s jobs/processes will run.\n-ansi-log false: Change Nextflow’s progress “logging” type to a format that works with Slurm log files4.\n-resume: Resume the pipeline where it “needs to” (e.g., where it left off) instead of always starting over.\n\n\n\n\n\n\n\nMore on -work-dir and -resume (Click to expand)\n\n\n\n\n\nwork-dir:\nThe pipeline’s final outputs will go to the --outdir we talked about earlier. But all jobs/processes will run in, and initial outputs will be written to, a so-called -work-dir. After each process finishes, its key output files will then be copied to the final output dir. (There are also several pipeline options to customize what will and will not be copied.)\nThe distinction between such a work-dir and a final output dir can be very useful on HPC systems like OSC: you can use a scratch dir (at OSC: /fs/scratch/) with lots of storage space and fast I/O as the work-dir, and a backed-up project dir (at OSC: /fs/ess/) as the outdir, which will then not become unnecessarily large.\n-resume:\nBesides resuming wherever the pipeline left off after an incomplete run (for example: it ran out of time or ran into an error), the -resume option also checks for any changes in input files or pipeline settings.\nFor example, if you have run the pipeline to completion previously, but rerun it after adding or replace one sample, -resume would make the pipeline only rerun the “single-sample steps” of the pipeline (which is most of them) for that sample as well as all steps that use all samples. Similarly, if you change an option that affects one of the first processes in the pipeline, the entire pipeline may be rerun, whereas if you change an option that only affects the last process, then only that last process would be rerun.\nThis option won’t make any difference when we run the pipeline for the first time, since there is nothing to resume. Nextflow will even give a warning along these lines, but this is not a problem.\n\n\n\nWith all the above-mentioned options, our final nextflow run command will be:\n# [Partial shell script code, don't copy or run]\nnextflow run software/nfc-rnaseq/3_14_0 \\\n    --input \"$samplesheet\" \\\n    --fasta \"$fasta\" \\\n    --gtf \"$gtf\" \\\n    --outdir \"$outdir\" \\\n    --remove_ribo_rna \\\n    -work-dir \"$outdir\"/raw \\\n    -profile singularity \\\n    -ansi-log false \\\n    -resume\nThe command uses several variables (e.g. \"$samplesheet\") — these will enter the script via command-line arguments.\n\n\n\n4.2 Creating an OSC configuration file\nTo speed things up and make use of the computing power at OSC, we want the pipeline to submit Slurm batch jobs for us.\nWe have to tell it to do this, and how, using a configuration (config) file. There are multiple ways of storing this file and telling Nextflow about it — the one we’ll use is to simply create a file nextflow.config in the dir from which we submit the nextflow run command: Nextflow will automatically detect and parse such a file.\nWe will keep this file as simple as possible, only providing the “executor” (in our case: the Slurm program) and the OSC project to use:\necho \"\nprocess.executor = 'slurm'\nprocess.clusterOptions='--account=PAS2658'\n\" &gt; nextflow.config\n\n\n\n4.3 Adding #SBATCH options\nWe will use #SBATCH header lines to define some parameters for our batch job for Slurm. Note that these are only for the “main” Nextflow job, not for the jobs that Nextflow itself will submit!\n#SBATCH --account=PAS2658\n#SBATCH --time=3:00:00\n#SBATCH --mail-type=END,FAIL\n#SBATCH --output=slurm-nfc_rnaseq-%j.out\n\n--account=PAS2658: As always, we have to specify the OSC project.\n--time=3:00:00: Ask for 3 hours (note that for a run of a full data set, you may want to use 6-24 hours).\n--mail-type=END,FAIL: Have Slurm send us an email when the job ends normally or with an error.\n--output=slurm-nfc_rnaseq-%j.out: Use a descriptive Slurm log file name (%j is the Slurm job number).\n\nWe only a need a single core and up to a couple GB of RAM, so the associated Slurm defaults will work for us.\n\n\n\n4.4 The final script\nWe’ve covered most of the pieces of our script. Below is the full code for the script, in which I also added:\n\nA shebang header line to indicate that this is a Bash shell script: #!/bin/bash.\nA line to use “strict Bash settings”, set -euo pipefail5.\nSome echo reporting of arguments/variables, printing the date, etc.\n\n Open your scripts/nfc-rnaseq.sh script and paste the following into it:\n#!/bin/bash\n#SBATCH --account=PAS2658\n#SBATCH --time=3:00:00\n#SBATCH --mail-type=END,FAIL\n#SBATCH --output=slurm-nfc_rnaseq-%j.out\n\n# Settings and constants\nWORKFLOW_DIR=software/nfc-rnaseq/3_14_0\n\n# Load the Nextflow Conda environment\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/nextflow\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS0471/containers\n\n# Strict Bash settings\nset -euo pipefail\n\n# Process command-line arguments\nsamplesheet=$1\nfasta=$2\ngtf=$3\noutdir=$4\n\n# Report\necho \"Starting script nfc-rnaseq.sh\"\ndate\necho \"Samplesheet:          $samplesheet\"\necho \"Reference FASTA:      $fasta\"\necho \"Reference GTF:        $gtf\"\necho \"Output dir:           $outdir\"\necho\n\n# Create the output dir\nmkdir -p \"$outdir\"\n\n# Create the config file\necho \"\nprocess.executor = 'slurm'\nprocess.clusterOptions='--account=PAS2658'\n\" &gt; nextflow.config\n\n# Run the workflow\nnextflow run \"$WORKFLOW_DIR\" \\\n    --input \"$samplesheet\" \\\n    --fasta \"$fasta\" \\\n    --gtf \"$gtf\" \\\n    --outdir \"$outdir\" \\\n    --remove_ribo_rna \\\n    -work-dir \"$outdir\"/raw \\\n    -profile singularity \\\n    -ansi-log false \\\n    -resume\n\n# Report\necho \"Done with script nfc-rnaseq.sh\"\ndate\n\n\n Exercise: Take a look at the script\nGo through your complete scripts/nfc-rnaseq.sh script and see if you understand everything that’s going on in there. Ask if you’re confused about anything!"
  },
  {
    "objectID": "rnaseq/02_nfc-rnaseq.html#running-the-pipeline",
    "href": "rnaseq/02_nfc-rnaseq.html#running-the-pipeline",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "5 Running the pipeline",
    "text": "5 Running the pipeline\nWe will now switch back to the run/run.sh script to add the code to submit our script. But we’ll have to create a sample sheet first.\n\n5.1 Preparing the sample sheet\nThis pipeline requires a “sample sheet” as one of its inputs. In the sample sheet, you provide the paths to your FASTQ files and the so-called “strandedness” of your RNA-Seq library.\n\n\n\n\n\n\nRNA-Seq library strandedness\n\n\n\nDuring RNA-Seq library prep, information about the directionality of the original RNA transcripts can be retained (resulting in a “stranded” library) or lost (resulting in an “unstranded” library: specify unstranded in the sample sheet).\nIn turn, stranded libraries can prepared either in reverse-stranded (reverse, by far the most common) or forward-stranded (forward) fashion. For more information about library strandedness, see this page.\nThe pipeline also allows for a fourth option: auto, in which case the strandedness is automatically determined at the start of the pipeline by pseudo-mapping a small proportion of the data with Salmon.\n\n\nThe sample sheet should be a plain-text comma-separated values (CSV) file. Here is the example file from the pipeline’s documentation:\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\nSo, we need a header row with column names, then one row per sample, and the following columns:\n\nSample ID (we will simply use the part of the file names shared by R1 and R2).\nR1 FASTQ file path (including the dir unless they are in your working dir).\nR2 FASTQ file path (idem).\nStrandedness: unstranded, reverse, forward, or auto — this data is forward-stranded, so we’ll use forward.\n\nYou can create this file in several ways — we will do it here with a helper script that comes with the pipeline6:\n\nFirst, we define an output dir (this will also be the output dir for the pipeline), and the sample sheet file name:\n# [Paste this into run/run.sh and then run it in the terminal]\n\n# Define the output dir and sample sheet file name\noutdir=results/nfc-rnaseq\nsamplesheet=\"$outdir\"/nfc_samplesheet.csv\nmkdir -p \"$outdir\"\nNext, we run that helper script, specifying the strandedness of our data, the suffices of the R1 and R2 FASTQ files, and as arguments at the end, the input FASTQ dir (data/fastq) and the output file ($samplesheet):\n# [Paste this into run/run.sh and then run it in the terminal]\n\n# Create the sample sheet for the nf-core pipeline\npython3 software/nfc-rnaseq/3_14_0/bin/fastq_dir_to_samplesheet.py \\\n    --strandedness forward \\\n    --read1_extension \"_R1.fastq.gz\" \\\n    --read2_extension \"_R2.fastq.gz\" \\\n    data/fastq \\\n    \"$samplesheet\"\nFinally, let’s check the contents of our newly created sample sheet file:\n# [Run this directly in the terminal]\ncat \"$samplesheet\"\nsample,fastq_1,fastq_2,strandedness\nERR10802863,data/fastq/ERR10802863_R1.fastq.gz,data/fastq/ERR10802863_R2.fastq.gz,forward\nERR10802864,data/fastq/ERR10802864_R1.fastq.gz,data/fastq/ERR10802864_R2.fastq.gz,forward\nERR10802865,data/fastq/ERR10802865_R1.fastq.gz,data/fastq/ERR10802865_R2.fastq.gz,forward\nERR10802866,data/fastq/ERR10802866_R1.fastq.gz,data/fastq/ERR10802866_R2.fastq.gz,forward\nERR10802867,data/fastq/ERR10802867_R1.fastq.gz,data/fastq/ERR10802867_R2.fastq.gz,forward\nERR10802868,data/fastq/ERR10802868_R1.fastq.gz,data/fastq/ERR10802868_R2.fastq.gz,forward\nERR10802869,data/fastq/ERR10802869_R1.fastq.gz,data/fastq/ERR10802869_R2.fastq.gz,forward\nERR10802870,data/fastq/ERR10802870_R1.fastq.gz,data/fastq/ERR10802870_R2.fastq.gz,forward\nERR10802871,data/fastq/ERR10802871_R1.fastq.gz,data/fastq/ERR10802871_R2.fastq.gz,forward\nERR10802874,data/fastq/ERR10802874_R1.fastq.gz,data/fastq/ERR10802874_R2.fastq.gz,forward\nERR10802875,data/fastq/ERR10802875_R1.fastq.gz,data/fastq/ERR10802875_R2.fastq.gz,forward\nERR10802876,data/fastq/ERR10802876_R1.fastq.gz,data/fastq/ERR10802876_R2.fastq.gz,forward\nERR10802877,data/fastq/ERR10802877_R1.fastq.gz,data/fastq/ERR10802877_R2.fastq.gz,forward\nERR10802878,data/fastq/ERR10802878_R1.fastq.gz,data/fastq/ERR10802878_R2.fastq.gz,forward\nERR10802879,data/fastq/ERR10802879_R1.fastq.gz,data/fastq/ERR10802879_R2.fastq.gz,forward\nERR10802880,data/fastq/ERR10802880_R1.fastq.gz,data/fastq/ERR10802880_R2.fastq.gz,forward\nERR10802881,data/fastq/ERR10802881_R1.fastq.gz,data/fastq/ERR10802881_R2.fastq.gz,forward\nERR10802882,data/fastq/ERR10802882_R1.fastq.gz,data/fastq/ERR10802882_R2.fastq.gz,forward\nERR10802883,data/fastq/ERR10802883_R1.fastq.gz,data/fastq/ERR10802883_R2.fastq.gz,forward\nERR10802884,data/fastq/ERR10802884_R1.fastq.gz,data/fastq/ERR10802884_R2.fastq.gz,forward\nERR10802885,data/fastq/ERR10802885_R1.fastq.gz,data/fastq/ERR10802885_R2.fastq.gz,forward\nERR10802886,data/fastq/ERR10802886_R1.fastq.gz,data/fastq/ERR10802886_R2.fastq.gz,forward\n\n\n\n\n\n\n\nCreating the sample sheet with shell commands instead (Click to expand)\n\n\n\n\n\n# A) Define the file name and create the header line\necho \"sample,fastq_1,fastq_2,strandedness\" &gt; \"$samplesheet\"\n  \n# B) Add a row for each sample based on the file names\nls data/fastq/* | paste -d, - - |\n    sed -E -e 's/$/,forward/' -e 's@.*/(.*)_R1@\\1,&@' &gt;&gt; \"$samplesheet\"\nHere is an explanation of the last command:\n\nThe ls command will spit out a list of all FASTQ files that includes the dir name.\npaste - - will paste that FASTQ files side-by-side in two columns — because there are 2 FASTQ files per sample, and they are automatically correctly ordered due to their file names, this will create one row per sample with the R1 and R2 FASTQ files next to each other.\nThe -d, option to paste will use a comma instead of a Tab to delimit columns.\nWe use sed with extended regular expressions (-E) and two separate search-and-replace expressions (we need -e in front of each when there is more than one).\nThe first sed expression 's/$/,forward/' will simply add ,forward at the end ($) of each line to indicate the strandedness.\nThe second sed expression, 's@.*/(.*)_R1@\\1,&@':\n\nHere we are adding the sample ID column by copying that part from the R1 FASTQ file name.\nThis uses s@&lt;search&gt;@replace@ with @ instead of /, because there is a / in our search pattern.\nIn the search pattern (.*/(.*)_R1), we capture the sample ID with (.*).\nIn the replace section (\\1,&), we recall the captured sample ID with \\1, then insert a comma, and then insert the full search pattern match (i.e., the path to the R1 file) with &.\n\nWe append (&gt;&gt;) to the file because we need to keep the header line that we had already put in it.\n\n\n\n\n\n\n\n5.2 Submitting our shell script\nAs a last preparatory step, we will save the paths of the reference genome files in variables:\n# [Paste this into run/run.sh and then run it in the terminal]\n# Define the reference genome files\nfasta=data/ref/GCF_016801865.2.fna\ngtf=data/ref/GCF_016801865.2.gtf\nBefore we submit the script, let’s check that all the variables have been assigned by prefacing the command with echo:\n# [ Run this directly in the terminal]\necho sbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\"\nsbatch scripts/nfc-rnaseq.sh results/nfc-rnaseq/nfc_samplesheet.csv data/ref/GCF_016801865.2.fna data/ref/GCF_016801865.2.gtf results/nfc-rnaseq\nNow we are ready to submit the script as a batch job:\n# [Paste this into run/run.sh and then run it in the terminal]\n# Submit the script to run the pipeline as a batch job\nsbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\"\nSubmitted batch job 27767854\n\n\n\n5.3 Checking the pipeline’s progress\nWe can check whether our job has started running, and whether Nextflow has already spawned jobs, with squeue:\n# [Run this directly in the terminal]\nsqueue -u $USER -l\nMon Mar 25 12:13:38 2024\n      JOBID PARTITION     NAME     USER    STATE   TIME TIME_LIMI  NODES NODELIST(REASON)\n  27767854 serial-40 nfc-rnas   jelmer  RUNNING    1:33   3:00:00      1 p0219\nIn the example output above, the only running job is the one we directly submitted, i.e. the main Nextflow process. Because we didn’t give the job a name, the NAME column is the script’s name, nfc-rnaseq.sh (truncated to nfc-rnas).\n\n\n\n\n\n\nSee examples of squeue output that includes Nextflow-submitted jobs (Click to expand)\n\n\n\n\n\nThe top job, with partial name nf-NFCOR, is a job that’s been submitted by Nextflow:\nsqueue -u $USER -l\nMon Mar 25 13:14:53 2024\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          27767861 serial-40 nf-NFCOR   jelmer  RUNNING       5:41  16:00:00      1 p0053\n          27767854 serial-40 nfc_rnas   jelmer  RUNNING    1:03:48   3:00:00      1 p0219\nUnfortunately, the columns in the output above are quite narrow, so it’s not possible to see which step of the pipeline is being run by that job. The following (awful-looking!) code can be used to make that column much wider, so we can see the job’s full name which makes clear which step is being run (rRNA removal with SortMeRNA):\nsqueue -u $USER --format=\"%.9i %.9P %.60j %.8T %.10M %.10l %.4C %R %.16V\"\nMon Mar 25 13:15:05 2024\n    JOBID PARTITION                                                          NAME    STATE       TIME TIME_LIMIT CPUS NODELIST(REASON)      SUBMIT_TIME\n 27767861 serial-40   nf-NFCORE_RNASEQ_RNASEQ_SORTMERNA_(SRR27866691_SRR27866691)  RUNNING       5:55   16:00:00   12 p0053 2024-03-23T09:37\n 27767854 serial-40                                                    nfc_rnaseq  RUNNING    1:04:02    3:00:00    1 p0219 2024-03-23T09:36\nYou might also catch the pipeline while there are many more jobs running, e.g.:\nMon Mar 25 13:59:50 2024\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          27823107 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0091\n          27823112 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0119\n          27823115 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0055\n          27823120 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0055\n          27823070 serial-40 nf-NFCOR   jelmer  RUNNING       0:43  16:00:00      1 p0078\n          27823004 serial-40 nfc-rnas   jelmer  RUNNING       2:13   3:00:00      1 p0146\n          27823083 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0078\n          27823084 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0096\n          27823085 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0096\n          27823086 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0115\n          27823087 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0115\n          27823088 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0123\n          27823089 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0123\n          27823090 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0057\n          27823091 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0057\n          27823092 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0058\n          27823093 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0058\n          27823095 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0118\n          27823099 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0118\n          27823103 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0119\n          27823121 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0625\n          27823122 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0744\n          27823123 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0780\n          27823124 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0780\n\n\n\nWe can keep an eye on the pipeline’s progress, and see if there are any errors, by checking the Slurm log file — the top of the file should look like this:\n# You will have a different job ID - replace as appropriate or use Tab completion\nless slurm-nfc_rnaseq-27767861.out\nStarting script nfc-rnaseq.sh\nMon Mar 25 13:01:30 EDT 2024\nSamplesheet:          results/nfc-rnaseq/nfc_samplesheet.csv\nReference FASTA:      data/ref/GCF_016801865.2.fna\nReference GTF:        data/ref/GCF_016801865.2.gtf\nOutput dir:           results/nfc-rnaseq\n\nN E X T F L O W  ~  version 23.10.1\nWARN: It appears you have never run this project before -- Option `-resume` is ignored\nLaunching `software/nfc-rnaseq/3_14_0/main.nf` [curious_linnaeus] DSL2 - revision: 746820de9b\nWARN: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  Multiple config files detected!\n  Please provide pipeline parameters via the CLI or Nextflow '-params-file' option.\n  Custom config files including those provided by the '-c' Nextflow option can be\n  used to provide any configuration except for parameters.\n\n  Docs: https://nf-co.re/usage/configuration#custom-configuration-files\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq v3.14.0\n------------------------------------------------------\nCore Nextflow options\n  runName                   : curious_linnaeus\n  containerEngine           : singularity\n[...output truncated...]\nThe warnings about -resume and config files shown above can be ignored. Some of this output actually has nice colors:\n\n\n\n\n\nIn the Slurm log file, the job progress is show in the following way — we only see which jobs are being submitted, not when they finish7:\n[e5/da8328] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_FILTER (GCF_016801865.2.fna)\n[b5/9427a1] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (GCF_016801865.2.fna)\n[05/e0e09f] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802863)\n[25/a6c2f5] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802863)\n[24/cef9a0] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802864)\n[b1/9cfa7e] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802864)\n[c4/3107c1] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802865)\n[7e/92ec89] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802866)\n[01/f7ccfb] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802866)\n[42/4b4da2] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802868)\n[8c/fe6ca5] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802867)\n[e6/a12ec8] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802867)\n[2e/f9059d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802865)\n[de/2735d1] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802868)\n\n\nYou should also see the following warning among the job submissions (Click to expand)\n\nThis warning can be ignored, the “Biotype QC” is not important and this information is indeed simply missing from our GTF file, there is nothing we can do about that.\nWARN: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  Biotype attribute 'gene_biotype' not found in the last column of the GTF file!\n\n  Biotype QC will be skipped to circumvent the issue below:\n  https://github.com/nf-core/rnaseq/issues/460\n\n  Amend '--featurecounts_group_type' to change this behaviour.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBut any errors would be reported in this file, and we can also see when the pipeline has finished:\n[28/79e801] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRAPH_BEDCLIP_BEDGRAPHTOBIGWIG_FORWARD:UCSC_BEDGRAPHTOBIGWIG (ERR10802864)\n[e0/ba48c9] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRAPH_BEDCLIP_BEDGRAPHTOBIGWIG_REVERSE:UCSC_BEDGRAPHTOBIGWIG (ERR10802864)\n[62/4f8c0d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQC (1)\n-[nf-core/rnaseq] Pipeline completed successfully -\nDone with script nfc-rnaseq.sh\nMon Mar 25 14:09:52 EDT 2024"
  },
  {
    "objectID": "rnaseq/02_nfc-rnaseq.html#checking-the-pipelines-outputs",
    "href": "rnaseq/02_nfc-rnaseq.html#checking-the-pipelines-outputs",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "6 Checking the pipeline’s outputs",
    "text": "6 Checking the pipeline’s outputs\nIf your pipeline run finished in time (it may finish in as little as 15-30 minutes, but this can vary substantially8), you can take a look at the files and dirs in the output dir we specified:\nls -lh results/nfc-rnaseq\ntotal 83K\ndrwxr-xr-x   2 jelmer PAS0471  16K Mar 25 13:02 fastqc\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 12:58 logs\ndrwxr-xr-x   3 jelmer PAS0471 4.0K Mar 25 13:14 multiqc\n-rw-r--r--   1 jelmer PAS0471 2.0K Mar 25 19:55 nfc_samplesheet.csv\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 13:14 pipeline_info\ndrwxr-xr-x 248 jelmer PAS0471  16K Mar 25 13:10 raw\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 13:06 sortmerna\ndrwxr-xr-x  33 jelmer PAS0471  16K Mar 25 13:12 star_salmon\ndrwxr-xr-x   3 jelmer PAS0471 4.0K Mar 25 13:02 trimgalore\nThe two outputs we are most interested in are:\n\nThe MultiQC report (&lt;outdir&gt;/multiqc/star_salmon/multiqc_report.html): this has lots of QC summaries of the data, both the raw data and the alignments, and even a gene expression PCA plot.\nThe gene count table (&lt;outdir&gt;/star_salmon/salmon.merged.gene_counts_length_scaled.tsv): if you do the Gene count table analysis lab, you will use an equivalent file (but then run on the full data set) as the main input.\n\n\n\n6.1 The MultiQC report\nYou can find a copy of the MultiQC report on this website, here. Go ahead and open that in a separate browser tab. There’s a lot of information in the report! Here are some items to especially pay attention to, with figures from our own data set:\n\nThe General Statistics table (the first section) is very useful, with the following notes:\n\nMost of the table’s content is also in later graphs, but the table allows for comparisons across metrics.\nThe %rRNA (% of reads identified as rRNA and removed by SortMeRNA) can only be found in this table.\nIt’s best to hide the columns with statistics from Samtools, which can be confusing if not downright misleading: click on “Configure Columns” and uncheck all the boxes for stats with Samtools in their name.\nSome stats are for R1 and R2 files only, and some are for each sample as a whole. Unfortunately, this means you get 3 rows per sample in the table.\n\n\n\n\n\n\n\n\n\nThe Qualimap &gt; Genomic origin of reads plot shows, for each sample, the proportion of reads mapping to exonic vs. intronic vs. intergenic regions. This is an important QC plot: the vast majority of your reads should be exonic9.\n\n\n\n\nThis is a good result, with 80-90% of mapped reads in exonic regions.\n\n\n\n\nThe STAR &gt; Alignment Scores plot shows, for each sample, the percentage of reads that was mapped. Note that “Mapped to multiple loci” reads are also included in the final counts, and that “Unmapped: too short” merely means unmapped, really, and not that the reads were too short.\n\n\n\n\nThis is a pretty good results, with 80-90% of reads mapped.\n\n\n\n\nFastQC checks your FASTQ files, i.e. your data prior to alignment. There are FastQC plots both before and after trimming with TrimGalore/Cutadapt. The most important FastQC modules are:\n\nSequence Quality Histograms — You’d like the mean qualities to stay in the “green area”.\nPer Sequence GC Content — Secondary peaks may indicate contamination.\nAdapter Content — Any adapter content should be gone in the post-trimming plot.\n\n\n\n\n Exercise: Interpreting FastQC results in the MultiQC report\nTake a look at the three FastQC modules discussed above, both before and after trimming.\n\nHas the base quality improved after trimming, and does this look good?\n\n\n\nClick to see the answer\n\n\nPre-trimming graph: The qualities are good overall, but there is more variation that what is usual, and note the poorer qualities in the first 7 or so bases. There is no substantial decline towards the end of the read as one often sees with Illumina data, but this is expected given that the reads are only 75 bp.\n\n\n\n\nPre-trimming (Mean base quality scores: one line is one sample.)\n\n\n\nPost-trimming graph: The qualities have clearly improved. The first 7 or so bases remain of clearly poorer quality, on average.\n\n\n\n\nPost-trimming\n\n\n\n\nDo you have any idea what’s going with the pre-trimming GC content distribution? What about after trimming — does this look good or is there reason to worry?\n\n\n\nClick to see the answer\n\n\nThe pre-trimming GC content is very odd but this is mostly due to a high number of reads with zero and near-zero percent GC content. These are likely reads with only Ns. There are also some reads with near-hundred percent GC content. These are likely artifactual G-only reads that NextSeq/NovaSeq machines can produce.\n\n\n\n\nPre-trimming. One line is one file.\n\n\n\nAfter trimming, things look a lot better but there may be contamination here, given the weird “shoulder” at 30-40% GC.\n\n\n\n\nPost-trimming\n\n\n\n\nDo you know what the “adapters” that FastQC found pre-trimming are? Were these sequences removed by the trimming?\n\n\n\nClick to see the answer\n\n\nPre-trimming, there seem to be some samples with very high adapter content throughout the read. This doesn’t make sense for true adapters, because these are usually only found towards the end of the read, when the read length is longer than the DNA fragment length. If you hover over the lines, you’ll see it says “polyg”. These are artifactual G-only reads that NextSeq/NovaSeq can produce, especially in the reverse reads — and you can see that all of the lines are for reverse-read files indeed.\n\n\n\n\nPre-trimming\n\n\n\nPost-trimming, no adapter content was found.\n\n\n\n\nPost-trimming\n\n\n\n\n\n\n\n\n\n\n\nSome additional graphs of interest in the MultiQC report (Click to expand)\n\n\n\n\n\n\nThe Qualimap &gt; Gene Coverage Profile plot. This shows average read-depth across the position of genes/transcripts (for all genes together), which helps to assess the amount of RNA degradation. For poly-A selected libraries, RNA molecules “begin” at the 3’ end (right-hand side of the graph), so the more degradation there is, the more you expect there to be a higher read-depth towards the 3’ end compared to the 5’ end. (Though note that sharp decreases at the very end on each side are expected.)\n\n\n\n\nThere depth at ~20% (near the 5’ end) is clearly lower than at ~80% (near the 3’ end),indicating some RNA degradation.\n\n\n\nThe RSeqQC &gt; Infer experiment (library strandedness) plot. If your library is:\n\nUnstranded, there should be similar percentages of Sense and Antisense reads.\nForward-stranded, the vast majority of reads should be Sense.\nReverse-stranded, the vast majority of reads should be Antisense.\n\n\n\n\n\nThis libary is clearly forward-stranded, as we indicated in our sample sheet.\n\n\n\nThe STAR_SALMON DESeq2 PCA plot is from a Principal Component Analysis (PCA) run on the final gene count table, thus showing overall patterns of gene expression similarity among samples.\n\n\n\n\nThe sampels clear form two distinct groups along PC1.\n\n\n\n\n\n\n\n\n\n\n\nDid your pipeline run finish? Here’s how to check out your own MultiQC report (Click to expand)\n\n\n\n\n\nTo download the MultiQC HTML file at results/nfc-rnaseq/multiqc/star_salmon/multiqc_report.html, find this file in the VS Code explorer (file browser) on the left, right-click on it, and select Download....\nYou can download it to any location on your computer. Then find the file on your computer and click on it to open it — it should be opened in your browser.\n\n\n\n\n\n\n6.2 The gene count table\nThe gene count table has one row for each gene and one column for each sample, with the first two columns being the gene_id and gene_name10. Each cell’s value contains the read count estimate for a specific gene in a specific sample:\n# [Paste this into the run/run.sh script and run it in the terminal]\n\n# Take a look at the count table:\n# ('column -t' lines up columns, and less's '-S' option turns off line wrapping)\ncounts=results/nfc-rnaseq/star_salmon/salmon.merged.gene_counts_length_scaled.tsv\ncolumn -t \"$counts\" | less -S\ngene_id             gene_name           ERR10802863        ERR10802864        ERR10802865        ERR10802866        ERR10802867        ERR10802868       \nATP6                ATP6                163.611027228009   178.19903533081    82.1025390726658   307.649552934133   225.78249209207    171.251589309856  \nATP8                ATP8                0                  1.01047333891691   0                  0                  0                  0                 \nCOX1                COX1                1429.24769032452   2202.82009602881   764.584344577622   2273.6965332904    2784.47391614249   2000.51277019854  \nCOX2                COX2                116.537361366535   175.137972566817   54.0166352459629   256.592955351283   193.291937038438   164.125833130119  \nCOX3                COX3                872.88670991359    1178.29247734231   683.167933227141   1200.01735304529   1300.3853323715    1229.11746824104  \nCYTB                CYTB                646.028108528182   968.256051104547   529.393909319439   1025.23768317788   1201.46662840336   842.533209911258  \nLOC120412322        LOC120412322        0                  0                  0                  0                  0.995135178345792  0.996805450081561 \nLOC120412324        LOC120412324        37.8326244586681   20.9489661184365   27.6702324729125   48.6417838830061   22.8313729348804   36.87899862428    \nLOC120412325        LOC120412325        3.21074365394071   2.10702898851342   4.40315394778926   5.47978997387391   4.33241716734803   4.23386924919438  \nLOC120412326        LOC120412326        0                  0                  0                  0                  0                  0                 \nLOC120412327        LOC120412327        37.8206758601034   35.9063291323018   38.517771617566    27.7802608986967   37.6979028802121   32.885944667709   \nLOC120412328        LOC120412328        35.0080600370267   20.0019192467143   23.9260736995594   30.0191332346116   21.0383665366408   28.9844776623531  \nLOC120412329        LOC120412329        121.777922287929   112.794544755113   131.434181046282   127.753086659103   114.864750589664   131.589608063253  \nLOC120412330        LOC120412330        42.8505448763697   28.9442284428204   36.6285174684674   46.7310765909945   42.7633834468768   26.9265243413636  \nLOC120412331        LOC120412331        11.013179311581    9.00559907892481   12.9836833055803   13.029954361225    7.02624958751718   16.000552787954   \nLOC120412332        LOC120412332        12.1055360835441   26.1231316926989   21.2767913384733   18.2783703626438   26.4932540325187   22.098808637857   \nLOC120412333        LOC120412333        19.1159998132169   17.0558058070299   12.0965688236319   14.1510477997588   15.2033452089903   9.02624985028677  \nLOC120412334        LOC120412334        9.01332125155807   3.00232591636489   5.99566364212933   11.0306919231504   8.03448732510427   11.0022053123759  \n# [...output truncated...]\n\n\n\n\n\n\nCount table versions\n\n\n\nThe workflow outputs several versions of the count table11, but the one with gene_counts_length_scaled is the one we want:\n\ngene_counts as opposed to transcript_counts for counts that are summed across transcripts for each gene.\nlength for estimates that have been adjusted to account for between-sample differences in mean transcript length (longer transcripts would be expected to produce more reads in sequencing).\nscaled for estimates that have been scaled back using the “library sizes”, per-sample total read counts."
  },
  {
    "objectID": "rnaseq/02_nfc-rnaseq.html#footnotes",
    "href": "rnaseq/02_nfc-rnaseq.html#footnotes",
    "title": "Lab: Running the nf-core rnaseq pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n These kinds of settings are more commonly specified with command options, but somewhat oddly, this is the only way we can specify that here.↩︎\n But if you want to run a pipeline yourself for your own research, make sure to use a dir that you have permissions to write to.↩︎\n Preferably in GTF (.gtf) format, but the pipeline can accept GFF/GFF3 (.gff/.gff3) format files as well.↩︎\n The default logging does not work well the output goes to a text file, as it will in our case because we will submit the script with the Nextflow command as a Slurm batch job.↩︎\n These setting will make the script abort whenever an error occurs, and it will also turn referencing unassigned/non-existing variables into an error. This is a recommended best-practice line to include in all shell scripts.↩︎\n The box below shows an alternative method with Unix shell commands↩︎\n The default Nextflow logging (without -ansi-log false) does show when jobs finish, but this would result in very messy output in a Slurm log file.↩︎\n This variation is mostly the result of variation in Slurm queue-ing times. The pipeline makes quite large resource requests, so you sometimes have to wait for a while for some jobs to start.↩︎\n A lot of intronic content may indicate that you have a lot of pre-mRNA in your data; this is more common when your library prep used rRNA depletion instead of poly-A selection. A lot of intergenic content may indicate DNA contamination. Poor genome annotation quality may also contribute to a low percentage of exonic reads. The RSeQC &gt; Read Distribution plot will show this with even more categories, e.g. separately showing UTRs.↩︎\n Which happen to be the same here, but these are usually different.↩︎\n And each version in two formats: .rds (a binary R object file type) and .tsv.↩︎"
  },
  {
    "objectID": "nf-run/02_nfcore.html#introduction",
    "href": "nf-run/02_nfcore.html#introduction",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "1 Introduction",
    "text": "1 Introduction\nTo learn how to run a Nextflow/nf-core pipeline, we’ll use nf-core rnaseq, which performs a reference-based RNA-seq analysis.\n\nGeneral mechanics of running a pipeline\nRunning a pipeline like this is a bit different, and more involved, than running a typical piece of bioinformatics software:\n\nThe pipeline submits Slurm batch jobs for us, and tries to parallelize as much as possible, spawning many jobs. The main pipeline process functions only to orchestrate these jobs, and will keep running until the pipeline has finished or failed. We will need a small “config file” to tell the pipeline how to submit jobs.\nWe only need an installation of Nextflow, and a download of the pipeline’s code and associated files. The pipeline then runs all its constituent tools via separate Singularity containers that it will download1.\nNextflow distinguishes between a final output dir and a “work dir”. All processes/jobs will run and produce outputs in various sub-dirs of the work dir. After each process, only certain output files are copied to the final output dir2. This distinction is very useful at OSC, where a Scratch dir is most suitable as the work dir (due to its fast I/O and ample storage space), while a Project dir is most suitable as the final output dir.\nWhen you run a pipeline, there is a distinction between:\n\nPipeline-specific options to e.g. set inputs and outputs and customize which steps will be run and how (there can be 100+ of these for larger pipelines…). These by convention use a double dash --, e.g. --input.\nGeneral Nextflow options to e.g. pass a configuration file for Slurm batch job submissions and determine resuming/rerunning behavior (see below). These always use a single dash, e.g. -resume.\n\n\n\n\nResuming a pipeline\nFinally, we talked about the need for flexible rerunning of parts of a pipeline earlier. Nextflow can do this when you use the -resume option, which will make a pipeline, for example:\n\nStart where it left off if the previous run failed before finishing, or timed out.\nCheck for changes in input files or pipeline settings, such that it will only rerun what is needed:\n\nAfter adding or removing samples.\nAfter changing/replacing the reference genome files, which would be all steps that make use of the affected file(s) and anything downstream of these steps.\nGiven the settings that have changed; a setting only affecting the very last step will mean that only that step has to be rerun."
  },
  {
    "objectID": "nf-run/02_nfcore.html#the-nf-core-rnaseq-pipeline",
    "href": "nf-run/02_nfcore.html#the-nf-core-rnaseq-pipeline",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "2 The nf-core rnaseq pipeline",
    "text": "2 The nf-core rnaseq pipeline\nThe nf-core rnaseq pipeline is meant for RNA-seq projects that:\n\nAttempt to sequence only mRNA while avoiding non-coding RNAs (“mRNA-seq”).\nDo not distinguish between RNA from different cell types (“bulk RNA-seq”).\nUse short reads (≤150 bp) that do not cover full transcripts but do uniquely ID genes.\nUse reference genomes (are reference-based) to associate reads with genes.\nDownstream of this pipeline, such projects typically aim to statistically compare expression between groups of samples, and have multiple biological replicates per group.\n\nThat might seem quite specific, but this is by far the most common use RNA-seq use case. The inputs of this pipeline are FASTQ files with raw reads, and reference genome files (assembly & annotation), while the outputs include a gene count table and many “QC outputs”.\n\n\n\n\n\n\nWhat the pipeline does and does not do (Click to expand)\n\n\n\n\n\nThere are typically two main parts to the kind of RNA-seq data analysis I just described, but this pipeline only does the first:\n\nFrom reads to counts: yes\nGenerating a count table using the reads & the reference genome.\nCount table analysis: no\nDifferential expression analysis, function enrichment analysis, etc.\n\nThat makes sense because the latter part is not nearly as “standardized” as the first. It also does not need much computing power or parallelization, and is best done interactively using a language like R.\n\n\n\n\n\nThe main steps\nLet’s take a closer look at the steps in the pipeline:\n\n\n\n\n\nRead QC and pre-processing\n\nRead QC (FastQC)\nAdapter and quality trimming (TrimGalore)\nOptional removal of rRNA (SortMeRNA) — off by default, but we will include this\n\nAlignment & quantification\n\nAlignment to the reference genome/transcriptome (STAR)\nGene expression quantification (Salmon)\n\nPost-processing, QC, and reporting\n\nPost-processing alignments: sort, index, mark duplicates (samtools, Picard)\nAlignment/count QC (RSeQC, Qualimap, dupRadar, Preseq, DESeq2)\nCreate a QC/metrics report (MultiQC)\n\n\n\n\n\n\n\n\nCustomizing what the pipeline runs (Click to expand)\n\n\n\n\n\nThis pipeline is quite flexible and you can turn several steps off, add optional steps, and change individual options for most tools that the pipeline runs.\n\nOptional removal of contaminants (BBSplit)\nMap to 1 or more additional genomes whose sequences may be present as contamination, and remove reads that map better to contaminant genomes.\nAlternative quantification routes\n\nUse RSEM instead of Salmon to quantify.\nSkip STAR and perform direct pseudo-alignment & quantification with Salmon.\n\nTranscript assembly and quantification (StringTie)\nWhile the pipeline is focused on gene-level quantification, it does produce transcript-level counts as well (this is run by default)."
  },
  {
    "objectID": "nf-run/02_nfcore.html#getting-set-up",
    "href": "nf-run/02_nfcore.html#getting-set-up",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "3 Getting set up",
    "text": "3 Getting set up\n\n3.1 How we’ll run the pipeline\nThe entire pipeline can be run with a single command. But we do need to do some prep before we can do so, such as:\n\nActivating the software environment and downloading the pipeline files.\nDefining the pipeline’s inputs and outputs, which includes creating a “sample sheet”.\nCreating a small “config file” so Nextflow knows how to submit Slurm batch jobs at OSC.\n\nThe main Nextflow process does not need much computing power (a single core with the default 4 GB of RAM will be sufficient). But even though our VS Code shell already runs on a compute node, we are better off submitting the main process as a batch job as well, because this process can run for many hours, and we want to be able to log off in the mean time.\nLike we’ve done before, we will use both a primary script that will be submitted as a batch job, and a runner script with commands to run interactively including the submission of said batch job:\nmkdir -p week06/nfc-rnaseq\ncd week06/nfc-rnaseq\nmkdir scripts run software\ntouch scripts/nfc-rnaseq.sh run/run.sh\nOpen the run/run.sh in the VS Code editor pane.\n\n\n\n3.2 Activating the Conda environment\nIf you did last week’s exercises, you created a Conda environment with Nextflow and nf-core tools. If not, you can use my Conda environment. We will activate this environment in our runner script because we’ll first interactively download the pipeline files.\n\n\nCode to create this Conda environment\n\nmodule load miniconda3/23.3.1-py310\nconda create -y -n nextflow -c bioconda nextflow=23.10.1 nf-core=2.13.1\n\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n# Activate the Conda environment (or use mine: /fs/ess/PAS0471/jelmer/conda/nextflow)\nmodule load miniconda3/23.3.1-py310\nconda activate nextflow-23.10\nCheck that Nextflow and nf-core tools can be run by printing the versions:\n# [Run this code directly in the terminal]\nnextflow -v\nnextflow version 23.10.1.5891\n# [Run this code directly in the terminal]\nnf-core --version\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.13.1 - https://nf-co.re\n\nnf-core, version 2.13.1\n\n\n\n3.3 Downloading the pipeline\nWe’ll use the nf-core download command to download the rnaseq pipeline’s files, including the Singularity containers for all individual tools that the pipeline runs.\nFirst, we need to set the environment variable NXF_SINGULARITY_CACHEDIR to tell Nextflow where to store these containers3. Here, we will use a shared PAS2700 dir that already has all the containers, to save some downloading time and storage space — this took around 15 minutes to download. But when you run a pipeline for your own research, you’ll want to use a dir that you have permissions to write to.\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n# Create an environment variable for the container dir\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS2700/containers\nNext, we’ll run the nf-core download command to download the latest version (3.14.0) of the rnaseq pipeline to software/nfc-rnaseq, and the associated container files to the previously specified dir:\n# [Paste this code into the run/run.sh script, then run it in the terminal]\n# Download the nf-core rnaseq pipeline files\nnf-core download rnaseq \\\n    --revision 3.14.0 \\\n    --outdir software/nfc-rnaseq \\\n    --compress none \\\n    --container-system singularity \\\n    --container-cache-utilisation amend \\\n    --download-configuration\n\n\n\n\n\n\n\n\n\n\n\nExplanation of all options given to nf-core download (Click to expand)\n\n\n\n\n\n\n--revision: The version of the rnaseq pipeline.\n--outdir: The dir to save the pipeline definition files.\n--compress: Whether to compress the pipeline files — we chose not to.\n--container-system: The type of containers to download. This should always be singularity at OSC, because that’s the only supported type.\n--container-cache-utilisation: This is a little technical and not terribly interesting, but we used amend, which will make it check our $NXF_SINGULARITY_CACHEDIR dir for existing containers, and simply download any that aren’t already found there.\n--download-configuration: This will download some configuration files that we will actually not use, but if you don’t provide this option, it will ask you about it when you run the command.\n\nAlso, don’t worry about the following warning, this doesn’t impact the downloading:\n\nWARNING Could not find GitHub authentication token. Some API requests may fail.\n\n\n\n\n\nLet’s take a quick peek at the dirs and files we just downloaded:\n# [Run this code directly in the terminal]\nls software/nfc-rnaseq\n3_14_0  configs\n# [Run this code directly in the terminal]\nls software/nfc-rnaseq/3_14_0\nassets        CODE_OF_CONDUCT.md  LICENSE       nextflow.config       subworkflows\nbin           conf                main.nf       nextflow_schema.json  tower.yml\nCHANGELOG.md  docs                modules       pyproject.toml        workflows\nCITATIONS.md  lib                 modules.json  README.md"
  },
  {
    "objectID": "nf-run/02_nfcore.html#defining-inputs-and-outputs",
    "href": "nf-run/02_nfcore.html#defining-inputs-and-outputs",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "4 Defining inputs and outputs",
    "text": "4 Defining inputs and outputs\nNext, we will define the pipeline’s inputs and outputs in the runner script. We’ll save these in variables, which we’ll later pass as arguments to the primary script. We will need the following inputs:\n\nInput 1: A sample sheet: a text file pointing to the FASTQ files (we’ll create this in a bit)\nInput 2: A FASTA reference genome assembly file (we already have this)\nInput 3: A GTF reference genome annotation file (we already have this)\n\n(If you’re not familiar with FASTA or GTF files, take a look a this self-study section at the bottom of this page.)\nAnd we will need these outputs:\n\nOutput 1: The desired output dir for the final pipeline output\nOutput 2: The desired Nextflow “workdir” for the initial pipeline output\n\n# [Paste this into run/run.sh and then run it in the terminal]\n# Defining the pipeline outputs\noutdir=results/nfc-rnaseq\nworkdir=/fs/scratch/PAS2700/week6/nfc-rnaseq/$USER\n\n# Defining the pipeline inputs\nsamplesheet=\"$outdir\"/nfc_samplesheet.csv\nfasta=../../garrigos_data/ref/GCF_016801865.2.fna\ngtf=../../garrigos_data/ref/GCF_016801865.2.gtf\nJust to give you an idea, here is how we will eventually run the primary script (that we have not yet written), passing those inputs and outputs as arguments:\n# [Don't copy or run this yet]\nsbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\" \"$workdir\"\n\n\n4.1 Preparing the sample sheet\nThis pipeline requires a “sample sheet” as one of its inputs. In the sample sheet, you provide the paths to your FASTQ files and the so-called “strandedness” of your RNA-Seq library.\n\n\n\n\n\n\nRNA-Seq library strandedness (Click to expand)\n\n\n\n\n\nDuring RNA-Seq library prep, information about the directionality of the original RNA transcripts can be retained (resulting in a “stranded” library) or lost (resulting in an “unstranded” library: specify unstranded in the sample sheet).\nIn turn, stranded libraries can prepared either in reverse-stranded (reverse, by far the most common) or forward-stranded (forward) fashion. For more information about library strandedness, see this page.\nThe pipeline also allows for a fourth option: auto, in which case the strandedness is automatically determined at the start of the pipeline by pseudo-mapping a small proportion of the data with Salmon.\n\n\n\nThe sample sheet should be a plain-text comma-separated values (CSV) file. Here is the example file from the pipeline’s documentation:\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\nSo, we need a header row with column names, then one row per sample, and the following columns:\n\nSample ID (we will simply use the file name part that is shared by R1 and R2).\nR1 FASTQ file path (including the dir, unless these files are in your working dir).\nR2 FASTQ file path (idem).\nStrandedness: unstranded, reverse, forward, or auto. This data is forward-stranded, so we’ll use forward.\n\nWe will create this file with a helper script that comes with the pipeline, and tell the script about the strandedness of the reads, the R1 and R2 FASTQ file suffices, the input FASTQ dir (data/fastq), and the output file ($samplesheet):\n# [Paste this into run/run.sh and then run it in the terminal]\n# Create the dir that will contain the sample sheet\nmkdir -p \"$outdir\"\n\n# Create the sample sheet for the nf-core pipeline\npython3 software/nfc-rnaseq/3_14_0/bin/fastq_dir_to_samplesheet.py \\\n    --strandedness forward \\\n    --read1_extension \"_R1.fastq.gz\" \\\n    --read2_extension \"_R2.fastq.gz\" \\\n    ../../garrigos_data/fastq \\\n    \"$samplesheet\"\nCheck the contents of your newly created sample sheet file:\n# [Run this directly in the terminal]\nhead \"$samplesheet\"\nsample,fastq_1,fastq_2,strandedness\nERR10802863,../../garrigos_data/fastq/ERR10802863_R1.fastq.gz,../../garrigos_data/fastq/ERR10802863_R2.fastq.gz,forward\nERR10802864,../../garrigos_data/fastq/ERR10802864_R1.fastq.gz,../../garrigos_data/fastq/ERR10802864_R2.fastq.gz,forward\nERR10802865,../../garrigos_data/fastq/ERR10802865_R1.fastq.gz,../../garrigos_data/fastq/ERR10802865_R2.fastq.gz,forward\nERR10802866,../../garrigos_data/fastq/ERR10802866_R1.fastq.gz,../../garrigos_data/fastq/ERR10802866_R2.fastq.gz,forward\nERR10802867,../../garrigos_data/fastq/ERR10802867_R1.fastq.gz,../../garrigos_data/fastq/ERR10802867_R2.fastq.gz,forward\nERR10802868,../../garrigos_data/fastq/ERR10802868_R1.fastq.gz,../../garrigos_data/fastq/ERR10802868_R2.fastq.gz,forward\nERR10802869,../../garrigos_data/fastq/ERR10802869_R1.fastq.gz,../../garrigos_data/fastq/ERR10802869_R2.fastq.gz,forward\nERR10802870,../../garrigos_data/fastq/ERR10802870_R1.fastq.gz,../../garrigos_data/fastq/ERR10802870_R2.fastq.gz,forward\nERR10802871,../../garrigos_data/fastq/ERR10802871_R1.fastq.gz,../../garrigos_data/fastq/ERR10802871_R2.fastq.gz,forward\n\n\n\n4.2 Creating an OSC configuration file\nTo tell the pipeline how to submit Slurm batch jobs for us, we have to use a configuration (config) file. There are multiple ways of storing this file and telling Nextflow about it — we’ll simply create a file nextflow.config in the dir from which we submit the nextflow run command: Nextflow will automatically detect and parse such a file.\nWe’ll keep this file as simple as possible, specifying only our “executor” program (Slurm) and OSC project:\n# [Paste this into run/run.sh and then run it in the terminal]\n# Create a config file for batch job submissions\necho \"\nprocess.executor='slurm'\nprocess.clusterOptions='--account=PAS2700'\n\" &gt; nextflow.config\n\n\n\n\n\n\nMulti-line echo strings\n\n\n\nIt might look odd, but you can include newlines in echo strings as shown above. Note also that we are using single quotes ('...') inside the string, because a double quote would mark the end the string."
  },
  {
    "objectID": "nf-run/02_nfcore.html#writing-a-shell-script-to-run-the-pipeline",
    "href": "nf-run/02_nfcore.html#writing-a-shell-script-to-run-the-pipeline",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "5 Writing a shell script to run the pipeline",
    "text": "5 Writing a shell script to run the pipeline\nNow, we’ll go through the key parts of the script scripts/nfc-rnaseq.sh that you will submit as a Slurm batch job.\n\n5.1 Processing arguments passed to the script\nThe inputs and outputs we discussed above, and the Nextflow “workdir”, will be the arguments passed to the script:\n# [Partial shell script code, don't copy or run]\n# Process command-line arguments\nsamplesheet=$1\nfasta=$2\ngtf=$3\noutdir=$4\nworkdir=$5\n\n\n\n5.2 Building the nextflow run command\nTo run the pipeline, use the command nextflow run, followed by the path to the dir with pipeline files:\n# [Partial shell script code, don't copy or run]\nnextflow run software/nfc-rnaseq/3_14_0\nAfter that, there are several required options (see the pipeline’s documentation), which represent the inputs and outputs that we talked about above:\n\n--input: The path to the “sample sheet”.\n--fasta: The path to the reference genome assembly FASTA file.\n--gtf: The path to the reference genome annotation GTF file.\n--outdir: The path to the output dir.\n\nThis pipeline has different options for e.g. alignment and quantification. We will stick close to the defaults, which includes alignment with STAR and quantification with Salmon, with one exception: we want to remove reads from ribosomal RNA (this step is skipped by default).\n\n\n Exercise: Find the option to remove rRNA\nTake a look at the “Parameters” tab on the pipeline’s documentation website:\n\nBrowse through the options for a bit to get a feel for the extent to which you can customize the pipeline.\nTry to find the option to turn on removal of rRNA with SortMeRNA.\n\n\n\nClick for the solution\n\nThe option we want is --remove_ribo_rna.\n\n\n\nWe’ll also use several general Nextflow options (note the single dash - option notation):\n\n-profile: A so-called “profile” to indicate how the pipeline should run software — this should be singularity when running the pipeline with Singularity containers.\n-work-dir: The dir in which all the pipeline’s jobs/processes will run.\n-ansi-log false: Change Nextflow’s progress “logging” type to a format that works with Slurm log files4.\n-resume: Resume the pipeline where it “needs to” (e.g., where it left off) instead of always starting over. (This option won’t make any difference when we run the pipeline for the first time, since there is nothing to resume. Nextflow will even give a warning along these lines, but this is not a problem.)\n\nWith all above-mentioned options, your final nextflow run command is:\n# [Partial shell script code, don't copy or run]\nnextflow run software/nfc-rnaseq/3_14_0 \\\n    --input \"$samplesheet\" \\\n    --fasta \"$fasta\" \\\n    --gtf \"$gtf\" \\\n    --outdir \"$outdir\" \\\n    --remove_ribo_rna \\\n    -work-dir \"$workdir\" \\\n    -profile singularity \\\n    -ansi-log false \\\n    -resume\n\n\n\n5.3 The final script\nBelow is the full code for the script, in which I also added:\n\nOur standard shell script header lines.\n#SBATCH options: note that these are only for the “main” Nextflow job, not for the jobs that Nextflow itself will submit! So we ask for quite a bit of time5, but we don’t need more than the default 1 core and 4 GB of RAM.\nSome echo reporting of arguments/variables, printing the date, etc.\n\nOpen your scripts/nfc-rnaseq.sh script and paste the following into it:\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --time=6:00:00\n#SBATCH --mail-type=END,FAIL\n#SBATCH --output=slurm-nfc_rnaseq-%j.out\nset -euo pipefail\n\n# Settings and constants\nWORKFLOW_DIR=software/nfc-rnaseq/3_14_0\n\n# Load the Nextflow Conda environment\nmodule load miniconda3/23.3.1-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/nextflow\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS0471/containers\n\n# Process command-line arguments\nif [[ ! \"$#\" -eq 5 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 5 are required.\"\n    echo \"Usage: nfc-rnaseq.sh &lt;samplesheet&gt; &lt;FASTA&gt; &lt;GTF&gt; &lt;outdir&gt; &lt;workdir&gt;\"\n    exit 1\nfi\nsamplesheet=$1\nfasta=$2\ngtf=$3\noutdir=$4\nworkdir=$5\n\n# Report\necho \"Starting script nfc-rnaseq.sh\"\ndate\necho \"Samplesheet:          $samplesheet\"\necho \"Reference FASTA:      $fasta\"\necho \"Reference GTF:        $gtf\"\necho \"Output dir:           $outdir\"\necho \"Nextflow work dir:    $workdir\"\necho\n\n# Create the output dirs\nmkdir -p \"$outdir\" \"$workdir\"\n\n# Run the workflow\nnextflow run \"$WORKFLOW_DIR\" \\\n    --input \"$samplesheet\" \\\n    --fasta \"$fasta\" \\\n    --gtf \"$gtf\" \\\n    --outdir \"$outdir\" \\\n    --remove_ribo_rna \\\n    -work-dir \"$workdir\" \\\n    -profile singularity \\\n    -ansi-log false \\\n    -resume\n\n# Report\necho \"Done with script nfc-rnaseq.sh\"\ndate"
  },
  {
    "objectID": "nf-run/02_nfcore.html#running-the-pipeline",
    "href": "nf-run/02_nfcore.html#running-the-pipeline",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "6 Running the pipeline",
    "text": "6 Running the pipeline\n\n6.1 Submitting your shell script\nBefore you submit the script, check that all variables have been assigned by prefacing the command with echo:\n# [ Run this directly in the terminal]\necho sbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\" \"$workdir\"\nsbatch scripts/nfc-rnaseq.sh results/nfc-rnaseq/nfc_samplesheet.csv data/ref/GCF_016801865.2.fna data/ref/GCF_016801865.2.gtf results/nfc-rnaseq /fs/scratch/PAS2700/week6/nfc-rnaseq/jelmer\nIf so, you are ready to submit the script as a batch job:\n# [Paste this into run/run.sh and then run it in the terminal]\n# Submit the script to run the pipeline as a batch job\nsbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\" \"$workdir\"\nSubmitted batch job 27767854\n\n\n\nNot sure your run.sh script is complete, or getting errors? Click for its intended content.\n\n# Activate the Conda environment (or use mine: /fs/ess/PAS0471/jelmer/conda/nextflow)\nmodule load miniconda3/23.3.1-py310\nconda activate nextflow-23.10\n\n# Create an environment variable for the container dir\nexport NXF_SINGULARITY_CACHEDIR=/fs/ess/PAS2700/containers\n\n# Download the nf-core rnaseq pipeline files\nnf-core download rnaseq \\\n    --revision 3.14.0 \\\n    --outdir software/nfc-rnaseq \\\n    --compress none \\\n    --container-system singularity \\\n    --container-cache-utilisation amend \\\n    --download-configuration\n\n# Defining the pipeline outputs\noutdir=results/nfc-rnaseq\nworkdir=/fs/scratch/PAS2700/week6/nfc-rnaseq/$USER\n\n# Defining the pipeline inputs\nsamplesheet=\"$outdir\"/nfc_samplesheet.csv\nfasta=../../garrigos_data/ref/GCF_016801865.2.fna\ngtf=../../garrigos_data/ref/GCF_016801865.2.gtf\n\n# Create the dir that will contain the sample sheet\nmkdir -p \"$outdir\"\n\n# Create the sample sheet for the nf-core pipeline\npython3 software/nfc-rnaseq/3_14_0/bin/fastq_dir_to_samplesheet.py \\\n    --strandedness forward \\\n    --read1_extension \"_R1.fastq.gz\" \\\n    --read2_extension \"_R2.fastq.gz\" \\\n    ../../garrigos_data/fastq \\\n    \"$samplesheet\"\n\n# Create a config file for batch job submissions\necho \"\nprocess.executor='slurm'\nprocess.clusterOptions='--account=PAS2700'\n\" &gt; nextflow.config\n\n# Submit the script to run the pipeline as a batch job\nsbatch scripts/nfc-rnaseq.sh \"$samplesheet\" \"$fasta\" \"$gtf\" \"$outdir\"\n\n\n\n\n6.2 Checking the pipeline’s progress\nLet’s check whether your job has started running, and if so, whether Nextflow has already spawned jobs:\n# [Run this directly in the terminal]\nsqueue -u $USER -l\nMon Mar 25 12:13:38 2024\n      JOBID PARTITION     NAME     USER    STATE   TIME TIME_LIMI  NODES NODELIST(REASON)\n  27767854 serial-40 nfc-rnas   jelmer  RUNNING    1:33   6:00:00      1 p0219\nIn the example output above, the only running job is the one we directly submitted, i.e. the main Nextflow process. The NAME column is the script’s name, nfc-rnaseq.sh (truncated to nfc-rnas).\n\n\n\n\n\n\nSee examples of squeue output that includes Nextflow-submitted jobs (Click to expand)\n\n\n\n\n\nThe top job, with partial name nf-NFCOR, is a job that’s been submitted by Nextflow:\nsqueue -u $USER -l\nMon Mar 25 13:14:53 2024\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          27767861 serial-40 nf-NFCOR   jelmer  RUNNING       5:41  16:00:00      1 p0053\n          27767854 serial-40 nfc_rnas   jelmer  RUNNING    1:03:48   6:00:00      1 p0219\nUnfortunately, the columns in the output above are quite narrow, so it’s not possible to see which step of the pipeline is being run by that job. The following (awful-looking!) code can be used to make that column much wider, so you can see the job’s full name which makes clear which step is being run (rRNA removal with SortMeRNA):\nsqueue -u $USER --format=\"%.9i %.9P %.60j %.8T %.10M %.10l %.4C %R %.16V\"\nMon Mar 25 13:15:05 2024\n    JOBID PARTITION                                                          NAME    STATE       TIME TIME_LIMIT CPUS NODELIST(REASON)      SUBMIT_TIME\n 27767861 serial-40   nf-NFCORE_RNASEQ_RNASEQ_SORTMERNA_(SRR27866691_SRR27866691)  RUNNING       5:55   16:00:00   12 p0053 2024-03-23T09:37\n 27767854 serial-40                                                    nfc_rnaseq  RUNNING    1:04:02    6:00:00    1 p0219 2024-03-23T09:36\nYou might also catch the pipeline while there are many more jobs running, e.g.:\nMon Mar 25 13:59:50 2024\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n          27823107 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0091\n          27823112 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0119\n          27823115 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0055\n          27823120 serial-40 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0055\n          27823070 serial-40 nf-NFCOR   jelmer  RUNNING       0:43  16:00:00      1 p0078\n          27823004 serial-40 nfc-rnas   jelmer  RUNNING       2:13   6:00:00      1 p0146\n          27823083 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0078\n          27823084 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0096\n          27823085 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0096\n          27823086 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0115\n          27823087 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0115\n          27823088 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0123\n          27823089 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0123\n          27823090 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0057\n          27823091 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0057\n          27823092 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0058\n          27823093 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0058\n          27823095 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0118\n          27823099 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0118\n          27823103 serial-40 nf-NFCOR   jelmer  RUNNING       0:37  16:00:00      1 p0119\n          27823121 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0625\n          27823122 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0744\n          27823123 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0780\n          27823124 serial-48 nf-NFCOR   jelmer  RUNNING       0:13  16:00:00      1 p0780\n\n\n\nYou can also keep an eye on the pipeline’s progress, and see if there are any errors, by checking the Slurm log file — the top of the file should look like this:\n# You will have a different job ID - replace as appropriate or use Tab completion\n# (We need the -R option to display colors properly)\nless -R slurm-nfc_rnaseq-27767861.out\nStarting script nfc-rnaseq.sh\nMon Mar 25 13:01:30 EDT 2024\nSamplesheet:          results/nfc-rnaseq/nfc_samplesheet.csv\nReference FASTA:      data/ref/GCF_016801865.2.fna\nReference GTF:        data/ref/GCF_016801865.2.gtf\nOutput dir:           results/nfc-rnaseq\nNextflow workdir:     /fs/scratch/PAS2700/week6/nfc-rnaseq/jelmer\n\nN E X T F L O W  ~  version 23.10.1\nWARN: It appears you have never run this project before -- Option `-resume` is ignored\nLaunching `software/nfc-rnaseq/3_14_0/main.nf` [curious_linnaeus] DSL2 - revision: 746820de9b\nWARN: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  Multiple config files detected!\n  Please provide pipeline parameters via the CLI or Nextflow '-params-file' option.\n  Custom config files including those provided by the '-c' Nextflow option can be\n  used to provide any configuration except for parameters.\n\n  Docs: https://nf-co.re/usage/configuration#custom-configuration-files\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq v3.14.0\n------------------------------------------------------\nCore Nextflow options\n  runName                   : curious_linnaeus\n  containerEngine           : singularity\n[...output truncated...]\nThe warnings about -resume and config files shown above can be ignored. Some of this output has nice colors that was not shown above:\n\n\n\n\n\nIn the Slurm log file, pipeline progress is shown in the following way — you can only see which jobs are being submitted, not when they finish6:\n[6e/d95b6f] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:CUSTOM_GETCHROMSIZES (GCF_016801865.2.fna)\n[ee/eba07d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802864)\n[28/131e16] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802863)\n[36/22cf36] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802863)\n[59/402585] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802864)\n[c3/27b16d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802865)\n[b0/f78597] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802865)\n[4c/253978] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_FILTER (GCF_016801865.2.fna)\n[39/264a8e] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802866)\n[ec/6aa13c] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:TRIMGALORE (ERR10802866)\n[dd/73a5e4] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802868)\n[d8/c1bb3e] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC (ERR10802867)\n\n\nYou should also see the following warning among the job submissions (Click to expand)\n\nThis warning can be ignored, the “Biotype QC” is not important and this information is indeed simply missing from our GTF file, there is nothing we can do about that.\nWARN: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  Biotype attribute 'gene_biotype' not found in the last column of the GTF file!\n\n  Biotype QC will be skipped to circumvent the issue below:\n  https://github.com/nf-core/rnaseq/issues/460\n\n  Amend '--featurecounts_group_type' to change this behaviour.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nBut if errors occur, they are reported in this file, and there is also a message when the entire pipeline has finished:\n[28/79e801] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRAPH_BEDCLIP_BEDGRAPHTOBIGWIG_FORWARD:UCSC_BEDGRAPHTOBIGWIG (ERR10802864)\n[e0/ba48c9] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRAPH_BEDCLIP_BEDGRAPHTOBIGWIG_REVERSE:UCSC_BEDGRAPHTOBIGWIG (ERR10802864)\n[62/4f8c0d] Submitted process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQC (1)\n-[nf-core/rnaseq] Pipeline completed successfully -\nDone with script nfc-rnaseq.sh\nMon Mar 25 14:09:52 EDT 2024\n\n\n\n\n\n\n\nWork-dir paths and output\n\n\n\nThe hexadecimals between square brackets (e.g. [62/4f8c0d]) that are printed before each job point to the sub-directory within the pipeline’s work-dir that the this process is running in (every individual process runs in its own dir). This can be handy for troubleshooting, or even just to take a closer look at what exactly the pipeline is running.\n\nFirst, list the contents of the top-level work-dir:\nls /fs/scratch/PAS2700/week6/nfc-rnaseq/$USER\n28  36  39  59  6e  b0  b7  c3  c9  collect-file  d5  d8  da  dd  e0  ec  ee  tmp\nLet’s look at the files for one of the processes (jobs) we saw in the log file, the last FastQC process, which had the workdir reference [d8/c1bb3e] — we’ll use the -a option to ls so hidden files are also listed, some of which are of interest:\n# (The full name of the `c1bb3e` dir is much longer, but these characters uniquely\n#  identify it, so we can use a *)\nls -a /fs/scratch/PAS2700/week6/nfc-rnaseq/jelmer/d8/c1bb3e*\n.               .command.err  .command.run    ERR10802867_1_fastqc.html  ERR10802867_2_fastqc.html  ERR10802867_R1.fastq.gz  versions.yml\n..              .command.log  .command.sh     ERR10802867_1_fastqc.zip   ERR10802867_2_fastqc.zip   ERR10802867_R2.fastq.gz\n.command.begin  .command.out  .command.trace  ERR10802867_1.gz           ERR10802867_2.gz           .exitcode\nTo see exactly how FastQC was run by the pipeline, show the contents of the .command.sh file:\ncat /fs/scratch/PAS2700/week6/nfc-rnaseq/jelmer/d8/c1bb3ea3e697573b4d5860308b2690/.command.sh \n#!/bin/bash -euo pipefail\nprintf \"%s %s\\n\" ERR10802867_R1.fastq.gz ERR10802867_1.gz ERR10802867_R2.fastq.gz ERR10802867_2.gz | while read old_name new_name; do\n    [ -f \"${new_name}\" ] || ln -s $old_name $new_name\ndone\n\nfastqc \\\n    --quiet \\\n    --threads 6 \\\n    ERR10802867_1.gz ERR10802867_2.gz\n\ncat &lt;&lt;-END_VERSIONS &gt; versions.yml\n\"NFCORE_RNASEQ:RNASEQ:FASTQ_FASTQC_UMITOOLS_TRIMGALORE:FASTQC\":\n    fastqc: $( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\nEND_VERSIONS\nLogging and errors would end up in the .command.log, .command.out, and/or .command.err files (when there’s an error, at least part of it will be printed in your Slurm log, but not always the full error).\nThe input files (as links/shortcuts, so the files don’t have to be copied in fill) and output files are also present in this dir, such as ERR10802867_1_fastqc.html in this case."
  },
  {
    "objectID": "nf-run/02_nfcore.html#the-pipelines-outputs",
    "href": "nf-run/02_nfcore.html#the-pipelines-outputs",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "7 The pipeline’s outputs",
    "text": "7 The pipeline’s outputs\nYou pipeline run may finish in as little as 15-30 minutes with our small test data set, but this can vary substantially, mostly due to variation in Slurm queue times (the pipeline makes quite large Slurm resource requests!).\nOnce it has finished, you can take a look at the files and dirs in the specified output dir:\nls -lh results/nfc-rnaseq\ntotal 83K\ndrwxr-xr-x   2 jelmer PAS0471  16K Mar 25 13:02 fastqc\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 12:58 logs\ndrwxr-xr-x   3 jelmer PAS0471 4.0K Mar 25 13:14 multiqc\n-rw-r--r--   1 jelmer PAS0471 2.0K Mar 25 19:55 nfc_samplesheet.csv\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 13:14 pipeline_info\ndrwxr-xr-x 248 jelmer PAS0471  16K Mar 25 13:10 raw\ndrwxr-xr-x   2 jelmer PAS0471 4.0K Mar 25 13:06 sortmerna\ndrwxr-xr-x  33 jelmer PAS0471  16K Mar 25 13:12 star_salmon\ndrwxr-xr-x   3 jelmer PAS0471 4.0K Mar 25 13:02 trimgalore\nThe two most important outputs are:\n\nThe MultiQC report (&lt;outdir&gt;/multiqc/star_salmon/multiqc_report.html): this has lots of QC summaries of the data, both the raw data and the alignments, and even a gene expression PCA plot. Importantly, this file also lists the versions of each piece of software that was used by the pipeline.\nThe gene count table (&lt;outdir&gt;/star_salmon/salmon.merged.gene_counts_length_scaled.tsv): This is what you would use for downstream analysis such as differential expression and functional enrichment analysis.\n\n\n\nOpening the MultiQC report\nIf your run has finished, you can open your MultiQC report — because it’s an HTML file, you’ll have to download it and open it on your own computer. To download the MultiQC HTML file at results/nfc-rnaseq/multiqc/star_salmon/multiqc_report.html, find this file in the VS Code explorer (file browser) on the left, right-click on it, and select Download.... You can download it to any location on your computer. Then find the file on your computer and click on it to open it — it should be opened in your browser.\nAlternatively, you can find a copy of the MultiQC report on this website, and open it in a separate browser tab. There’s a lot of information in the report — if you want to learn more, please go through the self-study section below."
  },
  {
    "objectID": "nf-run/02_nfcore.html#self-study-i-a-closer-look-at-the-output",
    "href": "nf-run/02_nfcore.html#self-study-i-a-closer-look-at-the-output",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "8 Self-study I: A closer look at the output",
    "text": "8 Self-study I: A closer look at the output\nIf you’re interested in RNA-seq data analysis, you may want to take a closer look at the outputs of the pipeline, especially the MultiQC report.\n\n\n8.1 The MultiQC report\nHere are some items in that report to pay particular attention to, with example figures from this data set:\nThe General Statistics table (the first section) is very useful, with the following notes:\n\nMost of the table’s content is also in later graphs, but the table allows for comparisons across metrics.\nThe %rRNA (% of reads identified as rRNA and removed by SortMeRNA) can only be found in this table.\nIt’s best to hide the columns with statistics from Samtools, which can be confusing if not downright misleading: click on “Configure Columns” and uncheck all the boxes for stats with Samtools in their name.\nSome stats are for R1 and R2 files only, and some are for each sample as a whole. Unfortunately, this means you get 3 rows per sample in the table.\n\n\n\n\n\n\n\nThe Qualimap &gt; Genomic origin of reads plot shows, for each sample, the proportion of reads mapping to exonic vs. intronic vs. intergenic regions. This is an important QC plot: the vast majority of your reads should be exonic7.\n\n\n\nThis is a good result, with 80-90% of mapped reads in exonic regions.\n\n\n\nThe STAR &gt; Alignment Scores plot shows, for each sample, the percentage of reads that was mapped. Note that “Mapped to multiple loci” reads are also included in the final counts, and that “Unmapped: too short” merely means unmapped, really, and not that the reads were too short.\n\n\n\nThis is a pretty good results, with 80-90% of reads mapped.\n\n\n\nFastQC checks your FASTQ files, i.e. your data prior to alignment. There are FastQC plots both before and after trimming with TrimGalore/Cutadapt. The most important FastQC modules are:\n\nSequence Quality Histograms — You’d like the mean qualities to stay in the “green area”.\nPer Sequence GC Content — Secondary peaks may indicate contamination.\nAdapter Content — Any adapter content should be gone in the post-trimming plot.\n\n\n\n Exercise: Interpreting FastQC results in the MultiQC report\nTake a look at the three FastQC modules discussed above, both before and after trimming.\n\nHas the base quality improved after trimming, and does this look good?\n\n\n\nClick to see the answer\n\n\nPre-trimming graph: The qualities are good overall, but there is more variation that what is usual, and note the poorer qualities in the first 7 or so bases. There is no substantial decline towards the end of the read as one often sees with Illumina data, but this is expected given that the reads are only 75 bp.\n\n\n\n\nPre-trimming (Mean base quality scores: one line is one sample.)\n\n\n\nPost-trimming graph: The qualities have clearly improved. The first 7 or so bases remain of clearly poorer quality, on average.\n\n\n\n\nPost-trimming\n\n\n\n\nDo you have any idea what’s going with the pre-trimming GC content distribution? What about after trimming — does this look good or is there reason to worry?\n\n\n\nClick to see the answer\n\n\nThe pre-trimming GC content is very odd but this is mostly due to a high number of reads with zero and near-zero percent GC content. These are likely reads with only Ns. There are also some reads with near-hundred percent GC content. These are likely artifactual G-only reads that NextSeq/NovaSeq machines can produce.\n\n\n\n\nPre-trimming. One line is one file.\n\n\n\nAfter trimming, things look a lot better but there may be contamination here, given the weird “shoulder” at 30-40% GC.\n\n\n\n\nPost-trimming\n\n\n\n\nDo you know what the “adapters” that FastQC found pre-trimming are? Were these sequences removed by the trimming?\n\n\n\nClick to see the answer\n\n\nPre-trimming, there seem to be some samples with very high adapter content throughout the read. This doesn’t make sense for true adapters, because these are usually only found towards the end of the read, when the read length is longer than the DNA fragment length. If you hover over the lines, you’ll see it says “polyg”. These are artifactual G-only reads that NextSeq/NovaSeq can produce, especially in the reverse reads — and you can see that all of the lines are for reverse-read files indeed.\n\n\n\n\nPre-trimming\n\n\n\nPost-trimming, no adapter content was found.\n\n\n\n\nPost-trimming\n\n\n\n\n\nThe Qualimap &gt; Gene Coverage Profile plot shows average read-depth across the length of genes/transcripts (averaged across all genes), which helps to assess the amount of RNA degradation. For poly-A selected libraries, RNA molecules “begin” at the 3’ end (right-hand side of the graph), so the more degradation there is, the more you expect a higher read-depth towards the 3’ end relative to that at the 5’ end. (Though note that sharp decreases at the very end on each side are expected.)\n\n\n\nThere depth at ~20% (near the 5’ end) is clearly lower than at ~80% (near the 3’ end),indicating some RNA degradation.\n\n\nThe RSeqQC &gt; Infer experiment (library strandedness) plot. If your library is:\n\nUnstranded, there should be similar percentages of Sense and Antisense reads.\nForward-stranded, the vast majority of reads should be Sense.\nReverse-stranded, the vast majority of reads should be Antisense.\n\n\n\n\nThis libary is clearly forward-stranded, as we indicated in our sample sheet.\n\n\nThe STAR_SALMON DESeq2 PCA plot is from a Principal Component Analysis (PCA) run on the final gene count table, showing overall patterns of gene expression similarity among samples.\n\n\n\nThe samples clearly form two distinct groups along PC1.\n\n\nFinally, the section nf-corer/rnaseq Software Versions way at the bottom lists the version of all programs used by the pipeline!\n\n\n\n8.2 The gene count table\nThe gene count table has one row for each gene and one column for each sample, with the first two columns being the gene_id and gene_name8. Each cell’s value contains the read count estimate for a specific gene in a specific sample:\n# [Paste this into the run/run.sh script and run it in the terminal]\n\n# Take a look at the count table:\n# ('column -t' lines up columns, and less's '-S' option turns off line wrapping)\ncounts=results/nfc-rnaseq/star_salmon/salmon.merged.gene_counts_length_scaled.tsv\ncolumn -t \"$counts\" | less -S\ngene_id             gene_name           ERR10802863        ERR10802864        ERR10802865        ERR10802866        ERR10802867        ERR10802868       \nATP6                ATP6                163.611027228009   178.19903533081    82.1025390726658   307.649552934133   225.78249209207    171.251589309856  \nATP8                ATP8                0                  1.01047333891691   0                  0                  0                  0                 \nCOX1                COX1                1429.24769032452   2202.82009602881   764.584344577622   2273.6965332904    2784.47391614249   2000.51277019854  \nCOX2                COX2                116.537361366535   175.137972566817   54.0166352459629   256.592955351283   193.291937038438   164.125833130119  \nCOX3                COX3                872.88670991359    1178.29247734231   683.167933227141   1200.01735304529   1300.3853323715    1229.11746824104  \nCYTB                CYTB                646.028108528182   968.256051104547   529.393909319439   1025.23768317788   1201.46662840336   842.533209911258  \nLOC120412322        LOC120412322        0                  0                  0                  0                  0.995135178345792  0.996805450081561 \nLOC120412324        LOC120412324        37.8326244586681   20.9489661184365   27.6702324729125   48.6417838830061   22.8313729348804   36.87899862428    \nLOC120412325        LOC120412325        3.21074365394071   2.10702898851342   4.40315394778926   5.47978997387391   4.33241716734803   4.23386924919438  \nLOC120412326        LOC120412326        0                  0                  0                  0                  0                  0                 \nLOC120412327        LOC120412327        37.8206758601034   35.9063291323018   38.517771617566    27.7802608986967   37.6979028802121   32.885944667709   \nLOC120412328        LOC120412328        35.0080600370267   20.0019192467143   23.9260736995594   30.0191332346116   21.0383665366408   28.9844776623531  \nLOC120412329        LOC120412329        121.777922287929   112.794544755113   131.434181046282   127.753086659103   114.864750589664   131.589608063253  \nLOC120412330        LOC120412330        42.8505448763697   28.9442284428204   36.6285174684674   46.7310765909945   42.7633834468768   26.9265243413636  \nLOC120412331        LOC120412331        11.013179311581    9.00559907892481   12.9836833055803   13.029954361225    7.02624958751718   16.000552787954   \nLOC120412332        LOC120412332        12.1055360835441   26.1231316926989   21.2767913384733   18.2783703626438   26.4932540325187   22.098808637857   \nLOC120412333        LOC120412333        19.1159998132169   17.0558058070299   12.0965688236319   14.1510477997588   15.2033452089903   9.02624985028677  \nLOC120412334        LOC120412334        9.01332125155807   3.00232591636489   5.99566364212933   11.0306919231504   8.03448732510427   11.0022053123759  \n# [...output truncated...]\n\n\n\n\n\n\nCount table versions\n\n\n\nThe workflow outputs several versions of the count table9, but the one with gene_counts_length_scaled is the one we want:\n\ngene_counts as opposed to transcript_counts for counts that are summed across transcripts for each gene.\nlength for estimates that have been adjusted to account for between-sample differences in mean transcript length (longer transcripts would be expected to produce more reads in sequencing).\nscaled for estimates that have been scaled back using the “library sizes”, per-sample total read counts."
  },
  {
    "objectID": "nf-run/02_nfcore.html#self-study-ii-reference-genome-files",
    "href": "nf-run/02_nfcore.html#self-study-ii-reference-genome-files",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "9 Self-study II: Reference genome files",
    "text": "9 Self-study II: Reference genome files\n\n9.1 The genome assembly FASTA\n\nThe FASTA format\nFASTA files contain one or more DNA or amino acid sequences, with no limits on the number of sequences or the sequence lengths. FASTA is the standard format for, e.g.:\n\nGenome assembly sequences\nTranscriptomes and proteomes (all of an organism’s transcripts & amino acid sequences, respectively)\nSequence downloads from NCBI such as a single gene/protein or other GenBank entry\n\nThe following example FASTA file contains two entries:\n&gt;unique_sequence_ID Optional description\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n&gt;unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\nEach entry consists of a header line and the sequence itself. Header lines start with a &gt; (greater-than sign) and are otherwise “free form”, though the idea is that they provide an identifier for the sequence that follows.10\n\n\n\n\n\n\nFASTA file name extensions are variable: .fa, .fasta, .fna, .faa (Click to expand)\n\n\n\n\n\n\n“Generic” extensions are .fasta and .fa (e.g: culex_assembly.fasta)\nAlso used are extensions that explicitly indicate whether sequences are nucleotides (.fna) or amino acids (.faa)\n\n\n\n\n\n\n\nYour Culex genome assembly FASTA\nYour reference genome files are in data/ref:\nls -lh data/ref\n-rw------- 1 jelmer PAS0471 547M Jan 22 12:34 GCF_016801865.2.fna\n-rw------- 1 jelmer PAS0471 123M Jan 22 12:34 GCF_016801865.2.gtf\nWhile we can easily open small to medium-size files in the editor pane, “visual editors” like that do not work well for larger files like these.\nA handy command to view text files of any size is less, which opens them up in a “pager” within your shell – you’ll see what that means if you try it with one of the assembly FASTA file:\nless data/ref/GCF_016801865.2.fna\n&gt;NC_068937.1 Culex pipiens pallens isolate TS chromosome 1, TS_CPP_V2, whole genome shotgun sequence\naagcccttttatggtcaaaaatatcgtttaacttgaatatttttccttaaaaaataaataaatttaagcaaacagctgag\ntagatgtcatctactcaaatctacccataagcacacccctgttcaatttttttttcagccataagggcgcctccagtcaa\nattttcatattgagaatttcaatacaattttttaagtcgtaggggcgcctccagtcaaattttcatattgagaatttcaa\ntacatttttttatgtcgtaggggcgcctccagtcaaattttcatattgagaatttcaatacattttttttaagtcgtagg\nggcgcctccagtcaaattttcatattgagaatttcaatacatttttttaagtcttaggggcgcctccagtcaaattttca\ntattgagaatttcaatacatttttttaagtcgtaggggcgcctccagtcaaattttcatattgagaattttaatacaatt\nttttaaatcctaggggcgccttcagacaaacttaatttaaaaaatatcgctcctcgacttggcgactttgcgactgactg\ncgacagcactaccttggaacactgaaatgtttggttgactttccagaaagagtgcatatgacttgaaaaaaaaagagcgc\nttcaaaattgagtcaagaaattggtgaaacttggtgcaagcccttttatggttaaaaatatcgtttaacttgaatatttt\ntccttaaaaaataaataaatttaagcaaacagctgagtagatgtcatctactcaaatctacccataagcacacccctgga\nCCTAATTCATGGAGGTGAATAGAGCATACGTAAATACAAAACTCATGACATTAGCCTGTAAGGATTGTGTaattaatgca\naaaatattgaTAGAATGAAAGATGCAAGTCccaaaaattttaagtaaatgaATAGTAATCATAAAGATAActgatgatga\n\n\n\n\n\n\nSide note: Lowercase vs. uppercase nucleotide letters? (Click to expand)\n\n\n\n\n\nAs you have probably noticed, nucleotide bases are typically typed in uppercase (A, C, G, T). What does the mixture of lowercase and uppercase bases in the Cx. pipiens assembly FASTA mean, then?\nLowercase bases are what is called “soft-masked”: they are repetitive sequences, and bioinformatics programs will treat them differently than non-repetitive sequences, which are in uppercase.\n\n\n\n\n\n\n\n9.2 The genome annotation GFF/GTF\n\nThe GFF/GTF format\nThe GTF and GFF formats are very similar tab-delimited tabular files that contain genome annotations, with:\n\nOne row for each annotated “genomic feature” (gene, exon, etc.)\nOne column for each piece of information about a feature, like its genomic coordinates\n\nSee the sample below, with an added header line (not normally present) with column names:\nseqname     source  feature start   end     score  strand  frame    attributes\nNC_000001   RefSeq  gene    11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"\"; db_xref \"GeneID:100287102\"; db_xref \"HGNC:HGNC:37102\"; description \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; gbkey \"Gene\"; gene \"DDX11L1\"; gene_biotype \"transcribed_pseudogene\"; pseudo \"true\"; \nNC_000001   RefSeq  exon    11874   12227   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; \nSome details on the more important/interesting columns:\n\nseqname — Name of the chromosome, scaffold, or contig\nfeature — Name of the feature type, e.g. “gene”, “exon”, “intron”, “CDS”\nstart & end — Start & end position of the feature\nstrand — Whether the feature is on the + (forward) or - (reverse) strand\nattribute — A semicolon-separated list of tag-value pairs with additional information\n\n\n\n\nYour Culex GTF file\nFor our Cx. pipiens reference genome, we only have a GTF file.11 Take a look at it, again with less (but now with the -S option):\nless -S data/ref/GCF_016801865.2.gtf\n#gtf-version 2.2\n#!genome-build TS_CPP_V2\n#!genome-build-accession NCBI_Assembly:GCF_016801865.2\n#!annotation-source NCBI RefSeq GCF_016801865.2-RS_2022_12\nNC_068937.1     Gnomon  gene    2046    110808  .       +       .       gene_id \"LOC120427725\"; transcript_id \"\"; db_xref \"GeneID:120427725\"; description \"homeotic protein deformed\"; gbkey \"Gene\"; gene \"LOC120427725\"; gene_biotype \"protein_coding\"; \nNC_068937.1     Gnomon  transcript      2046    110808  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; \nNC_068937.1     Gnomon  exon    2046    2531    .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1     Gnomon  exon    52113   52136   .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1     Gnomon  exon    70113   70962   .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1     Gnomon  exon    105987  106087  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1     Gnomon  exon    106551  106734  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \n\n\n Exercise: Transcripts and genes\nThe GTF file is sorted: all entries from the first line of the table, until you again see “gene” in the third column, belong to the first gene. Can you make sense of all these entries for this gene, given what you know of gene structures? How many transcripts does this gene have?\n\n\n(Click to see some pointers)\n\n\nThe first gene (“LOC120427725”) has 3 transcripts.\nEach transcript has 6-7 exons, 5 CDSs, and a start and stop codon.\n\nBelow, I’ve printed all lines belonging to the first gene:\nNC_068937.1 Gnomon  gene    2046    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"\"; db_xref \"GeneID:120427725\"; description \"homeotic protein deformed\"; gbkey \"Gene\"; gene \"LOC120427725\"; gene_biotype \"protein_coding\"; \nNC_068937.1 Gnomon  transcript  2046    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    2046    2531    .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    52113   52136   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  exon    109726  110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"7\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  transcript  5979    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    5979    6083    .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    52113   52136   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  exon    109726  110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"7\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  transcript  60854   110807  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    60854   61525   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109726  110807  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"2\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"2\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"6\";"
  },
  {
    "objectID": "nf-run/02_nfcore.html#footnotes",
    "href": "nf-run/02_nfcore.html#footnotes",
    "title": "Running an nf-core Nextflow pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Conda environments can also be used, but containers are recommended, and are what we will use.↩︎\n Typically, not all outputs are copied, especially for intermediate steps like read trimming — and pipelines have settings to determine what is copied↩︎\n Somewhat oddly, there is no command-line option available for this: we have to use this environment variable, also when running the pipeline.↩︎\n The default logging does not work well the output goes to a text file, as it will in our case because we will submit the script with the Nextflow command as a Slurm batch job.↩︎\n And for a run of a full data set, you may want to ask even more, e.g. 12-24 hours.↩︎\n The default Nextflow logging (without -ansi-log false) does show when jobs finish, but this would result in very messy output in a Slurm log file.↩︎\n A lot of intronic content may indicate that you have a lot of pre-mRNA in your data; this is more common when your library prep used rRNA depletion instead of poly-A selection. A lot of intergenic content may indicate DNA contamination. Poor genome annotation quality may also contribute to a low percentage of exonic reads. The RSeQC &gt; Read Distribution plot will show this with even more categories, e.g. separately showing UTRs.↩︎\n Which happen to be the same here, but these are usually different.↩︎\n And each version in two formats: .rds (a binary R object file type) and .tsv.↩︎\nNote that because individual sequence entries are commonly spread across multiple lines, FASTA entries do not necessarily cover 2 lines (cf. FASTQ).↩︎\nA GFF file would contain the same information but in a slightly different format. For programs used in RNA-seq analysis, GTF files tend to be the preferred format.↩︎"
  },
  {
    "objectID": "git/01_github-account.html",
    "href": "git/01_github-account.html",
    "title": "Creating a GitHub account",
    "section": "",
    "text": "What?\nCreate a personal GitHub account.\n\n\nWhy?\nGitHub is a website that hosts Git repositories, i.e. version-controlled projects.\n\n\nHow?\nIf you already have a GitHub account, log in and start at step 6.\n\nGo to https://github.com.\nClick “Sign Up” in the top right.\nFollow the prompts to create your account — some notes:\n\nWhen choosing your username, I would recommend to keep it professional and have it include or resemble your real name. (This is because you will hopefully continue to use GitHub to share your code, for instance when publishing a paper.)\nYou can choose whether you want to use your OSU email address or a non-institutional email address. (And note that you can always associate a second email address with your account.)\n\nCheck your email and click the link to verify your email address.\nBack on GitHub website, now logged in: in the far top-right of the page, click your randomly assigned avatar, and in the dropdown menu, click “Settings”.\nIn the “Emails” tab (left-hand menu), deselect the box “Keep my email addresses private”.\nIn the “Public profile” tab, enter your name.\nStill in the “Profile tab”, upload a Profile picture. This can be a picture of yourself but if you prefer, you can use something else.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "git/03_git2.html#remote-repositories",
    "href": "git/03_git2.html#remote-repositories",
    "title": "Git: Remotes on GitHub",
    "section": "1 Remote repositories",
    "text": "1 Remote repositories\nSo far, we have been locally version-controlling our originspecies repository. Now, we also want to put this repo online, so we can:\n\nShare our work (e.g. alongside a publication) and/or\nHave an online backup and/or\nCollaborate with others\n\nWe will use the GitHub website as the place to host our online repositories. Online counterparts of local repositories are usually referred to as “remote repositories” or simply “remotes”.\nUsing remote repositories will mean adding a couple of Git commands to our toolbox:\n\ngit remote to add and manage connections to remotes.\ngit push to upload (“push”) changes from local to remote.\ngit pull to download (“pull”) changes from remote to local.\n\nBut we will need to start with some one-time setup to enable GitHub authentication.\n\n\n1.1 One-time setup: GitHub authentication\nIn order to link your local Git repositories to their online counterparts on GitHub, you need to set up GitHub authentication. There are two options for this (see the box below) but we will use SSH access with an SSH key.\n\nUse the ssh-keygen command to generate a public-private SSH key pair — in the command below, replace your_email@example.com with the email address you used to sign up for a GitHub account:\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nYou’ll be asked three questions, and for all three, you can accept the default simply by pressing Enter:\n# Enter file in which to save the key (&lt;default path&gt;):\n# Enter passphrase (empty for no passphrase):\n# Enter same passphrase again: \nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/users/PAS0471/jelmer/.ssh/id_ed25519): \nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /users/PAS0471/jelmer/.ssh/id_ed25519.\nYour public key has been saved in /users/PAS0471/jelmer/.ssh/id_ed25519.pub.\nThe key fingerprint is:\nSHA256:KO73cE18HFC/aHKIXR7nf9Fk4++CiIw6GTk7ffG+p2c your_email@example.com\nThe key's randomart image is:\n+--[ED25519 256]--+\n|          ...    |\n|           . .   |\n|            + o.o|\n|       . + = *.+o|\n|    ... S * B oo.|\n|   .+.  .o =   .o|\n|    .*.o.+.. .  +|\n|   .= +o+ o E ...|\n|    o= o..+*   ..|\n+----[SHA256]-----+\nNow, you have a file ~/.ssh/id_ed25519.pub, which is your public key. To enable authentication, you will put this public key (which interacts with your private key) on GitHub. Print the public key to your screen using cat:\ncat ~/.ssh/id_ed25519.pub\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBz4qiqjbNrKPodoGyoF/v7n0GKyvc/vKiW0apaRjba2 your_email@example.com\nCopy the line that was printed to screen to your clipboard.\nIn your browser, go to https://github.com and log in.\nGo to your personal Settings. (Click on your avatar in the far top-right of the page, then select “Settings”.)\nClick on SSH and GPG keys in the sidebar.\nClick the green New SSH key button.\nIn the form (see screenshot below):\n\nGive the key an arbitrary, informative “Title” (name), e.g. “OSC” to indicate that you will use this key at OSC.\nPaste the public key, which you copied to your clipboard earlier, into the “Key” box.\nClick the green Add SSH key button. Done!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthentication and GitHub URLs\n\n\n\nThe current two options to authenticate to GitHub when connecting with remotes are:\n\nSSH access with an SSH key (which we have just set up)\nHTTPS access with a Public Access Token (PAT). See this GitHub page to set this up instead of or in addition to SSH access.\n\nFor everything on GitHub, there are separate SSH and HTTPS URLs. When using SSH (as we are), we need to use URLs with the following format:\ngit@github.com:&lt;USERNAME&gt;/&lt;REPOSITORY&gt;.git\n(And when using HTTPS, you would use URLs like https://github.com/&lt;USERNAME&gt;/&lt;REPOSITORY&gt;.git)\n\n\n\n\n\n1.2 Create a remote repository\nWhile we can interact with online repos using Git commands, we can’t create a new online repo with the Git CLI. Therefore, we will to go to the GitHub website to create a new online repo:\n\nOn the GitHub website, click the + next to your avatar (top-right) and select “New repository”:\n\n\n\n\n\n\n\nIn the box “Repository name”, we’ll use the same name that we gave to our local directory: originspecies2.\n\n\n\n\n\n\n\nLeave other options as they are, as shown below, and click “Create repository”:\n\n\n\n\n\n\n\n\n\n1.3 Link the local and remote repositories\nAfter you clicked “Create repository”, a page similar to this screenshot should appear, which gives us some information about linking the remote and local repositories:\n\n\n\n\n\nWe go back to VS Code, where we’ll enter the commands that GitHub provided to us under the “…or push an existing repository from the command line” heading shown at the bottom of the screenshot above:3\n\nFirst, we tell Git to add a “remote” connection with git remote, providing three arguments to this command:\n\nadd — because we’re adding a remote connection.\norigin — the arbitrary nickname we’re giving the connection (usually called “origin” by convention).\n\nThe URL to the GitHub repo (in SSH format: click on the HTTPS/SSH button to toggle the URL type).\n\n# git remote add &lt;remote-nickname&gt; &lt;URL&gt;\ngit remote add origin git@github.com:&lt;user&gt;/originspecies.git\nSecond, we upload (“push”) our local repo to remote using git push. When we push a repository for the first time, we need to use the -u option to set up an “upstream” counterpart:\n# git push -u &lt;connection&gt; &lt;branch&gt;\ngit push -u origin main\nYou should then get a message like this: type yes and press Enter.\nThe authenticity of host 'github.com (140.82.114.4)' can't be established.\nECDSA key fingerprint is SHA256:p2QAMXNIC1TJYWeIOttrVc98/R1BUFWu3/LiyKgUfQM.\nECDSA key fingerprint is MD5:7b:99:81:1e:4c:91:a5:0d:5a:2e:2e:80:13:3f:24:ca.\nAre you sure you want to continue connecting (yes/no)?\nThen, the push/upload should go through, with a message along these lines printed to screen:\nCounting objects: 18, done.\nDelta compression using up to 40 threads.\nCompressing objects: 100% (12/12), done.\nWriting objects: 100% (18/18), 1.67 KiB | 0 bytes/s, done.\nTotal 18 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), done.\nremote: To git@github.com:jelmerp/originspecies2.git\n* [new branch]      main -&gt; main\n\n\n\n\n\n\n\n\nPushing will be easier from now on\n\n\n\nNote that when we don’t give git push any arguments, it will push:\n\nTo the default remote connection.\nTo & from the currently active repository “branch” (default: main)4.\n\nBecause we only have one remote connection and one branch, we can from now on simply use the following to push:\ngit push\n\nAlso, note that you can check your remote connection settings for a repo with git remote -v:\ngit remote -v\norigin  git@github.com:jelmerp/originspecies.git (fetch)\norigin  git@github.com:jelmerp/originspecies.git (push)\n\n\n\n\n\n\n1.4 Explore the repository on GitHub\nBack at GitHub on your repo page, click where it says &lt;&gt; Code in the lower of the top bars:\n\n\n\n\n\nThis is basically our repo’s “home page”, and we can see the files that we just uploaded from our local repo:\n\n\n\n\n\nNext, click where it says x commits (x should be 10 in this case):\n\n\n\n\n\nYou’ll get an overview of the commits that you made, somewhat similar to what you get when you run git log:\n\n\n\n\n\nYou can click on a commit to see the changes that were made by it:\n\n\n\nThe line that is highlighted in green was added by this commit.\n\n\nOn the right-hand side, a &lt; &gt; button will allow you to see the state of the repo at the time of that commit:\n\n\n\n\n\nOn the commit overview page, scroll down all the way to the first commit and click the &lt; &gt;: you’ll see the repo’s “home page” again, but now with only the origin.txt file, since that was the only file in your repo at the time:\n\n\n\n\n\n\n\nGitHub “Issues”\nEach GitHub repository has an “Issues” tab — issues are mainly used to track bugs and other (potential) problems with a repository. In an issue, you can reference specific commits and people, and use Markdown formatting.\n\n\n\n\n\n\nWhen you hand in your final project submissions, you will create an issue simply to notify me about your repository.\n\n\n\n\n\n\nTo go to the Issues tab for your repo, click on Issues towards the top of the page:\n\n\n\n\n\nAnd you should see the following page — in which you can open a new issue with the “New” button:"
  },
  {
    "objectID": "git/03_git2.html#remote-repo-workflows-single-user",
    "href": "git/03_git2.html#remote-repo-workflows-single-user",
    "title": "Git: Remotes on GitHub",
    "section": "2 Remote repo workflows: single-user",
    "text": "2 Remote repo workflows: single-user\nIn a single-user workflow, all changes are typically made in the local repository, and the remote repo is simply periodically updated (pushed to). So, the interaction between local and remote is unidirectional:\n\n\n\n\n\nAWhen pushing to remote for the first time, you first set up the connection with git remote and use the u option to git push.\n\n\n\n\n\n\n\n\nBNow, the local (“My repo”) and remote (“GitHub”) are in sync.\n\n\n\n\n\n\n\n\n\n\nCNext, you’ve made a new commit locally: the local and remote are out of sync.\n\n\n\n\n\n\n\n\nDYou simply use git push to update the remote, after which the local and remote will be back in sync (latter not shown).\n\n\n\n\n\nIn a single-user workflow with a remote, you commit just like you would without a remote in your day-to-day work, and in addition, push to remote occasionally — let’s run through an example.\n\nStart by creating a README.md file for your repo:\necho \"# Origin\" &gt; README.md\necho \"Repo for book draft on my new **theory**\" &gt;&gt; README.md\nAdd and commit the file:\ngit add README.md\ngit commit -m \"Added a README file\"\n[main 63ce484] Added a README file\n1 file changed, 2 insertions(+)\ncreate mode 100644 README.md\nIf you now run git status, you’ll see that Git knows that your local repo is now one commit “ahead” of its remote counterpart:\nOn branch main\nYour branch is ahead of 'origin/main' by 1 commit.\n(use \"git push\" to publish your local commits)\n\nnothing to commit, working tree clean\nBut Git will not automatically sync. So now, push to the remote repository:\ngit push\nCounting objects: 4, done.\nDelta compression using up to 40 threads.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 404 bytes | 0 bytes/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nremote: To git@github.com:jelmerp/originspecies.git\n4968e62..b1e6dad  main -&gt; main\n\n\nLet’s go back to GitHub: note that the README.md Markdown has been automatically rendered!\n\n\n\n\n\n\n\n\n\n\n\n\nWant to collaborate on repositories?\n\n\n\nThe version control bonus page goes into collaboration with Git and Github, i.e. multi-user workflows."
  },
  {
    "objectID": "git/03_git2.html#footnotes",
    "href": "git/03_git2.html#footnotes",
    "title": "Git: Remotes on GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that Git is available at OSC even without loading this module, but that’s a much older version.↩︎\nThough note that these names don’t have to match up.↩︎\n Though we can skip the second one, git branch -M main, since our branch is already called “main”.↩︎\n To learn more about branches, see the Git bonus material.↩︎"
  },
  {
    "objectID": "osc/01_osc1.html#goals-for-this-session",
    "href": "osc/01_osc1.html#goals-for-this-session",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nThis session will provide an introduction to high-performance computing in general and to the Ohio Supercomputer Center (OSC)."
  },
  {
    "objectID": "osc/01_osc1.html#high-performance-computing",
    "href": "osc/01_osc1.html#high-performance-computing",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "2 High-performance computing",
    "text": "2 High-performance computing\nA supercomputer (also known as a “compute cluster” or simply a “cluster”) consists of many computers that are connected by a high-speed network, and that can be accessed remotely by its users. In more general terms, supercomputers provide high-performance computing (HPC) resources.\nThis is what Owens, one of the OSC supercomputers, physically looks like:\n\n\n\n\n\nHere are some possible reasons to use a supercomputer instead of your own laptop or desktop:\n\nYour analyses take a long time to run, need large numbers of CPUs, or a large amount of memory.\nYou need to run some analyses many times.\nYou need to store a lot of data.\nYour analyses require specialized hardware, such as GPUs (Graphical Processing Units).\nYour analyses require software available only for the Linux operating system, but you use Windows.\n\nWhen you’re working with omics data, many of these reasons typically apply. This can make it hard or sometimes simply impossible to do all your work on your personal workstation, and supercomputers provide a solution.\n\n\nThe Ohio Supercomputer Center (OSC)\nThe Ohio Supercomputer Center (OSC) is a facility provided by the state of Ohio in the US. It has two supercomputers, lots of storage space, and an excellent infrastructure for accessing these resources.\n\n\n\n\n\n\n\nOSC websites and “Projects”\n\n\n\nOSC has three main websites — we will mostly or only use the first:\n\nhttps://ondemand.osc.edu: A web portal to use OSC resources through your browser (login needed).\nhttps://my.osc.edu: Account and project management (login needed).\nhttps://osc.edu: General website with information about the supercomputers, installed software, and usage.\n\n\nAccess to OSC’s computing power and storage space goes through OSC “Projects”:\n\nA project can be tied to a research project or lab, or be educational like this material’s project, PAS2700.\nEach project has a budget in terms of “compute hours” and storage space1.\nAs a user, it’s possible to be a member of multiple different projects."
  },
  {
    "objectID": "osc/01_osc1.html#the-structure-of-a-supercomputer-center",
    "href": "osc/01_osc1.html#the-structure-of-a-supercomputer-center",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "3 The structure of a supercomputer center",
    "text": "3 The structure of a supercomputer center\n\n3.1 Terminology\nLet’s start with some (super)computing terminology, going from smaller things to bigger things:\n\nNode\nA single computer that is a part of a supercomputer.\nSupercomputer / Cluster\nA collection of computers connected by a high-speed network. OSC has two: “Pitzer” and “Owens”.\nSupercomputer Center\nA facility like OSC that has one or more supercomputers.\n\n\n\n3.2 Supercomputer components\nWe can think of a supercomputer as having three main parts:\n\nFile Systems: Where files are stored (these are shared between the two OSC supercomputers!)\nLogin Nodes: The handful of computers everyone shares after logging in\nCompute Nodes: The many computers you can reserve to run your analyses\n\n\n\n\n\n\n\n\nFile systems\nOSC has several distinct file systems:\n\n\n\n\n\n\n\n\n\n\n\nFile system\nLocated within\nQuota\nBacked up?\nAuto-purged?\nOne for each…\n\n\n\n\nHome\n/users/\n500 GB / 1 M files\nYes\nNo\nUser\n\n\nProject\n/fs/ess/\nFlexible\nYes\nNo\nOSC Project\n\n\nScratch\n/fs/scratch/\n100 TB\nNo\nAfter 90 days\nOSC Project\n\n\n\nWith this material, we will be working in the project directory of OSC Project PAS2700: /fs/ess/PAS2700.\n\n\n\n\n\n\nDirectory is just another word for folder, often written as “dir” for short\n\n\n\n\n\n\n\n\n\nLogin Nodes\nLogin nodes are set aside as an initial landing spot for everyone who logs in to a supercomputer. There are only a handful of them on each supercomputer, they are shared among everyone, and cannot be “reserved”.\nAs such, login nodes are meant only to do things like organizing your files and creating scripts for compute jobs, and are not meant for any serious computing, which should be done on the compute nodes.\n\n\n\nCompute Nodes\nData processing and analysis is done on compute nodes. You can only use compute nodes after putting in a request for resources (a “job”). The Slurm job scheduler, which you can learn about in this tutorial, will then assign resources to your request.\n\n\n\n\n\n\nCompute node types\n\n\n\nCompute nodes come in different shapes and sizes. Standard, default nodes work fine for the vast majority of analyses, even with large-scale omics data. But you will sometimes need non-standard nodes, such as when you need a lot of RAM memory or need GPUs2.\n\n\n\n\n\n\n\n\nAt-home reading: What works differently on a supercomputer like at OSC? (Click to expand)\n\n\n\n\n\nCompared to command-line computing on a laptop or desktop, a number of aspects are different when working on a supercomputer like at OSC. You’ll learn much more about these later on, but here is an overview:\n\n“Non-interactive” computing is common\nIt is common to write and “submit” scripts to a queue instead of running programs interactively.\nSoftware\nYou generally can’t install “the regular way”, and a lot of installed software needs to be “loaded”.\nOperating system\nSupercomputers run on the Linux operating system.\nLogin versus compute nodes\nAs mentioned, the nodes you end up on after logging in are not meant for heavy computing and you have to request access to “compute nodes” to run most analyses."
  },
  {
    "objectID": "osc/01_osc1.html#osc-ondemand",
    "href": "osc/01_osc1.html#osc-ondemand",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "4 OSC OnDemand",
    "text": "4 OSC OnDemand\nThe OSC OnDemand web portal allows you to use a web browser to access OSC resources such as:\n\nA file browser where you can also create and rename folders and files, etc.\nA Unix shell\n“Interactive Apps”: programs such as RStudio, Jupyter, VS Code and QGIS.\n\n Go to https://ondemand.osc.edu and log in (use the boxes on the left-hand side)\nYou should see a landing page similar to the one below:\n\n\n\nWe will now go through some of the dropdown menus in the blue bar along the top.\n\n\n4.1 Files: File system access\nHovering over the Files dropdown menu gives a list of directories that you have access to. If your account is brand new, and you were added to PAS2700, you should only have three directories listed:\n\nA Home directory (starts with /users/)\nThe PAS2700 project’s “scratch” directory (/fs/scratch/PAS2700)\nThe PAS2700 project’s “project” directory (/fs/ess/PAS2700)\n\nYou will only ever have one Home directory at OSC, but for every additional project you are a member of, you should usually see additional /fs/ess and /fs/scratch directories appear.\n Click on our focal directory /fs/ess/PAS2700.\n\n\n\n\n\nOnce there, you should see whichever directories and files are present at the selected location, and you can click on the directories to explore the contents further:\n\n\n\n\n\nThis interface is much like the file browser on your own computer, so you can also create, delete, move and copy files and folders, and even upload (from your computer to OSC) and download (from OSC your computer) files3 — see the buttons across the top.\n\n\n\n4.2 Interactive Apps\nWe can access programs with Graphical User Interfaces (GUIs; point-and-click interfaces) via the Interactive Apps dropdown menu:\n\n\n\n\n\nIn the next tutorial, we will start using the VS Code text editor, which is listed here as Code Server.\n\n\n\n4.3 Clusters: Unix shell access\n\n\n\n\n\n\nSystem Status within Clusters (Click to expand)\n\n\n\n\n\nIn the “Clusters” dropdown menu, click on the item at the bottom, “System Status”:\n\n\n\n\n\nThis page shows an overview of the live, current usage of the two clusters — that can be interesting to get a good idea of the scale of the supercomputer center, which cluster is being used more, what the size of the “queue” (which has jobs waiting to start) is, and so on.\n\n\n\n\n\n\n\n\nInteracting with a supercomputer is most commonly done using a Unix shell. Under the Clusters dropdown menu, you can access a Unix shell either on Owens or Pitzer:\n\n\n\n\n\nI’m selecting a shell on the Pitzer supercomputer (“Pitzer Shell Access”), which will open a new browser tab, where the bottom of the page looks like this:\n\n\n\n\n\nWe’ll return to this Unix shell in the next session."
  },
  {
    "objectID": "osc/01_osc1.html#footnotes",
    "href": "osc/01_osc1.html#footnotes",
    "title": "Intro to the Ohio Supercomputer Center (OSC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n But we don’t have to pay anything for educational projects like this one. Otherwise, for OSC’s rates for academic research, see this page.↩︎\nGPUs are e.g. used for Nanopore basecalling↩︎\nThough this is not meant for large (&gt;1 GB) transfers. Different methods are available — we’ll talk about those later on.↩︎"
  },
  {
    "objectID": "osc/02_vscode.html#vs-code",
    "href": "osc/02_vscode.html#vs-code",
    "title": "VS Code",
    "section": "1 VS Code",
    "text": "1 VS Code\n\n1.1 Why VS Code?\nVS Code is basically a fancy text editor. Its full name is Visual Studio Code, and it’s also called “Code Server” at OSC.\nTo emphasize the additional functionality relative to basic text editors like Notepad and TextEdit, editors like VS Code are also referred to as “IDEs”: Integrated Development Environments. The RStudio program is another good example of an IDE. Just like RStudio is an IDE for R, VS Code will be our IDE for shell code.\nSome advantages of VS Code:\n\nWorks with all operating systems, is free, and open source.\nHas an integrated terminal.\nVery popular nowadays – lots of development going on including by users (extensions).\nAvailable at OSC OnDemand (and also allows you to SSH-tunnel-in with your local installation).\n\n\n\n\n1.2 Starting VS Code at OSC\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and near the bottom, click Code Server.\nInteractive Apps like VS Code and RStudio run on compute nodes (not login nodes). Because compute nodes always need to be “reserved”, we have to fill out a form and specify the following details:\n\nThe OSC “Project” that we want to bill for the compute node usage: PAS2700.\nThe “Number of hours” we want to make a reservation for: 2\nThe “Working Directory” for the program: your personal folder in /fs/ess/PAS2700/users (e.g. /fs/ess/PAS2700/users/jelmer)\nThe “Codeserver Version”: 4.8 (most recent)\n\nClick Launch.\nFirst, your job will be “Queued” — that is, waiting for the job scheduler to allocate compute node resources to it:\n\n\n\n\n\n\n\nYour job is typically granted resources within a few seconds (the card will then say “Starting”), and should be ready for usage (“Running”) in another couple of seconds:\n\n\n\n\n\n\n\nOnce it appears, click on the blue Connect to VS Code button to open VS Code in a new browser tab.\nWhen VS Code opens, you may get these two pop-ups (and possibly some others) — click “Yes” (and check the box) and “Don’t Show Again”, respectively:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll also get a Welcome/Get Started page — you don’t have to go through steps that may be suggested there.\n\n\n\n\n1.3 The VS Code User Interface\n\n\n\n\n\n\nSide bars\nThe Activity Bar (narrow side bar) on the far left has:\n\nA  (“hamburger menu”), which has menu items like File that you often find in a top bar.\nA  (cog wheel icon) in the bottom, through which you can mainly access settings.\nIcons to toggle (wide/Primary) Side Bar options:\n\nExplorer: File browser & outline for the active file.\nSearch: To search recursively across all files in the active folder.\nSource Control: To work with Git\nDebugger\nExtensions: To install extensions\n\n\n\n\n\n\n\n\nToggle (hide/show) the side bars\n\n\n\nIf you want to save some screen space while coding along in class, you may want to occasionally hide the side bars:\n\nIn  &gt; View &gt; Appearance you can toggle both the Activity Bar and the Primary Side Bar.\nOr use keyboard shortcuts:\n\nCtrl/⌘+B for the primary/wide side bar\nCtrl+Shift+B for the activity/narrow side bar\n\n\n\n\n\n\n\n Exercise: Try a few color themes\n\nAccess the “Color Themes” option by clicking  =&gt; Color Theme.\nTry out a few themes and see pick one you like!\n\n\n\n\nTerminal (with a Unix shell)\n Open a terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nCreate a directory for this tutorial, e.g.:\n# You should be in your personal dir in /fs/ess/PAS2700\npwd\n/fs/ess/PAS2700/users/jelmer\nmkdir week02\n\n\n\nEditor pane and Welcome document\nThe main part of the VS Code is the editor pane. Here, we can open files like scripts and other types of text files, and images. (Whenever you open VS Code, an editor tab with a Welcome document is automatically opened. This provides some help and some shortcuts like to recently opened files and folders.)\n Let’s create and save a new file:\n\nOpen a new file: Click the hamburger menu , then File &gt; New File.\nSave the file (Ctrl/⌘+S), inside the dir you just created, as a Markdown file, e.g. markdown-intro.md. (Markdown files have the extension .md.)\n\n\n\n\n\n1.4 A folder as a starting point\nConveniently, VS Code takes a specific directory as a starting point in all parts of the program:\n\nIn the file explorer in the side bar\nIn the terminal\nWhen saving files in the editor pane.\n\nThis is why your terminal was “already” located in /fs/ess/PAS2700/users/$USER.\n\n\n\n\n\n\nIf you need to switch folders, click      &gt;   File   &gt;   Open Folder.\n\n\n\n\n\n\n\n\n\n\n\n\nTaking off where you were\n\n\n\nWhen you reopen a folder you’ve had open before, VS Code will resume where you were before in that it will:\n\nRe-open any files you had open in the editor pane\nRe-open a terminal if you had one active\n\nThis is quite convenient, especially when you start working on multiple projects and frequently switch between those.\n\n\n\n\nSome tips and tricks\n\nResizing panes\nYou can resize panes (terminal, editor, side bar) by hovering your cursor over the borders and then dragging.\nThe Command Palette\nTo access all the menu options that are available in VS Code, the so-called “Command Palette” can be handy, especially if you know what you are looking for. To access the Command Palette, click      and then Command Palette (or press F1 or Ctrl/⌘+Shift+P). To use it, start typing something to look for an option.\nKeyboard shortcuts\nFor a single-page PDF overview of keyboard shortcuts for your operating system:    =&gt;   Help   =&gt;   Keyboard Shortcut Reference. (Or for direct links to these PDFs: Windows / Mac / Linux.) A couple of useful keyboard shortcuts are highlighted below.\n\n\n\n\n\n\n\nSpecific useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nWorking with keyboard shortcuts for common operations can be a lot faster than using your mouse. Below are some useful ones for VS Code (for Mac, in some case, you’ll have to replace Ctrl with ⌘):\n\nOpen a terminal: Ctrl+` (backtick) or Ctrl+Shift+C.\nToggle between the terminal and the editor pane: Ctrl+` and Ctrl+1.\nLine actions:\n\nCtrl/⌘+X / C will cut/copy the entire line where the cursor is, when nothing is selected (!)\nCtrl/⌘+Shift+K will delete a line\nAlt/Option+⬆/⬇ will move lines up or down.\n\n\n\n\n\n\n\n Exercise: Install two extensions\nClick the gear icon  and then Extensions, and search for and then install:\n\nshellcheck (by simonwong) — this will check our shell scripts later on!\nRainbow CSV (by mechatroner) — make CSV/TSV files easier to view with column-based colors"
  },
  {
    "objectID": "reprod/project-org.html#overview-of-this-week",
    "href": "reprod/project-org.html#overview-of-this-week",
    "title": "Project organization",
    "section": "1 Overview of this week",
    "text": "1 Overview of this week\n\nThis page:\n\nLearn some best practices for project organization, documentation, and management.\n\nAlso today:\n\nGet to know our text editor, VS Code.\nLearn how to use Markdown for documentation (and beyond).\n\nThursday\n\nLearn how to manage files in the Unix shell."
  },
  {
    "objectID": "reprod/project-org.html#project-organization-best-practices-recommendations",
    "href": "reprod/project-org.html#project-organization-best-practices-recommendations",
    "title": "Project file organization",
    "section": "1 Project organization: best practices & recommendations",
    "text": "1 Project organization: best practices & recommendations\nGood project organization and documentation facilitates:\n\nCollaborating with others (and yourself in the future…)\nReproducibility\nAutomation\nVersion control\nPreventing your files slowly devolving into a state of incomprehensible chaos\n\n\n\n1.1 Some underlying principles\n\nUse one dir (dir hierarchy) for one project\nUsing one directory hierarchy for one project means:\n\nDon’t mix files/subdirs for multiple distinct projects inside one dir.\nDon’t keep files for one project in multiple places.\n\nWhen you have a single directory hierarchy for each project, it is:\n\nEasier to find files, share your project, not throw away stuff in error, etc.\nPossible to use relative paths within a project’s scripts, which makes it more portable (more on that in a bit).\n\n\n\n\n\nTwo project dir hierarchies, nicely separated and self-contained.\n\n\n\n\n\nSeparate different kinds of files using a consistent dir structure\nWithin your project’s directory hierarchy:\n\nSeparate code from data.\nSeparate raw data from processed data & results.\n\nAlso:\n\nTreat raw data as read-only.\nTreat generated output as somewhat disposable and as possible to regenerate.\n\nAnd, as we’ll talk about below:\n\nUse consistent dir and file naming that follow certain best practices.\nSlow down and document what you’re doing.\n\n\n\n\n\n1.2 Absolute versus relative paths\nRecall that:\n\nAbsolute paths start from the computer’s root dir and do not depend on your working dir.\nRelative paths start from a specific working dir (and won’t work if you’re elsewhere).\n\n\n\nDon’t absolute paths sound better? What could be a disadvantage of them?\n\nAbsolute paths: - Don’t generally work across computers - Break when your move a project\nWhereas relative paths, as long as you consistently use the root of the project as the working dir, keep working when moving the project within and between computers.\n\n\n\n\n\nTwo project dir hierarchies, and the absolute and relative path to a FASTQ file.\n\n\n\n\n\n\nNow everything was moved into Dropbox.The absolute path has changed, but the relative path remains the same.\n\n\n\n\n\n1.3 But how to define and separate projects?\nFrom Wilson et al. 2017 - Good Enough Practices in Scientific Computing:\n\nAs a rule of thumb, divide work into projects based on the overlap in data and code files:\n\nIf 2 research efforts share no data or code, they will probably be easiest to manage independently.\nIf they share more than half of their data and code, they are probably best managed together.\nIf you are building tools that are used in several projects, the common code should probably be in a project of its own.\n\n\n\nProjects with shared data or code\nTo access files outside of the project (e.g., shared across projects), it is easiest to create links to these files:\n\n\n\nThe data is located in project1 but used in both projects.project2 contains a link to the data.\n\n\nBut shared data or scripts are generally better stored in separate dirs, and then linked to by each project using them:\n\n\n\nNow, the data is in it’s own top-level dir, with links to it in both projects.\n\n\nThese strategies do decrease the portability of your project, and moving the shared files even within your own computer will cause links to break.\nA more portable method is to keep shared (multi-project) files online — this is especially feasible for scripts under version control:\n\n\n\nA set of scripts shared by two projects is stored in an online repository like at GitHub.\n\n\n\nFor data, this is also possible but often not practical due to file sizes. It’s easier after data has been deposited in a public repository.\n\n\n\n\n\n1.4 Example project dir structure\nHere is one good way of organizing a project with top-levels dirs:\n\n\n\n\n\n\n\n\n\n\n\nOther reasonable options\n\n\n\nThese recommendations only go so far, and several things do depend on personal preferences and project specifics:\n\ndata as single top-level dir, or separate metadata, refdata, raw_data dirs?\n\nNaming of some dirs, like:\n\nresults vs analysis (Buffalo)\nsrc (source) vs scripts\n\nSometimes the order of subdirs can be done in multiple different ways. For example, where to put QC figures — results/plots/qc or results/qc/plots/?\n\n\n\nAnother important good practice is to use subdirectories liberally and hierarchically. For example, in omics data analysis, it often makes sense to create subdirs within results for each piece of software that you are using:"
  },
  {
    "objectID": "reprod/project-org.html#file-naming",
    "href": "reprod/project-org.html#file-naming",
    "title": "Project file organization",
    "section": "2 File naming",
    "text": "2 File naming\nThree principles for good file names (from Jenny Bryan):\n\nMachine-readable\nHuman-readable\nPlaying well with default ordering\n\n\nMachine-readable\nConsistent and informative naming helps you to programmatically find and process files.\n\nIn file names, provide metadata like Sample ID, date, and treatment:\n\nsample032_2016-05-03_low.txt\n\nsamples_soil_treatmentA_2019-01.txt\n\nWith such file names, you can easily select samples from e.g. a certain month or treatment (more on Thursday):\nls *2016-05*\n\nls *treatmentA*\nSpaces in file names lead to inconvenience at best and disaster at worst (see example below).\nMore generally, only use the following in file names:\n\nAlphanumeric characters A-Za-z0-9\nUnderscores _\nHyphens (dashes) -\nPeriods (dots) .\n\n\n\n\n\n Spaces in file names — what could go wrong?\n\nSay, you have a dir with some raw data in the dir raw:\nls\nraw\nNow you create a dir for sequences, with a space in the file name:\nmkdir \"raw sequences\"\nYou don’t want this dir after all, and carelessly try to remove it\nrm -r raw sequences\n\n\n\n\nWhat will go wrong in the example above? (Click for the answer)\n\nThe rm command will not remove the dir with the space in the file name, but it will remove the “earlier” raw dir.\n\n\n\n\nHuman-readable\n\n“Name all files to reflect their content or function. For example, use names such as bird_count_table.csv, manuscript.md, or sightings_analysis.py.”\n— Wilson et al. 2017\n\n\n\n\nCombining machine- and human-readable\n\nOne good way (opinionated recommendations):\n\nUse underscores (_) to delimit units you may later want to separate on: sampleID, batch, treatment, date.\nWithin such units, use dashes (-) to delimit words: grass-samples.\nLimit the use of periods (.) to indicate file extensions.\nGenerally avoid capitals.\n\nFor example:\nmmus001_treatmentA_filtered-mq30-only_sorted_dedupped.bam\nmmus002_treatmentA_filtered-mq30-only_sorted_dedupped.bam\n.\n.\nmmus086_treatmentG_filtered-mq30-only_sorted_dedupped.bam\n\n\n\n\nPlaying well with default ordering\n\nUse leading zeros for lexicographic sorting: sample005.\nDates should always be written as YYYY-MM-DD: 2020-10-11.\nGroup similar files together by starting with same phrase, and number scripts by execution order:\nDE-01_normalize.R\nDE-02_test.R\nDE-03_process-significant.R"
  },
  {
    "objectID": "reprod/project-org.html#slow-down-and-document",
    "href": "reprod/project-org.html#slow-down-and-document",
    "title": "Project file organization",
    "section": "3 Slow down and document",
    "text": "3 Slow down and document\n\nUse README files to document\nUse README files to document the following:\n\nYour methods\nWhere/when/how each data and metadata file originated\nVersions of software, databases, reference genomes\n…Everything needed to rerun whole project\n\n\n\n\n\n\n\nSee this week’s Buffalo chapter (Ch. 2) for further details.\n\n\n\n\n\n\n\n\n\nFor documentation, use plain text files\nPlain text files offer several benefits over proprietary & binary formats (like .docx and .xlsx)1:\n\nCan be accessed on any computer, including over remote connections\nAre future-proof\nAllow to be version-controlled\n\nMarkdown files are plain-text and strike a nice balance between ease of writing and reading, and added functionality — we’ll talk about those next."
  },
  {
    "objectID": "reprod/project-org.html#footnotes",
    "href": "reprod/project-org.html#footnotes",
    "title": "Project file organization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese considerations apply not just to files for documentation, but also to data files, etc!↩︎"
  },
  {
    "objectID": "reprod/markdown.html",
    "href": "reprod/markdown.html",
    "title": "Markdown",
    "section": "",
    "text": "Markdown is a very lightweight text markup language that is:\n\nEasy to write — a dozen or so syntax constructs is nearly all you use.\nEasy to read — also in its raw (non-rendered) form.\n\nFor example, surrounding one or more characters by single or double asterisks (*) will make those characters italic or bold, respectively:\n\nWhen you write *italic example* this will be rendered as: italic example.\nWhen you write **bold example**this will be rendered as: bold example.\n\nSource Markdown files are plain text files (they can be “rendered” to HTML or PDF). I recommend that you use Markdown files (.md) instead of plain text (.txt) files to document your research projects as outlined in the previous session.\n\n\n\n\n\n\nMarkdown documentation\n\n\n\nLearn more about Markdown and its syntax in this excellent documentation: https://www.markdownguide.org.\n\n\n\n\n\nBelow, we’ll be trying some Markdown syntax in the markdown-intro.md file we created earlier.\nWhen you save a file in VS Code with an .md extension, as you have done:\n\nSome formatting will be automatically applied in the editor.\nYou can open a live rendered preview by pressing the icon to “Open Preview to the Side” (top-right corner):\n\n\n\n\n\n\nThat will look something like this in VS Code:\n\n\n\n\n\n\n\n\n\nHere is an overview of the most commonly used Markdown syntax:\n\n\n\nSyntax\nResult\n\n\n\n\n*italic*\nitalic (alternative: single _)\n\n\n**bold**\nbold (alternative: double _)\n\n\n[link text](website.com)\nlink text\n\n\n&lt;https://website.com&gt;\nClickable link: https://website.com\n\n\n# My Title\nHeader level 1 (largest)\n\n\n## My Section\nHeader level 2\n\n\n### My Subsection\nHeader level 3 – and so forth\n\n\n- List item\nUnordered (bulleted) list\n\n\n1. List item\nOrdered (numbered) list\n\n\n`inline code`\ninline code\n\n\n```\nStart/end of generic code block (on its own line)\n\n\n```bash\nStart of bash code block (end with ```)\n\n\n---\nHorizontal rule (line)\n\n\n&gt; Text\nBlockquote (like quoted text in emails)\n\n\n![](path/to/figure.png)\n[The figure will be inserted]\n\n\n\n\nLet’s try some of these things — type:\n# Introduction to Markdown\n\n## Part 1: Documentation\n\n- The Markdown _documentation_ can be found [here](https://www.markdownguide.org/)\n- To be clear, **the URL is &lt;https://www.markdownguide.org/&gt;**.\n\n## Part 2: The basics\n\n1. When you create a _numbered_ list...\n1. ...you don't need the numbers to increment.\n1. Markdown will take care of that for you.\n\n--------\n\n### Part 2b: Take it from the experts\n\n&gt; Markdown will take your science to the next level\n&gt; -- Wilson et al. 1843\n\n--------\n\n## Part 3: My favorite shell commands\n\nThe `date` command is terribly useful.\n\nHere is my shell code in a code block:\n\n```bash\n# Print the current date and time\ndate\n\n# List the files with file sizes\nls -lh\n```\n\n**The end.**\n\nThat should be previewed/rendered as:\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for a side-by-side screenshot in VS Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTables are not all that convenient to create in Markdown, but you can do it as follows.\n\n\n\n\nThis:\n| city             | inhabitants |\n|——————|——————|\n| Columbus   | 906 K       |\n| Cleveland   | 368 K       |\n| Cincinnati   | 308 K       |\n\nWill be rendered as:\n\n\n\ncity            \ninhabitants\n\n\n\n\nColumbus  \n906 K      \n\n\nCleveland  \n368 K      \n\n\nCincinnati  \n308 K      \n\n\n\n\n\n\n\n\n\n\nIt’s recommended (in some cases necessary) to leave a blank line between different sections: lists, headers, etc.:\n## Section 2: List of ...\n\n- Item 1\n- Item 2\n\nFor example, ....\n\n\n\nA blank line between regular text will start a new paragraph, with some whitespace between the two:\n\n\n\n\n\nThis:\n\nParagraph 1.\n  \nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1.\nParagraph 2.\n\n\n\n\nWhereas a single newline will be completely ignored!:\n\n\n\n\n\nThis:\n\nParagraph 1.\nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1. Paragraph 2.\n\n\n\n\n\n\n\nThis:\n\nWriting  \none  \nword  \nper  \nline.\n\nWill be rendered as:\n\nWriting one word per line.\n\n\n\n\nMultiple consecutive spaces and blank line will be “collapsed” into a single space/blank line:\n\n\n\n\n\nThis:\n\nEmpty             space\n\nWill be rendered as:\n\nEmpty space\n\n\n\n\n\n\n\nThis:\n\nMany\n\n\n\n\nblank lines\n\nWill be rendered as:\n\nMany\nblank lines\n\n\n\n\nA single linebreak can be forced using two or more spaces (i.e., press the spacebar twice) or a backslash \\ after the last character on a line:\n\n\n\n\n\nThis:\n\nMy first sentence.\\\nMy second sentence.\n\nWill be rendered as:\n\nMy first sentence.\nMy second sentence.\n\n\n\n\nIf you want more vertical whitespace than what is provided between paragraphs, you’ll have to resort to HTML1: each &lt;br&gt; item forces a visible linebreak.\n\n\n\n\n\nThis:\n\nOne &lt;br&gt; word &lt;br&gt; per line\nand &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;\nseveral blank lines.\n\nWill be rendered as:\n\nOne  word  per line and      several blank lines.\n\n\n\n\n\n\n\n\n\nSidenote: HTML and CSS in Markdown\n\n\n\n\nIf you need “inline colored text”, you can also use HTML:\ninline &lt;span style=\"color:red\"&gt;colored&lt;/span&gt; text.\nFor systematic styling of existing or custom elements, you need to use CSS. For example, including the following anywhere in a Markdown document will turn all level 1 headers (#) red:\n&lt;style&gt;\nh1 {color: red}\n&lt;/style&gt;\n\n\n\n\n\n\n\nSeveral Markdown extensions allow Markdown documents to contain code that runs, and whose output can be included in rendered documents:\n\nR Markdown (.Rmd) and the follow-up Quarto\nJupyter Notebooks\n\nThere are many possibilities with Markdown! For instance, consider that:\n\nThis website is written using Quarto.\nR Markdown/Quarto also has support for citations, journal-specific formatting, etc., so you can even write manuscripts with it.\n\n\n\n\n\n\n\n\nPandoc to render Markdown files (Click to expand)\n\n\n\n\n\nI very rarely render “plain” Markdown files because:\n\nMarkdown source is so well readable\nGitHub will render Markdown files for you\n\nThat said, if you do need to render a Markdown file to, for example, HTML or PDF, use Pandoc:\npandoc README.md &gt; README.html\npandoc -o README.pdf README.md\nFor installation (all OS’s): see https://pandoc.org/installing.html.\n\n\n\n\n\n\n\n\n\nSome additional Markdown syntax (Click to expand)\n\n\n\n\n\nThe below is “extended syntax” that is not supported by all interpreters:\n\n\n\nSyntax\nResult\n\n\n\n\n~~strikethrough~~\nstrikethrough\n\n\nFootnote ref[^1]\nFootnote ref1\n\n\n[^1]: Text\nThe actual footnote"
  },
  {
    "objectID": "reprod/markdown.html#an-introduction-to-markdown",
    "href": "reprod/markdown.html#an-introduction-to-markdown",
    "title": "Markdown",
    "section": "",
    "text": "Markdown is a very lightweight text markup language that is:\n\nEasy to write — a dozen or so syntax constructs is nearly all you use.\nEasy to read — also in its raw (non-rendered) form.\n\nFor example, surrounding one or more characters by single or double asterisks (*) will make those characters italic or bold, respectively:\n\nWhen you write *italic example* this will be rendered as: italic example.\nWhen you write **bold example**this will be rendered as: bold example.\n\nSource Markdown files are plain text files (they can be “rendered” to HTML or PDF). I recommend that you use Markdown files (.md) instead of plain text (.txt) files to document your research projects as outlined in the previous session.\n\n\n\n\n\n\nMarkdown documentation\n\n\n\nLearn more about Markdown and its syntax in this excellent documentation: https://www.markdownguide.org.\n\n\n\n\n\nBelow, we’ll be trying some Markdown syntax in the markdown-intro.md file we created earlier.\nWhen you save a file in VS Code with an .md extension, as you have done:\n\nSome formatting will be automatically applied in the editor.\nYou can open a live rendered preview by pressing the icon to “Open Preview to the Side” (top-right corner):\n\n\n\n\n\n\nThat will look something like this in VS Code:\n\n\n\n\n\n\n\n\n\nHere is an overview of the most commonly used Markdown syntax:\n\n\n\nSyntax\nResult\n\n\n\n\n*italic*\nitalic (alternative: single _)\n\n\n**bold**\nbold (alternative: double _)\n\n\n[link text](website.com)\nlink text\n\n\n&lt;https://website.com&gt;\nClickable link: https://website.com\n\n\n# My Title\nHeader level 1 (largest)\n\n\n## My Section\nHeader level 2\n\n\n### My Subsection\nHeader level 3 – and so forth\n\n\n- List item\nUnordered (bulleted) list\n\n\n1. List item\nOrdered (numbered) list\n\n\n`inline code`\ninline code\n\n\n```\nStart/end of generic code block (on its own line)\n\n\n```bash\nStart of bash code block (end with ```)\n\n\n---\nHorizontal rule (line)\n\n\n&gt; Text\nBlockquote (like quoted text in emails)\n\n\n![](path/to/figure.png)\n[The figure will be inserted]\n\n\n\n\nLet’s try some of these things — type:\n# Introduction to Markdown\n\n## Part 1: Documentation\n\n- The Markdown _documentation_ can be found [here](https://www.markdownguide.org/)\n- To be clear, **the URL is &lt;https://www.markdownguide.org/&gt;**.\n\n## Part 2: The basics\n\n1. When you create a _numbered_ list...\n1. ...you don't need the numbers to increment.\n1. Markdown will take care of that for you.\n\n--------\n\n### Part 2b: Take it from the experts\n\n&gt; Markdown will take your science to the next level\n&gt; -- Wilson et al. 1843\n\n--------\n\n## Part 3: My favorite shell commands\n\nThe `date` command is terribly useful.\n\nHere is my shell code in a code block:\n\n```bash\n# Print the current date and time\ndate\n\n# List the files with file sizes\nls -lh\n```\n\n**The end.**\n\nThat should be previewed/rendered as:\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for a side-by-side screenshot in VS Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTables are not all that convenient to create in Markdown, but you can do it as follows.\n\n\n\n\nThis:\n| city             | inhabitants |\n|——————|——————|\n| Columbus   | 906 K       |\n| Cleveland   | 368 K       |\n| Cincinnati   | 308 K       |\n\nWill be rendered as:\n\n\n\ncity            \ninhabitants\n\n\n\n\nColumbus  \n906 K      \n\n\nCleveland  \n368 K      \n\n\nCincinnati  \n308 K      \n\n\n\n\n\n\n\n\n\n\nIt’s recommended (in some cases necessary) to leave a blank line between different sections: lists, headers, etc.:\n## Section 2: List of ...\n\n- Item 1\n- Item 2\n\nFor example, ....\n\n\n\nA blank line between regular text will start a new paragraph, with some whitespace between the two:\n\n\n\n\n\nThis:\n\nParagraph 1.\n  \nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1.\nParagraph 2.\n\n\n\n\nWhereas a single newline will be completely ignored!:\n\n\n\n\n\nThis:\n\nParagraph 1.\nParagraph 2.\n\nWill be rendered as:\n\nParagraph 1. Paragraph 2.\n\n\n\n\n\n\n\nThis:\n\nWriting  \none  \nword  \nper  \nline.\n\nWill be rendered as:\n\nWriting one word per line.\n\n\n\n\nMultiple consecutive spaces and blank line will be “collapsed” into a single space/blank line:\n\n\n\n\n\nThis:\n\nEmpty             space\n\nWill be rendered as:\n\nEmpty space\n\n\n\n\n\n\n\nThis:\n\nMany\n\n\n\n\nblank lines\n\nWill be rendered as:\n\nMany\nblank lines\n\n\n\n\nA single linebreak can be forced using two or more spaces (i.e., press the spacebar twice) or a backslash \\ after the last character on a line:\n\n\n\n\n\nThis:\n\nMy first sentence.\\\nMy second sentence.\n\nWill be rendered as:\n\nMy first sentence.\nMy second sentence.\n\n\n\n\nIf you want more vertical whitespace than what is provided between paragraphs, you’ll have to resort to HTML1: each &lt;br&gt; item forces a visible linebreak.\n\n\n\n\n\nThis:\n\nOne &lt;br&gt; word &lt;br&gt; per line\nand &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;\nseveral blank lines.\n\nWill be rendered as:\n\nOne  word  per line and      several blank lines.\n\n\n\n\n\n\n\n\n\nSidenote: HTML and CSS in Markdown\n\n\n\n\nIf you need “inline colored text”, you can also use HTML:\ninline &lt;span style=\"color:red\"&gt;colored&lt;/span&gt; text.\nFor systematic styling of existing or custom elements, you need to use CSS. For example, including the following anywhere in a Markdown document will turn all level 1 headers (#) red:\n&lt;style&gt;\nh1 {color: red}\n&lt;/style&gt;\n\n\n\n\n\n\n\nSeveral Markdown extensions allow Markdown documents to contain code that runs, and whose output can be included in rendered documents:\n\nR Markdown (.Rmd) and the follow-up Quarto\nJupyter Notebooks\n\nThere are many possibilities with Markdown! For instance, consider that:\n\nThis website is written using Quarto.\nR Markdown/Quarto also has support for citations, journal-specific formatting, etc., so you can even write manuscripts with it.\n\n\n\n\n\n\n\n\nPandoc to render Markdown files (Click to expand)\n\n\n\n\n\nI very rarely render “plain” Markdown files because:\n\nMarkdown source is so well readable\nGitHub will render Markdown files for you\n\nThat said, if you do need to render a Markdown file to, for example, HTML or PDF, use Pandoc:\npandoc README.md &gt; README.html\npandoc -o README.pdf README.md\nFor installation (all OS’s): see https://pandoc.org/installing.html.\n\n\n\n\n\n\n\n\n\nSome additional Markdown syntax (Click to expand)\n\n\n\n\n\nThe below is “extended syntax” that is not supported by all interpreters:\n\n\n\nSyntax\nResult\n\n\n\n\n~~strikethrough~~\nstrikethrough\n\n\nFootnote ref[^1]\nFootnote ref1\n\n\n[^1]: Text\nThe actual footnote"
  },
  {
    "objectID": "reprod/markdown.html#footnotes",
    "href": "reprod/markdown.html#footnotes",
    "title": "Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can use any HTML markup in Markdown!↩︎"
  },
  {
    "objectID": "metabar/01_qc-trim.html#introduction",
    "href": "metabar/01_qc-trim.html#introduction",
    "title": "Read QC and primer trimming",
    "section": "Introduction",
    "text": "Introduction\nThe first series of steps of a metabarcoding analysis workflow concerns the quality control (QC) & “pre-processing” of the raw sequence reads, which are stored in FASTQ files.\nThe QC part will leave the data untouched, whereas the pre-processing involves the removal of unwanted bits of sequence: in our case, amplicon primers. After the pre-processing step, we will still have FASTQ files, just with somewhat less content.\nSpecifically, we will go through the following steps:\n\nQC with FastQC\nSummarizing FastQC results with MultiQC\nRemoving primers with Cutadapt\n\n\n\n\n\n\n\n\nYou should have an active VS Code session. If not, follow these steps (Click to expand)\n\n\n\n\n\nStart a new VS Code session with an open terminal:\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2714\nThe starting directory: /fs/ess/PAS2714/&lt;user&gt; (replace &lt;user&gt; with your username)\nNumber of hours: 3\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal."
  },
  {
    "objectID": "metabar/01_qc-trim.html#running-fastqc-for-1-sample",
    "href": "metabar/01_qc-trim.html#running-fastqc-for-1-sample",
    "title": "Read QC and primer trimming",
    "section": "1 Running FastQC for 1 sample",
    "text": "1 Running FastQC for 1 sample\n\n1.1 Intro to FastQC\nFastQC is a ubiquitous tools for quality control of FASTQ files. Running FastQC or a similar program is the first step in nearly any high-throughput sequencing project. FastQC is also a good introductory example of a tool with a command-line interface.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser with about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph shown below.\n\n\n\nA FastQC per-base quality score graph for files with reasonably good quality reads. The y-axis shows Phred quality scores (higher is better, see also the color-coding) and the x-axis shows the position along the read.\n\n\n\n\n\n1.2 Our FASTQ files\nOur FASTQ files contain reads from 2x300 bp (i.e. paired-end with 300 bp forward and 300 bp reverse reads) sequencing on an Illumina MiSeq machine.\nLet’s take a look at our list of FASTQ files:\nls -lh data/fastq\ntotal 150M\n-rw-r-----+ 1 jelmer PAS0471 2.0M Mar  1 17:09 NW102AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 17:09 NW102AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 17:09 NW102C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.0M Mar  1 17:09 NW102C_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 1.9M Mar  1 17:09 NW103AB_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.6M Mar  1 17:09 NW103AB_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 2.3M Mar  1 17:09 NW103C_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 3.1M Mar  1 17:09 NW103C_R2.fastq.gz\n# [...output truncated...]\nNote in the file listing above that:\n\nThere are two files per sample: _R1 (forward reads) and _R2 (reverse reads). This indicates that we have data from paired-end reads, as is customary when doing amplicon metabarcoding.\nThe files all have a .gz extension, indicating they have been compressed with the gzip utility.\n\n\n\n\n\n\n\nDon’t have the FASTQ files? Click here for instructions to copy them.\n\n\n\n\n\nRun the following two cp commands to copy the necessary data:\ncp -rv /fs/ess/PAS2714/share/data /fs/ess/PAS2714/users/$USER\ncp -rv /fs/ess/PAS2714/share/results /fs/ess/PAS2714/users/$USER\n\n\n\n\n\n\n1.3 Building our FastQC command\nTo run FastQC, we can use the command fastqc.\nIf you want to analyze one of your FASTQ files with default FastQC settings, a complete FastQC command to do so would simply be fastqc followed by the name of the file:\n# (Don't run this)\nfastqc data/fastq/NW102AB_R1.fastq.gz\nHowever, an annoying FastQC default behavior is that it writes its output files in the dir where the input files are — in general, it’s not great practice to directly mix your primary data and results like that!\nTo figure out how we can change that behavior, first consider that many commands and bioinformatics tools alike have an option -h and/or --help to print usage information to the screen. Let’s try that:\nfastqc -h\nbash: fastqc: command not found...\nHowever, there is a wrinkle: while FastQC is installed at OSC1, we have to first “load it”. The way we will do this here is with a a so-called “Conda environment” that has FastQC installed along with the other programs we will need today.\nHere’s how we can load that Conda software environment — we first load OSC’s (mini)conda installation, and then we can load (“activate”) the Conda environment that I created for you:\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n\n Exercise: FastQC help and output dir\nPrint FastQC’s help info, and figure out which option you can use to specify a custom output directory.\n\n\nClick for the solution\n\nfastqc -h and fastqc --help will both work to show the help info.\nYou’ll get quite a bit of output printed to screen, including the snippet about output directories that is reproduced below:\nfastqc -h\n  -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\nSo, you can use -o or equivalently, --outdir to specify an output dir.\n\n\n\nWith the added --outdir (or -o) option, let’s try to run the following FastQC command:\n# We'll have to first create the outdir ourselves, in this case\nmkdir -p results/fastqc\n\n# Now we run FastQC\nfastqc --outdir results/fastqc data/fastq/NW102AB_R1.fastq.gz\napplication/gzip\nStarted analysis of NW102AB_R1.fastq.gz\nApprox 5% complete for NW102AB_R1.fastq.gz\nApprox 10% complete for NW102AB_R1.fastq.gz\nApprox 15% complete for NW102AB_R1.fastq.gz\n[...truncated...]\nAnalysis complete for NW102AB_R1.fastq.gz\nSuccess!! 🎉\n\n\n\n1.4 FastQC output files\nLet’s take a look at the files in the output dir we specified:\nls -lh results/fastqc\ntotal 1.2M\n-rw-r--r-- 1 jelmer PAS0471 241K Mar 13 14:50 NW102AB_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 256K Mar 13 14:50 NW102AB_R1_fastqc.zip\n\nThere is a .zip file, which contains tables with FastQC’s data summaries.\nThere is an .html (HTML) file, which contains plots — this is what we’ll look at next.\n\n\n\n Exercise: Another FastQC run\nRun FastQC for the corresponding R2 FASTQ file. Would you use the same output dir?\n\n\nClick for the solution\n\nYes, it makes sense to use the same output dir, since as you could see above, the output file names have the input file identifiers in them. As such, we don’t need to worry about overwriting files, and it will be more convenient to have all results in a single dir.\nTo run FastQC for the R2 (reverse-read) file:\nfastqc --outdir results/fastqc data/fastq/NW102AB_R2.fastq.gz\nStarted analysis of NW102AB_R2.fastq.gz\nApprox 5% complete for NW102AB_R2.fastq.gz\nApprox 10% complete for NW102AB_R2.fastq.gz\nApprox 15% complete for NW102AB_R2.fastq.gz\n[...truncated...]\nAnalysis complete for NW102AB_R2.fastq.gz\nls -lh results/fastqc\n-rw-r--r-- 1 jelmer PAS0471 241K Mar 13 14:50 NW102AB_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 256K Mar 13 14:50 NW102AB_R1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 234K Mar 13 14:53 NW102AB_R2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 244K Mar 13 14:53 NW102AB_R2_fastqc.zip\nNow, we have four files: two for each of our preceding successful FastQC runs."
  },
  {
    "objectID": "metabar/01_qc-trim.html#interpreting-fastqc-output",
    "href": "metabar/01_qc-trim.html#interpreting-fastqc-output",
    "title": "Read QC and primer trimming",
    "section": "2 Interpreting FastQC output",
    "text": "2 Interpreting FastQC output\n\n2.1 FastQC HTML modules\nWe’ll now go through a couple of the FastQC plots/modules with example plots2 with good/bad results for reference.\nFastQC has “pass” (checkmark in green), “warning” (exclamation mark in orange), and “fail” (cross in red) assessments for each module. These assessments are handy, but a “warning”/“fail” is not necessarily the bad news it may appear to be:\n\nSome of these modules are perhaps overly strict.\nSome warnings and fails are easily remedied or simply not a very big deal.\nFastQC effectively assumes that your data is derived from whole-genome shotgun sequencing — some other types of data with different properties will therefore always trigger a couple of warnings and fails, but these are not meaningful. This is very much the case for metabarcoding data.\n\n\n\n\nAn example module results overview from a FastQC HTML file.The green checkmarks indicate Pass, the orange exclamation marks indicate Warning, and the red crosses indicate Fail.\n\n\n\n\nBasic statistics\nThis contains, among other things, the number of sequences (reads) and the read length range:\n\n\n\n\n\n\n\n\nPer base quality sequence quality\nThis figure visualizes the mean per-base quality score (y-axis) along the length of the reads (x-axis). Note that:\n\nA decrease in sequence quality along the reads (from left to right) is normal.\nR2 (reverse) reads are usually of worse quality than R1 (forward) reads.\n\n\n\n\n\n\nGood / acceptable\n\n\n\n\n\n\nBad\n\n\n\n\nTo interpret the y-axis quality scores, note the color scaling in the graphs above, and see this table for details:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent\n\n\n\n\n\n\nPer sequence quality scores\nThis shows the same quality scores we saw above, but now simply as a density plot of per-read averages, with the quality score now along the x-axis, and the number of reads with that quality score along the y-axis:\n\n\n\n\n\nGood\n\n\n\n\n\n\nBad\n\n\n\n\n\n\nPer tile sequence quality\nThis graph shows whether specific tiles (areas on the flow cell, see box below) have lower quality bases in some parts of or across all of the read.\nPoor qualities in some but not other tiles may indicate some (transient) problems with the flow cell. For example, if a tile has poor qualities for a stretch of bases, this could indicate that there was a bubble. If such tile-based problems are severe, you can contact the sequencing facility as they may be able to re-run your data at no cost.\n\n\n\n\n\n\nWhat is a tile? (Click to expand)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood\n\n\n\n\n\n\nSome problems\n\n\n\n\n\n\nSequence length distribution\nWith Illumina sequencing, the vast majority of reads typically have nearly the same read length — in our case, 300 bp. This graph can help you check if that is indeed the case. The module will throw a warning as soon as not all sequences are of the same length (like below), but having reads with a slightly shorter read length is normal and does not matter.\n\n\n\n\n\nGoodNearly all reads have the same read length. FastQC still threw a warning.\n\n\n\n\n\n\nSomewhat badThere are unusually many reads that are shorter than the aimed-for, 150 bp read length. Though the large majority of reads are still 150 bp.\n\n\n\n\n\n\nOther FastQC modules\nAnother module that can help to check the overall quality of your reads is Per base N content: its line graph should typically not visibly rise above 0%.\nThe remaining modules are not that useful for metabarcoding data:\n\nAdapter content3\nSequence duplication levels4\nOverrepresented sequences5\nPer sequence GC content6\n\n\n\n\n\n2.2 Checking your FastQC results\nFirst, you’ll unfortunately have to download FastQC’s output HTML files to your computer to be able to view them:\n\nFind the FastQC HTML files in the file explorer in the VS Code side bar.\nRight-click on one of them, click Download... and follow the prompt to download the file somewhere to your computer (doesn’t matter where).\nRepeat this for the second file.\nThen, open your computer’s file browser, find the downloaded files, and double-click on one. It should be opened in your default web browser.\n\nAlternatively, you can take a look at the FastQC output files on this website: R1 and R2.\n\n Exercise: Interpreting your FastQC results\n\nOpen the HTML file for the R1 FASTQ file and go through the modules we discussed above. Can you make sense of it? Does the data look good to you, overall?\nNow open the HTML file for the R2 FASTQ file and take a look just at the quality scores. Does it look any worse than the R1?"
  },
  {
    "objectID": "metabar/01_qc-trim.html#running-fastqc-for-all-samples",
    "href": "metabar/01_qc-trim.html#running-fastqc-for-all-samples",
    "title": "Read QC and primer trimming",
    "section": "3 Running FastQC for all samples",
    "text": "3 Running FastQC for all samples\nIf we want to run FastQC for all samples, it will be much better to write a shell script and submit that as a so-called Slurm batch job, rather than running FastQC “interactively” like we did for the first sample.\nThis is especially true for a complete data set, which would have much larger FASTQ files and possibly more samples.\n\n3.1 The components of our script\nHere is all the code we needed for out first FastQC run:\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Create the output dir\nmkdir -p results/fastqc\n\n# Run FastQC\nfastqc --outdir results/fastqc data/fastq/NW102AB_R1.fastq.gz\n\nWe will need that same code in the script, except that we will need to modify our call to fastqc — we will loop over all FASTQ files as follows:\n# Run FastQC (replacement for fastqc line above)\nfor fastq_file in data/fastq/*fastq.gz; do\n    fastqc --outdir results/fastqc \"$fastq_file\"\ndone\n\nWe are looping over all FASTQ files with the globbing pattern data/fastq/*fastq.gz. The loop will run as many times as we have FASTQ files.\nIn every iteration of the loop, the \"$fastq_file\" variable will contain 1 FASTQ file name, and we will run fastqc for that file7.\n\n\nWe will be submitting this script as a batch job to the Slurm compute job scheduler. To do so, we should also add some lines at the top of the script:\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-fastqc.out\n\nThe first line #!/bin/bash merely indicates that this is a shell script8 rather than, say, an R or Python script.\nThe lines starting with #SBATCH tell Slurm some details about our compute job request (much like we did when we filled out the form to start a VS Code session):\n\nWe always need to specify an “account”, i.e. OSC project, that should be billed.\nThe only other option (of many possible!) we will use here is to specify the output file: this is where any output will go that would otherwise be printed to screen, such as the FastQC progress output we saw above.\n\n\n\nWe will also add the following line to change some shell script settings, which will cause the script to stop running if any errors occur:\n# Strict bash settings\nset -euo pipefail\n\n\n\n3.2 Our final script\n\nOpen a new file in VS Code: click , then File, then New File.\nSave the file (e.g. press Ctrl/⌘+S) in your scripts directory as fastqc.sh.\nPaste the following code in the script:\n\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-fastqc.out\n\n# Strict bash settings\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Create the output dir\nmkdir -p results/fastqc\n\n# Run FastQC for all FASTQ files\nfor fastq_file in data/fastq/*fastq.gz; do\n    fastqc --outdir results/fastqc \"$fastq_file\"\ndone\n\n# Report\necho \"Done with script fastqc.sh\"\ndate\n\n\n\n3.3 Submitting the script\nSubmit the script to Slurm (“submit it to the queue”) with the sbatch command:\nsbatch scripts/fastqc.sh\nSubmitted batch job 27047185\nAfter some seconds (sometimes up to a minute or so9), the Slurm job should start and create the output file that we specified at the top of the script: slurm-fastqc.out.\n\n\n\n3.4 Checking the output\nIn our earlier two FastQC runs, FastQC logging output (“10% complete”, etc.) was printed to screen. But because this job now runs remotely on another compute node, such output will end up in the “Slurm log file” whose name we specified in the script. Let’s take a look:\nless slurm-fastqc.out\napplication/gzip\nStarted analysis of NW102AB_R1.fastq.gz\nApprox 5% complete for NW102AB_R1.fastq.gz\nApprox 15% complete for NW102AB_R1.fastq.gz\nApprox 20% complete for NW102AB_R1.fastq.gz\nApprox 30% complete for NW102AB_R1.fastq.gz\nApprox 35% complete for NW102AB_R1.fastq.gz\nApprox 45% complete for NW102AB_R1.fastq.gz\nApprox 50% complete for NW102AB_R1.fastq.gz\nApprox 60% complete for NW102AB_R1.fastq.gz\nApprox 70% complete for NW102AB_R1.fastq.gz\nApprox 75% complete for NW102AB_R1.fastq.gz\nApprox 85% complete for NW102AB_R1.fastq.gz\nApprox 90% complete for NW102AB_R1.fastq.gz\nAnalysis complete for NW102AB_R1.fastq.gz\napplication/gzip\nStarted analysis of NW102AB_R2.fastq.gz\nApprox 5% complete for NW102AB_R2.fastq.gz\nApprox 15% complete for NW102AB_R2.fastq.gz\nApprox 20% complete for NW102AB_R2.fastq.gz\n#[...output truncated...]\nThat looks good, in the output I printed above we can see that FastQC ran to completion for one FASTQ file and then started a second — and this will go on and on, for all of our 64 FASTQ files.\nYou will know that the job has successfully finished when the last few lines of the Slurm log file read “Done with script fastqc.sh” and print the date and time (as per the last lines of our script!):\ntail slurm-fastqc.out\nApprox 75% complete for W404BC_R2.fastq.gz\nApprox 85% complete for W404BC_R2.fastq.gz\nApprox 90% complete for W404BC_R2.fastq.gz\nApprox 95% complete for W404BC_R2.fastq.gz\nAnalysis complete for W404BC_R2.fastq.gz\nDone with script fastqc.sh\nWed Mar  6 13:34:10 EST 2024\n\nOf course, we should also check the main output files — the HTMLs and zip files:\nls results/fastqc\nNW102AB_R1_fastqc.html  NW103C_R1_fastqc.zip    NW203A_R2_fastqc.html   NW304BC_R2_fastqc.zip   NW403BC_R1_fastqc.html  W101AB_R1_fastqc.zip   W103C_R2_fastqc.html   W205A_R2_fastqc.zip    W304AB_R1_fastqc.html  W403C_R1_fastqc.zip\nNW102AB_R1_fastqc.zip   NW103C_R2_fastqc.html   NW203A_R2_fastqc.zip    NW305AB_R1_fastqc.html  NW403BC_R1_fastqc.zip   W101AB_R2_fastqc.html  W103C_R2_fastqc.zip    W205BC_R1_fastqc.html  W304AB_R1_fastqc.zip   W403C_R2_fastqc.html\nNW102AB_R2_fastqc.html  NW103C_R2_fastqc.zip    NW203BC_R1_fastqc.html  NW305AB_R1_fastqc.zip   NW403BC_R2_fastqc.html  W101AB_R2_fastqc.zip   W204A_R1_fastqc.html   W205BC_R1_fastqc.zip   W304AB_R2_fastqc.html  W403C_R2_fastqc.zip\nNW102AB_R2_fastqc.zip   NW201AB_R1_fastqc.html  NW203BC_R1_fastqc.zip   NW305AB_R2_fastqc.html  NW403BC_R2_fastqc.zip   W101C_R1_fastqc.html   W204A_R1_fastqc.zip    W205BC_R2_fastqc.html  W304AB_R2_fastqc.zip   W404A_R1_fastqc.html\nNW102C_R1_fastqc.html   NW201AB_R1_fastqc.zip   NW203BC_R2_fastqc.html  NW305AB_R2_fastqc.zip   NW404A_R1_fastqc.html   W101C_R1_fastqc.zip    W204A_R2_fastqc.html   W205BC_R2_fastqc.zip   W304C_R1_fastqc.html   W404A_R1_fastqc.zip\nNW102C_R1_fastqc.zip    NW201AB_R2_fastqc.html  NW203BC_R2_fastqc.zip   NW305C_R1_fastqc.html   NW404A_R1_fastqc.zip    W101C_R2_fastqc.html   W204A_R2_fastqc.zip    W303AB_R1_fastqc.html  W304C_R1_fastqc.zip    W404A_R2_fastqc.html\nNW102C_R2_fastqc.html   NW201AB_R2_fastqc.zip   NW304A_R1_fastqc.html   NW305C_R1_fastqc.zip    NW404A_R2_fastqc.html   W101C_R2_fastqc.zip    W204BC_R1_fastqc.html  W303AB_R1_fastqc.zip   W304C_R2_fastqc.html   W404A_R2_fastqc.zip\nNW102C_R2_fastqc.zip    NW201C_R1_fastqc.html   NW304A_R1_fastqc.zip    NW305C_R2_fastqc.html   NW404A_R2_fastqc.zip    W103AB_R1_fastqc.html  W204BC_R1_fastqc.zip   W303AB_R2_fastqc.html  W304C_R2_fastqc.zip    W404BC_R1_fastqc.html\nNW103AB_R1_fastqc.html  NW201C_R1_fastqc.zip    NW304A_R2_fastqc.html   NW305C_R2_fastqc.zip    NW404BC_R1_fastqc.html  W103AB_R1_fastqc.zip   W204BC_R2_fastqc.html  W303AB_R2_fastqc.zip   W403AB_R1_fastqc.html  W404BC_R1_fastqc.zip\nNW103AB_R1_fastqc.zip   NW201C_R2_fastqc.html   NW304A_R2_fastqc.zip    NW403A_R1_fastqc.html   NW404BC_R1_fastqc.zip   W103AB_R2_fastqc.html  W204BC_R2_fastqc.zip   W303C_R1_fastqc.html   W403AB_R1_fastqc.zip   W404BC_R2_fastqc.html\nNW103AB_R2_fastqc.html  NW201C_R2_fastqc.zip    NW304BC_R1_fastqc.html  NW403A_R1_fastqc.zip    NW404BC_R2_fastqc.html  W103AB_R2_fastqc.zip   W205A_R1_fastqc.html   W303C_R1_fastqc.zip    W403AB_R2_fastqc.html  W404BC_R2_fastqc.zip\nNW103AB_R2_fastqc.zip   NW203A_R1_fastqc.html   NW304BC_R1_fastqc.zip   NW403A_R2_fastqc.html   NW404BC_R2_fastqc.zip   W103C_R1_fastqc.html   W205A_R1_fastqc.zip    W303C_R2_fastqc.html   W403AB_R2_fastqc.zip\nNW103C_R1_fastqc.html   NW203A_R1_fastqc.zip    NW304BC_R2_fastqc.html  NW403A_R2_fastqc.zip    W101AB_R1_fastqc.html   W103C_R1_fastqc.zip    W205A_R2_fastqc.html   W303C_R2_fastqc.zip    W403C_R1_fastqc.html\nThat’s a lot of files! Do we need to check all of them? Luckily not, thanks to MultiQC."
  },
  {
    "objectID": "metabar/01_qc-trim.html#summarizing-qc-results-with-multiqc",
    "href": "metabar/01_qc-trim.html#summarizing-qc-results-with-multiqc",
    "title": "Read QC and primer trimming",
    "section": "4 Summarizing QC results with MultiQC",
    "text": "4 Summarizing QC results with MultiQC\nHere are some challenges you may run into after running FastQC:\n\nWhen you have many FASTQ files, you’ll generate a lot of FastQC HTML files to sort through (as we did above).\nEven if you diligently go through each file, it’s not that easy to compare the results across samples in detail, since they are not drawn in the same graphs.\n\nMultiQC addresses these problems by aggregating FastQC results from many files, and summarizing them into a single HTML file with (still) one graph per FastQC module.\n\n\n\n\n\n\nNot just for FastQC results! MultiQC can recognize and process the output of dozens of bioinformatics tools.\n\n\n\n\n\n\nMultiQC’s graphs are also interactive, but here is a static example of a graph showing the mean base quality scores along the read for many FASTQ files:\n\n\n\n\n\n\n\n\n Above, what could the two “groups of lines”, which diverge towards the right-hand side, represent?\n\nThese are the files with forward (top lines, better quality) and reverse (bottom lines, worse quality) reads.\n\n\nMultiQC will also create a graph comparing the number of reads across files, which can be quite useful:\n\n\n\n\n\n\n\n4.1 Running MultiQC\nWe will only need to run MultiQC once (because it will aggregate all FastQC results at once), and that will only take a few seconds — therefore, we can run the command interactively without using a script.\n\n\n\n\n\n\nSide note: Checking the MultiQC help (Click to expand)\n\n\n\n\n\nWe can check MultiQC’s help with the --help option:\nmultiqc --help\n# (Only the top part of the output is shown in the screenshot below)\n\n\n\n\n\nAs the first couple of help lines in the paler gray color explain, MultiQC will search the [ANALYSIS DIRECTORY], a dir that we pass to it as an argument at the end of the command line.\n\n\n\nIf we tell MultiQC (command multiqc) about the results/fastqc directory like so, it should find and then aggregate all the FastQC results in there:\n# (Don't run this - we'll complete the command in a second)\nmultiqc results/fastqc\n\n\n\n\n\n\nNeed to active the Conda environment?\n\n\n\nIf you don’t/no longer have the mbar24 Conda environment active, (re-)activate it as follows:\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n\nThe default output directory of MultiQC is the current working directory, so just like with FastQC, we do want to use the option for the output dir — this is our final command and you can go ahead and execute it:\n# Run MultiQC to summarize the FastQC results\nmultiqc --outdir results/multiqc results/fastqc\n\n\n\n\n\n\n\n\n4.2 MultiQC output\nOnce its done, you should have the following files in the output dir:\nls -lh results/multiqc\ntotal 1.7M\ndrwxr-xr-x 2 jelmer PAS2250 4.0K Mar  13 14:57 multiqc_data\n-rw-r--r-- 1 jelmer PAS2250 1.7M Mar  13 14:57 multiqc_report.html\nGo ahead and find the multiqc_report.html listed above in VS Code’s file browser, right-click on it and download it to your computer. Then, click on the file in your own computer to open it in your browser (i.e., just like we did with the FastQC output).\nYou can also find a copy of the MultiQC HTML output file here.\n\n Exercise: Explore the MultiQC results\nCheck for example whether patterns are consistent across samples, or if there are any outliers."
  },
  {
    "objectID": "metabar/01_qc-trim.html#cutadapt",
    "href": "metabar/01_qc-trim.html#cutadapt",
    "title": "Read QC and primer trimming",
    "section": "5 Cutadapt",
    "text": "5 Cutadapt\nWhen you prepare samples for amplicon metabarcoding, you amplify a specific region with primers, and these primers will be included in the sequences that you receive. Before we go any further, we need to remove these primer sequences, which we can do with the program Cutadapt.\nWe will write a script with a loop to run Cutadapt for all samples and submit it as a batch job like we did with FastQC.\n\n5.1 Primer sequences\nWhen we run Cutadapt, we need to tell it about our primer sequences as well as their reverse complements. We’ll start by storing our particular primer sequences in variables:\n# Primer sequences\nprimer_f=GTGTGYCAGCMGCCGCGGTAA\nprimer_r=GGACTACNVGGGTWTCTAAT\n\n\n\n\n\n\nAmbiguity codes\n\n\n\nNote that the Y, M, N, V and W in the primer sequences are so-called “ambiguity codes” that represent multiple possible bases. For example, a Y represents a C or a T, and an N represents any of the 4 bases.\n\n\nThere are many ways of getting the reverse complement of a sequence, including manually building it up, but here we’ll use a trick with the tr command to change each base into its complement, followed by the rev command to get the reverse complement — for example, for the forward primer:\necho \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev\nTTACCGCGGCKGCTGRCACAC\nBelow, we’ll get the reverse complement for both primers, and will store those in a variable as well using the construct variable=$(command)10:\n# Get the reverse-complements of the primers\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\n\n# Check the sequences\necho \"$primer_f_rc\"\necho \"$primer_r_rc\"\nTTACCGCGGCKGCTGRCACAC\nATTAGAWACCCBNGTAGTCC\n\n\n\n\n\n\nMore about the above tr | rev command (Click to expand)\n\n\n\n\n\n\ntr A T changes every A to a T\ntr ATC TAG changes every A to a T (first character in each of the two sequences), every T to an a (second character in each of the two sequences), and every C to a G (third character in each of the two sequences).\nTherefore, in the full tr command above, we list all possible bases and ambiguity codes, and then change them to their complement.\nThe rev command simply reverses a sequence, so that we end up with the reverse complement.\n\necho ACCT | rev\nTCCA\n\n\n\n\n\n\n\n5.2 Building the Cutadapt command\nFirst, here is how we can tell Cutadapt about the primer sequences:\n\nWith the -a and -A option we specify the primer sequences in the forward and reverse reads, respectively.\nThe forward reads should contain the forward primer at the beginning of the read. Because reads are sometimes longer than the amplicon length, the reverse primer may be present at the end of the read, but as its reverse complement. We specify this using -a \"$primer_f\"...\"$primer_r_rc\".\nSimilarly, the reverse reads should contain the reverse primer at the beginning of the read, and may contain the reverse complement of the forward primer at the end of the read, which we specify using -A \"$primer_r\"...\"$primer_f_rc\".\n\nAll in all, our primer specification looks like this:\n# (Don't run this - this will be part of our script)\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\"\n\n\n\n\n\n\nSpreading commands across multiple lines with \\\n\n\n\nAbove, I spread the command across multiple lines, which makes it a little easier to read. You can run the command exactly like that: the backslashes (\\) at the end of all except the last line tell the shell that our command will continue on the next line.\n\n\nWe will also:\n\nTell Cutadapt to only keep sequences that contain the primer11, with the --trimmed-only option.\nInstruct Cutadapt to use 8 “cores” with --cores 8, which can speed up the run by up to 8-fold. For our small FASTQ files, this isn’t really necessary, but for a larger dataset, that can save quite some time.\n\n# (Don't run this - this will be part of our script)\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\" \\\n    --trimmed-only \\\n    --cores 8\nFinally, let’s also add the output files (--output for R1 and --paired-output for R2) and the input files (as positional arguments at the end of the command) for a single example sample. With that, we have a final example command of running Cutadapt for a single sample:\n# (Don't run this - this will be part of our script)\ncutadapt \\\n    -a \"$primer_f\"...\"$primer_r_rc\" \\\n    -A \"$primer_r\"...\"$primer_f_rc\" \\\n    --trimmed-only \\\n    --cores 8 \\\n    --output results/cutadapt/NW102AB_R1.fastq.gz \\\n    --paired-output results/cutadapt/NW102AB_R2.fastq.gz \\\n    data/fastq/NW102AB_R1.fastq.gz \\\n    data/fastq/NW102AB_R2.fastq.gz\n\n\n\n5.3 Our Cutadapt loop\nIn our script, we will run Cutadapt inside a loop, similar to how we ran FastQC. However, this case is a bit more complicated, because we need to run Cutadapt for one sample and therefore two FASTQ files at a time, rather than for one FASTQ file at a time.\nWe will do that by looping over the R1 (forward read) files only, and inside the loop, inferring the name of the R2 file:\n# (Don't run this - this will be part of our script)\n\n# Loop over the R1 files\nfor R1_in in data/fastq/*R1.fastq.gz; do\n    # Get the R2 file name with \"parameter expansion\"\n    # This does a search-and-replace: replace \"_R1\" with \"_R2\"\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    \n    # Define the output files\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Run Cutadapt\n    cutadapt \\\n        -a \"$primer_f\"...\"$primer_r_rc\" \\\n        -A \"$primer_r\"...\"$primer_f_rc\" \\\n        --trimmed-only \\\n        --cores 8 \\\n        --output \"$R1_out\" \\\n        --paired-output \"$R2_out\" \\\n        \"$R1_in\" \"$R2_in\"\ndone\n\n\n\n5.4 The final script\n Open a new text file and save it as scripts/cutadapt.sh. Then paste the following code into the script:\n#!/bin/bash\n\n#SBATCH --account=PAS2714\n#SBATCH --output=slurm-cutadapt.out\n#SBATCH --cpus-per-task=8\n\n# Strict bash settings\nset -euo pipefail\n\n# Load the software\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/mbar24\n\n# Primer sequences\nprimer_f=GTGTGYCAGCMGCCGCGGTAA\nprimer_r=GGACTACNVGGGTWTCTAAT\n\n# Get the reverse-complements of the primers\nprimer_f_rc=$(echo \"$primer_f\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\nprimer_r_rc=$(echo \"$primer_r\" | tr ATCGYRKMBDHV TAGCRYMKVHDB | rev)\n\n# Create the output dir\noutdir=results/cutadapt\nmkdir -p \"$outdir\"\n\n# Loop over the R1 files\nfor R1_in in data/fastq/*R1.fastq.gz; do\n    # Get the R2 file name\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    \n    # Define the output files\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Run Cutadapt\n    cutadapt \\\n            -a \"$primer_f\"...\"$primer_r_rc\" \\\n            -A \"$primer_r\"...\"$primer_f_rc\" \\\n            --trimmed-only \\\n            --cores 8 \\\n            --output \"$R1_out\" \\\n            --paired-output \"$R2_out\" \\\n            \"$R1_in\" \"$R2_in\"\ndone\n\n# Report\necho \"Done with script cutadapt.sh\"\ndate\nNow we submit the script as a batch job in the same way we did with the FastQC script:\nsbatch scripts/cutadapt.sh\nSubmitted batch job 27047247\n\n\n\n5.5 Check the output\nOnce we see the “Done with script” line when we use tail on the Slurm log file, we know the job has finished:\ntail slurm-cutadapt.out\n22      4       0.0     2       0 3 1\n24      1       0.0     2       0 1\n25      1       0.0     2       0 0 1\n34      1       0.0     2       0 0 1\n38      2       0.0     2       0 2\n42      1       0.0     2       0 0 1\n44      1       0.0     2       0 1\n47      1       0.0     2       0 1\nDone with script cutadapt.sh\nWed Mar  6 14:46:56 EST 2024\nLet’s use less to take a closer look at the logging output:\nless slurm-cutadapt.out\nInput files: data/fastq/NW102AB_R1.fastq.gz data/fastq/NW102AB_R2.fastq.gz\nThis is cutadapt 4.6 with Python 3.10.13\nCommand line parameters: -a GTGTGYCAGCMGCCGCGGTAA...ATTAGAWACCCBNGTAGTCC -A GGACTACNVGGGTWTCTAAT...TTACCGCGGCKGCTGRCACAC --trimmed-only --cores 8 --output results/cutadapt/NW102AB_R1.fastq.gz --paired-output results/cutadapt/NW102AB_R2.fast\nq.gz data/fastq/NW102AB_R1.fastq.gz data/fastq/NW102AB_R2.fastq.gz\nProcessing paired-end reads on 8 cores ...\nFinished in 0.875 s (68.148 µs/read; 0.88 M reads/minute).\n\n=== Summary ===\n\nTotal read pairs processed:             12,844\n  Read 1 with adapter:                  12,798 (99.6%)\n  Read 2 with adapter:                  12,541 (97.6%)\n\n== Read fate breakdown ==\nPairs discarded as untrimmed:              337 (2.6%)\nPairs written (passing filters):        12,507 (97.4%)\n# [...output truncated...]\nWe can see that Cutadapt reports the numbers and percentages of reads that contained what it calls the “adapter”: in our case, that’s the primer. In the example above, and that is typical, the percentages are in the upper 90s.\n\n\n\n\n\n\nIf you see much lower percentages here, then something is wrong, e.g. with the primer sequences you provided or the Cutadapt syntax you used.\n\n\n\n\n\n\nWe will use the grep command to print all lines that contain the information on numbers and percentages of reads with the primer sequences:\ngrep \"with adapter:\" slurm-cutadapt.out\n  Read 1 with adapter:                  12,798 (99.6%)\n  Read 2 with adapter:                  12,541 (97.6%)\n  Read 1 with adapter:                  14,499 (99.7%)\n  Read 2 with adapter:                  14,211 (97.7%)\n  Read 1 with adapter:                  12,174 (99.7%)\n  Read 2 with adapter:                  11,835 (97.0%)\n  Read 1 with adapter:                  15,054 (99.7%)\n  Read 2 with adapter:                  14,737 (97.6%)\n# [...output truncated...]\nYou should always take a careful look at this output, to check if there are no samples with much lower percentages: it looks like there are no such samples in this case, fortunately.\n\n\n\n\n\n\nWant to quickly see the lowest % of reads with adapter? (Click to expand)\n\n\n\n\n\nUse this command to extract and sort the percentages:\ngrep \"with adapter:\" slurm-cutadapt.out | cut -d\"(\" -f2 | sort -n | head\n97.0%)\n97.3%)\n97.5%)\n97.5%)\n97.5%)\n97.5%)\n97.6%)\n\n\n\n\n\n\n\n\n\nNeed to identify the samples with a specific percentage? (Click to expand)\n\n\n\n\n\nUse this command to print the 7 lines preceding each match, so you can see the file names (samples) that correspond to each percentage of reads with primers — in the box below, you’ll have to scroll all the way to the right to see those file names.\n(An alternative way, of course, would just be to scroll through the entire output file, but this is (even) more of a hassle, as Cutadapt has quite some output.)\ngrep -B7 \"with adapter:\" results/cutadapt/logs/slurm-cutadapt.out\nCommand line parameters: -a GTGTGYCAGCMGCCGCGGTAA...ATTAGAWACCCBNGTAGTCC -A GGACTACNVGGGTWTCTAAT...TTACCGCGGCKGCTGRCACAC --trimmed-only --cores 8 --output results/cutadapt/NW102AB_R1.fastq.gz --paired-output results/cutadapt/NW102AB_R2.fastq.gz data/fastq/NW102AB_R1.fastq.gz data/fastq/NW102AB_R2.fastq.gz\nProcessing paired-end reads on 8 cores ...\nFinished in 0.875 s (68.148 µs/read; 0.88 M reads/minute).\n\n=== Summary ===\n\nTotal read pairs processed:             12,844\n  Read 1 with adapter:                  12,798 (99.6%)\n  Read 2 with adapter:                  12,541 (97.6%)\n--\nCommand line parameters: -a GTGTGYCAGCMGCCGCGGTAA...ATTAGAWACCCBNGTAGTCC -A GGACTACNVGGGTWTCTAAT...TTACCGCGGCKGCTGRCACAC --trimmed-only --cores 8 --output results/cutadapt/NW102C_R1.fastq.gz --paired-output results/cutadapt/NW102C_R2.fastq.gz data/fastq/NW102C_R1.fastq.gz data/fastq/NW102C_R2.fastq.gz\nProcessing paired-end reads on 8 cores ...\nFinished in 0.801 s (55.030 µs/read; 1.09 M reads/minute).\n\n=== Summary ===\n\nTotal read pairs processed:             14,549\n  Read 1 with adapter:                  14,499 (99.7%)\n  Read 2 with adapter:                  14,211 (97.7%)\n--\n# [...output truncated...]\n\n\n\n\nFinally, let’s check the output dir, which contains the trimmed FASTQ files we’ll use in the next step of the workflow:\nls results/cutadapt\nNW102AB_R1.fastq.gz  NW103C_R1.fastq.gz   NW203A_R1.fastq.gz   NW304BC_R1.fastq.gz  NW403A_R1.fastq.gz   NW404BC_R1.fastq.gz  W103AB_R1.fastq.gz  W204BC_R1.fastq.gz  W303AB_R1.fastq.gz  W304C_R1.fastq.gz   W404A_R1.fastq.gz\nNW102AB_R2.fastq.gz  NW103C_R2.fastq.gz   NW203A_R2.fastq.gz   NW304BC_R2.fastq.gz  NW403A_R2.fastq.gz   NW404BC_R2.fastq.gz  W103AB_R2.fastq.gz  W204BC_R2.fastq.gz  W303AB_R2.fastq.gz  W304C_R2.fastq.gz   W404A_R2.fastq.gz\nNW102C_R1.fastq.gz   NW201AB_R1.fastq.gz  NW203BC_R1.fastq.gz  NW305AB_R1.fastq.gz  NW403BC_R1.fastq.gz  W101AB_R1.fastq.gz   W103C_R1.fastq.gz   W205A_R1.fastq.gz   W303C_R1.fastq.gz   W403AB_R1.fastq.gz  W404BC_R1.fastq.gz\nNW102C_R2.fastq.gz   NW201AB_R2.fastq.gz  NW203BC_R2.fastq.gz  NW305AB_R2.fastq.gz  NW403BC_R2.fastq.gz  W101AB_R2.fastq.gz   W103C_R2.fastq.gz   W205A_R2.fastq.gz   W303C_R2.fastq.gz   W403AB_R2.fastq.gz  W404BC_R2.fastq.gz\nNW103AB_R1.fastq.gz  NW201C_R1.fastq.gz   NW304A_R1.fastq.gz   NW305C_R1.fastq.gz   NW404A_R1.fastq.gz   W101C_R1.fastq.gz    W204A_R1.fastq.gz   W205BC_R1.fastq.gz  W304AB_R1.fastq.gz  W403C_R1.fastq.gz\nNW103AB_R2.fastq.gz  NW201C_R2.fastq.gz   NW304A_R2.fastq.gz   NW305C_R2.fastq.gz   NW404A_R2.fastq.gz   W101C_R2.fastq.gz    W204A_R2.fastq.gz   W205BC_R2.fastq.gz  W304AB_R2.fastq.gz  W403C_R2.fastq.gz\nls -lh results/cutadapt\n-rw-rw----+ 1 jelmer PAS0471 1.9M Mar  6 14:46 NW102AB_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.4M Mar  6 14:46 NW102AB_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.2M Mar  6 14:46 NW102C_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.8M Mar  6 14:46 NW102C_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 1.8M Mar  6 14:46 NW103AB_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.4M Mar  6 14:46 NW103AB_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.2M Mar  6 14:46 NW103C_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 2.9M Mar  6 14:46 NW103C_R2.fastq.gz\n# [...output truncated...]"
  },
  {
    "objectID": "metabar/01_qc-trim.html#bonus-content",
    "href": "metabar/01_qc-trim.html#bonus-content",
    "title": "Read QC and primer trimming",
    "section": "6 Bonus content",
    "text": "6 Bonus content\n\n6.1 The FASTQ format\nFASTQ is a very common output format of high-throughput sequencing machines. Like most genomic data files, these are plain text files. Each sequence that is read by the sequencer (i.e., each “read”) forms one FASTQ entry represented by four lines. The lines contain, respectively:\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe sequence itself\nA + (plus sign)\nOne-character quality scores for each base in the sequence\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than what is typical.\n\n\nThe “Q” in FASTQ stands for “quality”, to contrast this format with FASTA, a more basic and generic sequence data format that does not include base quality scores. FASTQ files have the extension .fastq or .fq, but they are very commonly gzip-compressed, in which case their name ends in .fastq.gz or .fq.gz.\n\n\n\n\n\n\nFASTQ quality scores (Click to expand)\n\n\n\n\n\nThe quality scores we saw in the read above represent an estimate of the error probability of the base call. Specifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\nFor some specific probabilities and their rough qualitative interpretations for Illumina data:\n\n\n\n\n\n\n\n\n\nPhred quality score\nError probability\nRough interpretation\nASCII character\n\n\n\n\n10\n1 in 10\nterrible\n+\n\n\n20\n1 in 100\nbad\n5\n\n\n30\n1 in 1,000\ngood\n?\n\n\n40\n1 in 10,000\nexcellent\n?\n\n\n\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character” (last column in the table). This allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read. (For your reference, here is a complete lookup table — look at the top table, “BASE=33”).\n\n\n\n\n\n\n6.2 Viewing FASTQ files\nNext, we’ll take a peak inside one of these FASTQ files.\nThe head command prints the first lines of a file. Let’s use it try to and print 8 lines, which should show us two reads:\nhead -n 8 data/fastq/NW102AB_R1.fastq.gz\n�\nԽے�8�E��_1f�\"�QD�J��D�fs{����Yk����d��*��\n|��x���l޴�j�N������?������ٔ�bUs�Ng�Ǭ���i;_��������������|&lt;�v����3��������|���ۧ��3ĐHyƕ�bIΟD�%����Sr#~��7��ν��1y�Ai,4\nw\\]\"b�#Q����8��+[e�3d�4H���̒�l�9LVMX��U*�M����_?���\\[\"��7�s\\&lt;_���:�$���N��v�}^����sw�|�n;&lt;�&lt;�oP����\ni��k��q�ְ(G�ϫ��L�^��=��&lt;���K��j�_/�[ۭV�ns:��U��G�z�ݎ�j����&��~�F��٤ZN�'��r2z}�f\\#��:�9$�����H�݂�\"�@M����H�C�\n�0�pp���1�O��I�H�P됄�.Ȣe��Q�&gt;���\n�'�;@D8���#��St�7k�g��|�A䉻���_���d�_c������a\\�|�_�mn�]�9N������l�٢ZN�c�9u�����n��n�`��\n\"gͺ�\n    ���H�?2@�FC�S$n���Ԓh�       nԙj��望��f      �?N@�CzUlT�&�h�Pt!�r|��9~)���e�A�77�h{��~��     ��\n# [...output truncated...]\n\n\nOuch! 😳 What went wrong here? (Click for the solution)\n\nWhat happened here is that we are directly seeing the contents of the compressed file, which is simply not human-readable.\n\n\n\n\n\n\n\n\nNo need to decompress\n\n\n\nTo get around the problem we just encountered with head, we might be inclined to uncompress these files, which we could do with the gunzip command. However, uncompressed files take up several times as much disk storage space as compressed ones. Fortunately, we don’t need to decompress them:\n\nAlmost any bioinformatics tool will accept compressed FASTQ files.\nWe can still view these files in compressed form, as shown below.\n\n\n\nInstead, we’ll use the less command, which will automatically display gzip-compressed files in human-readable form:\nless -S data/fastq/NW102AB_R1.fastq.gz\n@M02815:77:000000000-KPK85:1:2101:3678:10660 1:N:0:CCTAAGAC+TTCTAGCT\nCGAGCAATCCACTCGAGTGCCAGCAGCCGCAGTAATACGGAGGGTGCGAGCGTTGTCCGGAATCACTGGGCGTAAAGGGCGCGTAGGCGGCGCGGATAGTCGGCGGTGAAAGCCCGGAGCTCAACTCCGGGTCGGCCGTCGATACTTCCGGGCTTGAGCACTGTAGAGGCAGATGGAATTCCGGGTGTAGCGGTGGAATGCGTAGAGATCCGGAAGAACACCGGTGGCGAAGGCGGTCTGCTGGGCAGTTGCTGACGCTGATGCGCGACAGCGTGGGGAGCAAACAGGATTAGATACC\n+\nCCCCCGGGGGGGGGGGGGGFGGGGGGGGGG+CFGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGGGGGGGGGGGGGGGGFGGGGGGGGGGGGGGGGEGGGGGGGGGGGGGGGGGGGGDGGGGGGGGGGGGGGFGGFGFFFFEBFFGFFFDGFGFGBFGFGFGFFFF6?FFFGBF?FBFFF\n@M02815:77:000000000-KPK85:1:2108:2535:14400 1:N:0:CCTAAGAC+TTCTAGCT\nCGAGCAATCCACTCGAGTGTCAGCCGCCGCGGTAATACAGAGGTCCCGAGCGTTGTTCGGATTCATTGGGCGTAAAGGGTGCGTAGGCGGCGGGGAAAGTCTGATGTGAAATCCTGGGGCTCAACCCTGGAACTGCATTGGATACTTCCTTGCTAGAGTACTGGAGAGGAAACTGGAATTTACGGTGTAGCAGTGAAATGCGTAGAGATCGTAAGGAAGACCAGTGGCGAAGGCGAGTTTCTGGACAGTTACTGACGCTGAGGCACGAAGGCCAGGGGAGCAAACGGGATTAGATACC\n+\nCCCCCCGFGFGGGC-FFFGFGFFGGDFFGGGGGECGEGGAEGGGGGGGFGGDGG7CFFGGDCCFGGFCF8FGGGGGGCEGDGGGGGCGGGGGGDEGGGGBFGGDFGGGDG&lt;DFGGGGCEGGGD:FFGGGGFFGFGGFFFFGGGFGGCFGGFGGGGG9CGCGGGG7FGGC:FFGGGGGFGG&lt;?FCGGGGGGGGGGG9CG&lt;ACC?EG5CFGGGGF8CCCC:C@FGCFGGGGGC58=EEG8??77:9@:&lt;3A&gt;7AGFGGGGC?DFC?5&lt;5&gt;&gt;BGGGFGGGGG&gt;4?C42::3:DG=&gt;&lt;&lt;*)*\n\n\n\n\n\n\nless -S suppresses line-wrapping: lines in the file will not be “wrapped” across multiple lines\n\n\n\n\n\n\n\n\n\n\nExercise: Explore the file with less\nless doesn’t print stuff to screen but instead opens it in a “pager”. After running the command above, you should be viewing the file inside the less pager.\nYou can move around in the file in several ways: by scrolling with your mouse, with up and down arrows, or, if you have them, PgUp and PgDn keys (also, u will move up half a page and d down half a page).\nNotice you won’t get your shell prompt back until you press q to quit less."
  },
  {
    "objectID": "metabar/01_qc-trim.html#footnotes",
    "href": "metabar/01_qc-trim.html#footnotes",
    "title": "Read QC and primer trimming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a full list of installed software at OSC: https://www.osc.edu/resources/available_software/software_list↩︎\n Attribution: Some of the FastQC example plots were taken from here.↩︎\nChecks for adapters at the ends of reads. Since we have to remove primers anyway, any adapters past the primers will be automatically removed.↩︎\n Will throw a Fail but this is not meaningful here: metabarcoding data has many duplicate sequences by design.↩︎\nAs with the previous module, this will throw a Fail but this is not meaningful here: metabarcoding data has many duplicate sequences by design.↩︎\nUseful in a whole-genome sequencing context or to detect contamination.↩︎\nTherefore, our FastQC analysis will run sequentially (1-by-1) for each file, not in parallel.↩︎\n Using the shell language Bash, specifically↩︎\n And very large jobs can sometimes take hours to start, but our jobs are small so that should not happen.↩︎\nThis is called “command substitution”.↩︎\n This is not the default: Cutadapt is even more commonly used to remove adapters, and then this doesn’t apply↩︎"
  },
  {
    "objectID": "shell/01_shell1.html#goals-for-this-session",
    "href": "shell/01_shell1.html#goals-for-this-session",
    "title": "Intro to the Unix Shell",
    "section": "1 Goals for this session",
    "text": "1 Goals for this session\nIn this session, we’ll learn:\n\nWhy using a command-line interface can be beneficial.\nWhat the Unix shell is and what you can do with it.\nUsing the shell, learn how to:\n\nNavigate around your computer.\nCreate and manage directories and files.\nView text files.\nSearch within, manipulate, and extract information from text files."
  },
  {
    "objectID": "shell/01_shell1.html#introduction-ch.-1.1-1.2",
    "href": "shell/01_shell1.html#introduction-ch.-1.1-1.2",
    "title": "Intro to the Unix Shell",
    "section": "2 Introduction (Ch. 1.1-1.2)",
    "text": "2 Introduction (Ch. 1.1-1.2)\n\n2.1 Some terminology\n\nUnix (vs. Linux)\nWe can conceptualize Unix (or “Unix-like” / “*nix”)1 as a family of operating systems, which includes Linux and Mac but not Windows.\n\n\nUnix shell-related terms\n\nCommand Line — the most general term, an interface2 where you type commands\nTerminal — the program/app/window that can run a Unix shell\nShell — a command line interface to your computer\nUnix Shell — the types of shells on Unix family (Linux + Mac) computers\nBash — the specific Unix shell language that is most common on Unix computers\n\nWhile these are not synonyms, in day-to-day computing/bioinformatics, they are often used interchangeably.\n\n\n\n2.2 Why use the Unix shell?\n\nVersus programs with graphical user interfaces:\n\nUsing software\nBest or only option to use many programs, especially in bioinformatics.\nAutomation and fewer errors\nThe shell allows you to repeat and automate tasks easily and without introducing errors.\nReproducibility\nThe shell keeps a record of what you have done.\nWorking with large files\nShell commands are good at processing large files, which are common when working with omics data\nRemote computing – especially HPCs\nIt is often only possible to work in a terminal when doing remote computing.\n\nVersus scripting languages like Python or R:\n\nFor many simpler tasks, built-in shell tools are faster — in terms of coding time, processing time, and the ease of processing very large files.\nThe Unix shell has a direct interface to other programs."
  },
  {
    "objectID": "shell/01_shell1.html#getting-started-with-unix-ch.-1.3",
    "href": "shell/01_shell1.html#getting-started-with-unix-ch.-1.3",
    "title": "Intro to the Unix Shell",
    "section": "3 Getting started with Unix (Ch. 1.3)",
    "text": "3 Getting started with Unix (Ch. 1.3)\n\nUnix directory structure\n\n“Directory” (or “dir” for short) is the term for folder that is commonly used in Unix contexts.\nThe Unix directory structure is hierarchical, with a single starting point: the root, depicted as /.\nA “path” gives the location of a file or directory, in which directories are separated by forward slashes /.\nFor example, the path to our OSC project’s dir is /fs/ess/PAS2700. This means: the dir PAS2700 is located inside the dir ess, which in turn is inside the dir fs, which in turn is in the computer’s root directory.\nThe OSC dir structure is somewhat different from that of a personal computer. Our Home dir is not /home/&lt;username&gt; like in the book and the schematic on the left, but /users/&lt;a-project&gt;/&lt;username&gt;.\n\n\n\n\n\n\nGeneric example, from O’Neil 2017\n\n\n\n\n\n\nKey dirs in OSC dir structure"
  },
  {
    "objectID": "shell/01_shell1.html#getting-started-with-the-shell-ch.-1.4",
    "href": "shell/01_shell1.html#getting-started-with-the-shell-ch.-1.4",
    "title": "Intro to the Unix Shell",
    "section": "4 Getting started with the shell (Ch. 1.4)",
    "text": "4 Getting started with the shell (Ch. 1.4)\n\n4.1 Accessing a shell at OSC\n\nGo to https://ondemand.osc.edu in your browser, and log in.\nClick on Clusters and then Pitzer Shell Access.\n\n\n\n\n\n\n\n\nCopying and pasting in this shell\n\n\n\nYou can’t right-click in this shell, so to copy-and-paste:\n\nCopy simply by selecting text (you should see a copy () icon appear).\nPaste using Ctrl+V.\n\n Try copying and pasting a random word into your shell. This may just work, you may get a permission pop-up, or it may silently fail — if the latter, click on the clipboard icon in your browser’s address bar (see red circle in screenshot below):\n\n\n\n\n\n4.2 The prompt\nInside your terminal, the “prompt” indicates that the shell is ready for a command. What is shown exactly varies across shells and can also be customized, but our prompts at OSC should show the following information:\n&lt;username&gt;@&lt;node-name&gt; &lt;working-dir&gt;]$\nFor example (and note that ~ means your Home dir):\n[jelmer@pitzer-login02 ~]$ \nWe type our commands after the dollar sign, and then press Enter to execute the command. When the command has finished executing, we’ll get our prompt back and can type a new command.\n\n\n\n\n\n\nClearing the screen\n\n\n\nOSC prints welcome messages and storage quota information when you open a shell. To reduce the amount of text on the screen, I will clear the screen now and regularly throughout. This can be done with the keyboard shortcut Ctrl+L.\n\n\n\n\n\n4.3 A few simple commands: date and pwd\nThe Unix shell comes with hundreds of “commands”: small programs that perform specific actions. If you’re familiar with R or Python, a Unix command is like an R/Python function.\nLet’s start with a few simple commands:\n\nThe date command prints the current date and time:\ndate\nThu Feb 29 14:58:19 EST 2024\nThe pwd (Print Working Directory) command prints the path to the directory you are currently located in:\npwd\n/users/PAS0471/jelmer\n\nBoth of those commands provided us with some output. That output was printed to screen, which is the default behavior for nearly every Unix command.\n\n\n\n4.4 The cal command — and options & arguments\nThe cal command is another example of a command that simply prints some information to the screen, in this case a calendar. We’ll use it to learn about command options and arguments.\nInvoking cal without options or arguments will show the current month:\ncal\n    February 2024   \nSu Mo Tu We Th Fr Sa\n             1  2  3\n 4  5  6  7  8  9 10\n11 12 13 14 15 16 17\n18 19 20 21 22 23 24\n25 26 27 28 29\n\nOption examples\nUse the option -j (dash and then j) for a Julian calendar, in which day numbering is continuous instead of restarting each month:\n# Make sure to leave a space between `cal` and `-j`!\ncal -j\n       February 2024       \nSun Mon Tue Wed Thu Fri Sat\n                 32  33  34\n 35  36  37  38  39  40  41\n 42  43  44  45  46  47  48\n 49  50  51  52  53  54  55\n 56  57  58  59  60\nUse the -3 option to show 3 months (adding the previous and next month):\ncal -3\n    January 2024          February 2024          March 2024     \nSu Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa  Su Mo Tu We Th Fr Sa\n    1  2  3  4  5  6               1  2  3                  1  2\n 7  8  9 10 11 12 13   4  5  6  7  8  9 10   3  4  5  6  7  8  9\n14 15 16 17 18 19 20  11 12 13 14 15 16 17  10 11 12 13 14 15 16\n21 22 23 24 25 26 27  18 19 20 21 22 23 24  17 18 19 20 21 22 23\n28 29 30 31           25 26 27 28 29        24 25 26 27 28 29 30\n                                            31   \nWe can always combine multiple options, for example:\ncal -j -3\n        January 2024                February 2024                  March 2024        \nSun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat  Sun Mon Tue Wed Thu Fri Sat\n      1   2   3   4   5   6                   32  33  34                       61  62\n  7   8   9  10  11  12  13   35  36  37  38  39  40  41   63  64  65  66  67  68  69\n 14  15  16  17  18  19  20   42  43  44  45  46  47  48   70  71  72  73  74  75  76\n 21  22  23  24  25  26  27   49  50  51  52  53  54  55   77  78  79  80  81  82  83\n 28  29  30  31               56  57  58  59  60           84  85  86  87  88  89  90\n                                                           91 \nHandily, options can be “pasted together” like so (output not shown - same as above):\ncal -j3\n\n\nOptions summary\nAs we’ve seen, options are specified with a dash - (or --, as you’ll see later). So far, we’ve only worked with the type of options that are also called “flags”, which change some functionality in an ON/OFF type way:\n\nTurning a Julian calender display ON with -j\nTurning a 3-month display ON with -3.\n\nIn general terms, options change the behavior of a command.\n\n\nArguments\nWhereas options change the behavior of a command, arguments typically tell the command what to operate on. Most commonly, these are file or directory paths.\nAdmittedly, the cal command is not the best illustration of this pattern — when you give it one argument, this is supposed to be the year to show a calendar for:\ncal 2020\n                              2020                               \n\n       January               February                 March       \nSu Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa   Su Mo Tu We Th Fr Sa\n          1  2  3  4                      1    1  2  3  4  5  6  7\n 5  6  7  8  9 10 11    2  3  4  5  6  7  8    8  9 10 11 12 13 14\n12 13 14 15 16 17 18    9 10 11 12 13 14 15   15 16 17 18 19 20 21\n19 20 21 22 23 24 25   16 17 18 19 20 21 22   22 23 24 25 26 27 28\n26 27 28 29 30 31      23 24 25 26 27 28 29   29 30 31\n\n# [...output truncated, entire year is shown...]\nWe can also combine options and arguments:\ncal -j 2020\n                            2020                          \n\n          January                       February         \nSun Mon Tue Wed Thu Fri Sat   Sun Mon Tue Wed Thu Fri Sat\n              1   2   3   4                            32\n  5   6   7   8   9  10  11    33  34  35  36  37  38  39\n 12  13  14  15  16  17  18    40  41  42  43  44  45  46\n 19  20  21  22  23  24  25    47  48  49  50  51  52  53\n 26  27  28  29  30  31        54  55  56  57  58  59  60\n \n# [...output truncated, entire year is shown...]\nSo, arguments to a command:\n\nAre not preceded by a - (or --)\nIf options and arguments are combined, arguments come after options3.\n\n\n\n\n\n4.5 Getting help\nMany commands – and other command-line programs! – have a -h option for help, which usually gives a concise summary of the command’s syntax, i.e. it’s available options and arguments:\ncal -h\n\nUsage:\n cal [options] [[[day] month] year]\n\nOptions:\n -1, --one        show only current month (default)\n -3, --three      show previous, current and next month\n -s, --sunday     Sunday as first day of week\n -m, --monday     Monday as first day of week\n -j, --julian     output Julian dates\n -y, --year       show whole current year\n -V, --version    display version information and exit\n -h, --help       display this help text and exit\n\n\n\n\n\n\nAnother way to see documentation: the man command (Click to expand)\n\n\n\n\n\nAn alternative way of getting help for Unix commands is with the man command:\nman cal\nThis manual page often includes a lot more details than the --help output, and it is opened inside a “pager” rather than printed to screen: type q to exit the pager that man launches.\n\n\n\n\n Exercise: Interpreting the help output\n\nLook through the options listed when you ran cal -h, and try an option we haven’t used yet4. (You can also combine this new option with other options, if you want.)\n\n\n\nSolution\n\n# Print a calendar with Monday as the first day of th week (instead of the default, Sunday) \ncal -m\n    February 2024   \nMo Tu We Th Fr Sa Su\n          1  2  3  4\n 5  6  7  8  9 10 11\n12 13 14 15 16 17 18\n19 20 21 22 23 24 25\n26 27 28 29\n\n\nTry using one or more options in their “long form” (with --). Why would those be useful?\n\n\n\nSolution\n\nFor example:\ncal --julian --monday\n       February 2024       \nMon Tue Wed Thu Fri Sat Sun\n             32  33  34  35\n 36  37  38  39  40  41  42\n 43  44  45  46  47  48  49\n 50  51  52  53  54  55  56\n 57  58  59  60\nThe advantage of using long options is that it is much more likely that any reader of the code (including yourself next week) will immediately understand what these options are doing.\nNote that long options cannot be “pasted together” like short options.\n\n\nBonus: Try to figure out / guess what the cal [options] [[[day] month] year] means. Can you print a calendar for April 2017?\n\n\n\nSolution\n\nFirst of all, the square brackets around options and all of the possible arguments means that none of these are required — as we’ve seen, just cal (with no options or arguments) is a valid command.\nThe structure of the multiple square brackets around the day-month-year arguments indicate that:\n\nIf you provide only one argument, it should be a year\nIf you provide a second argument, it should be a month\nIf you provide a third argument, it should be a day\n\nTherefore, to print a calendar for April 2017:\ncal 4 2017\n     April 2017     \nSu Mo Tu We Th Fr Sa\n                   1\n 2  3  4  5  6  7  8\n 9 10 11 12 13 14 15\n16 17 18 19 20 21 22\n23 24 25 26 27 28 29\n30\n\n\n\n\n\n4.6 cd and command “actions”\nAll the commands so far “merely” provided some information, which was printed to screen.\nBut many commands perform another kind of action. For example, the command cd will change your working directory. And like many commands that perform a potentially invisible action, cd normally has no output at all.\nFirst, let’s check again where we are — we should be in our Home directory:\n# (Note: you will have a different project listed in your Home dir. This is not important.)\npwd\n/users/PAS0471/jelmer\nNow, let’s use cd to move to another directory by specifying the path to that directory as an argument:\ncd /fs/ess/PAS2700\n# Double-check that we moved:\npwd\n/fs/ess/PAS2700\nIn summary:\n\nLike cal, cd accepts an argument. Unlike cal, this argument takes the form of a path that the command should operate on, which is much more typical.\ncd gives no output when it successfully changed the working directory. This is very common behavior for Unix commands that perform operations: when they succeed, they are silent.\n\nLet’s also see what happens when cd does not succeed — it gives an error:\ncd /fs/ess/PAs2700\n-bash: cd: /fs/ess/PAs2700: No such file or directory\n\n\n\n\n\n\n\nGeneral shell tips\n\n\n\n\nEverything in the shell is case-sensitive, including commands and file names (hence the error above).\nYour cursor can be anywhere on a line (not just at the end) when you press Enter to execute a command!\nAny text that comes after a # is considered a comment instead of code!\n\n# This entire line is a comment - you can run it and nothing will happen\npwd    # 'pwd' will be executed but everything after the '#' is ignored\n/users/PAS0471/jelmer\n\n\n\n\n\n\n4.7 Keyboard shortcuts\nUsing keyboard shortcuts help you work much more efficiently in the shell. And some are invaluable:\n\nCancel/stop/abort — If your prompt is “missing”, the shell is still busy executing your command, or you typed an incomplete command. To abort, press Ctrl+C and you will get your prompt back.\nCommand history — Up / Down arrow keys to cycle through your command history.\nTab completion — The shell will auto-complete partial commands or file paths when you press Tab.\n\n\n Practice with Tab completion & command history\n\nType /f and press Tab (will autocomplete to /fs/)\nAdd e (/fs/e) and press Tab (will autocomplete to /fs/ess/).\nAdd PAS (/fs/ess/PAS) and press Tab. Nothing should happen: there are multiple (many!) options.\nPress Tab Tab (i.e., twice in quick succession) and it should say:\nDisplay all 503 possibilities? (y or n)\nType n to answer no: we don’t need to see all the dirs starting with PAS.\nAdd 27 (/fs/ess/PAS27) and press Tab (should autocomplete to /fs/ess/PAS2700).\nPress Enter. What does the resulting error mean?\nbash: /fs/ess/PAS2700/: Is a directory\n\n\n\nClick to see the solution\n\nBasically, everything you type in the shell should start with a command. Just typing the name of a dir or file will not make the shell print some info about or the contents of said dir or file, as you perhaps expected.\n\n\nPress ⇧ to get the previous “command” back on the prompt.\nPress Ctrl+A to move to the beginning of the line at once.\nAdd cd and a space in front of the dir, and press Enter again.\ncd /fs/ess/PAS2700/\n\n\n\n Practice with canceling\nTo simulate a long-running command that we may want to abort, we can use the sleep command, which will make the computer wait for a specified amount of time until giving your prompt back. Run the below command and instead of waiting for the full 60 seconds, press Ctrl + C to get your prompt back sooner!\nsleep 60s\nOr, use Ctrl + C after running this example of an incomplete command (an opening parenthesis ():\n(\n\n\n\n\n\n\n\nTable with useful keyboard shortcuts (Click to expand)\n\n\n\n\n\nNote that even on Macs, you should use Ctrl instead of switching them out for Cmd as you may be used to doing (in some cases, like copy/paste, both work).\n\n\n\n\n\n\n\nShortcut\nFunction\n\n\n\n\nTab\nTab completion\n\n\n⇧ / ⇩\nCycle through previously issued commands\n\n\nCtrl(+Shift)+C\nCopy selected text\n\n\nCtrl(+Shift)+V\nPaste text from clipboard\n\n\nCtrl+A / Ctrl+E\nGo to beginning/end of line\n\n\nCtrl+U /Ctrl+K\nCut from cursor to beginning / end of line\n\n\nCtrl+W\nCut word before before cursor (Only works on Macs in our shell in the browser!)\n\n\nCtrl+Y\nPaste (“yank”) text that was cut with one of the shortcuts above\n\n\nAlt+. / Esc+.\nRetrieve last argument of previous command (very useful!) (Esc+. for Mac)\n\n\nCtrl+R\nSearch history: press Ctrl+R again to cycle through matches, Enter to put command in prompt.\n\n\nCtrl+C\nCancel (kill/stop/abort) currently active command\n\n\nCtrl+D\nExit (a program or the shell, depending on the context) (same as exit command)\n\n\nCtrl+L\nClear the screen (same as clear command)\n\n\n\n\n\n\n\n\n\n4.8 Environment variables\nYou may be familiar with the concept of variables from previous experience with perhaps R or another language. Variables can hold values and other pieces of data and are essential in programming.\n\nAssigning and printing the value of a variable in R:\n\n# (Don't run this)\nx &lt;- 5\nx\n\n[1] 5\n\n\nAssigning and printing the value of a variable in the Unix shell:\nx=5\necho $x\n5\n\n\n\n\n\n\n\nIn the Unix shell code above, note that:\n\n\n\n\nThere cannot be any spaces around the = in x=5.\nYou need a $ prefix to reference (but not to assign) variables in the shell5.\nYou need the echo command, a general command to print text, to print the value of $x (cf. in R).\n\nBy the way, echo can also print literal text (as shown below) or combinations of literals and variables (next exercise):\necho \"Welcome to PLNTPTH 6193\"\nWelcome to PLNTPTH 6193\n\n\n\nEnvironment variables are pre-existing variables that have been automatically assigned values. Two examples:\n# $HOME contains the path to your Home dir:\necho $HOME\n/users/PAS0471/jelmer\n# $USER contains your user name:\necho $USER\njelmer\n\n Exercise: environment variables\nUsing an environment variable, print “Hello, my name is &lt;your username&gt;” (e.g. “Hello, my name is natalie”).\n\n\nClick to see the solution\n\n# (This would also work without the \" \" quotes)\necho \"Hello, my name is $USER\"\nHello, my name is jelmer\n\n\n\n\n\n4.9 Create your own dir & get the CSB data\nOur base OSC directory is the /fs/ess/PAS2700 dir we are currently in. Now, let’s all create our own subdir in here, and get the data from the CSB book.\n\nCreate a directory for yourself using the mkdir (make dir) command:\nmkdir users/$USER\nMove there using cd:\n# (Instead of $USER, you can also start typing your username and press Tab)\ncd users/$USER\nGet the files associated with the CSB book by “cloning” (downloading) its Git repository:\ngit clone https://github.com/CSB-book/CSB.git\nMove into the sandbox dir for the Unix chapter (remember to use tab completion):\ncd CSB/unix/sandbox\n\n\n\n\n4.10 Paths & navigation shortcuts\n\nAbsolute (full) paths versus relative paths\n\nAbsolute (full) paths (e.g. /fs/ess/PAS2700)\nPaths that begin with a / always start from the computer’s root directory, and are called “absolute paths”.\n(They are equivalent to GPS coordinates for a geographical location, as they work regardless of where you are).\nRelative paths (e.g. CSB/unix/sandbox)\nPaths that instead start from your current working directory are called “relative paths”.\n(These work like directions along the lines of “take the second left:” they depend on your current location.)\n\nIn this tutorial, you can find out talk more about the distinction between absolute and relative paths, and their respective merits.\n\n\nPath shortcuts\n\n~ (a tilde) — represents your Home directory. For example, cd ~ moves you to your Home dir.\n. (a single period) — represents the current working directory (we’ll soon see how why that can be useful).\n.. (two periods) — Represents the directory “one level up”, i.e. towards the computer’s root dir.\n\nSome examples of ..:\n\nUse .. to go up one level in the dir hierarchy:\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix/sandbox\ncd ..\npwd\n/fs/ess/PAS2700/users/jelmer/CSB/unix\nThis pattern can be continued all the way to the root of the computer, so ../.. means two levels up:\ncd ../..\npwd\n/fs/ess/PAS2700/users/jelmer\n\n\n\n\n\n\n\nThese path shortcuts work with many commands\n\n\n\nThese are general shell shortcuts that work with any command that accepts a path/file name.\n\n\n\n\n\n Exercise\n\nA) Use a relative path to move back to the /fs/ess/PAS2700/users/$USER/CSB/unix/sandbox dir.\n\n\n\n(Click for the solution)\n\ncd CSB/unix/sandbox\n\n\nB) Use a relative path (with ..) to move into the /fs/ess/PAS2700/users/$USER/CSB/unix/data dir.\n\n\n\n(Click for the solution)\n\ncd ../data\nYou may have done this in two steps, because you may not have realized that you can “add to” a path after .. like we did above. So you may have done this:\ncd ..\ncd data\nThat’s OK, but is obviously more typing.\n\n\nC) The ls command lists files and dirs, and accepts one or more paths as arguments. Use ls to list the files in your Home dir with a shortcut and without moving there.\n\n\n\n(Click for the solution)\n\n\nWith the ~ shortcut:\n\nls ~\n# (Output not shown)\n\nWith the $HOME environment variable:\n\nls $HOME\n# (Output not shown)"
  },
  {
    "objectID": "shell/01_shell1.html#basic-unix-commands-ch.-1.5",
    "href": "shell/01_shell1.html#basic-unix-commands-ch.-1.5",
    "title": "Intro to the Unix Shell",
    "section": "5 Basic Unix commands (Ch. 1.5)",
    "text": "5 Basic Unix commands (Ch. 1.5)\n\n5.1 ls to list files\nThe ls command, short for “list”, will list files and directories — by default those in your current working dir:\n# (You should be in /fs/ess/PAS2700/users/$USER/CSB/unix/data)\nls\nBuzzard2015_about.txt  Gesquiere2011_about.txt  Marra2014_about.txt   miRNA                   Pacifici2013_data.csv  Saavedra2013_about.txt\nBuzzard2015_data.csv   Gesquiere2011_data.csv   Marra2014_data.fasta  Pacifici2013_about.txt  Saavedra2013\n\n\n\n\n\n\nls output colors (click to expand)\n\n\n\n\n\nThe ls output above does not show the different colors you should see in your shell — the most common ones are:\n\nEntries in blue are directories (like miRNA and Saavedra2013 above)\nEntries in black are regular files (like all other entries above)\nEntries in red are compressed files (we’ll see examples of this later).\n\n\n\n\nFor which dir ls lists its contents can be changed with arguments, and how it shows the output can be changed with options. For example, we can call ls with the option -l (lowercase L):\nls -l \ntotal 1793\n-rw-rw----+ 1 jelmer PAS0471     562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471   39058 Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471     447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471   38025 Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471     756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471  566026 Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471     520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1076150 Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471    4096 Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471     322 Feb 24 20:30 Saavedra2013_about.txt\nIt lists the same items as earlier, but printed in a different format: one item per line, with additional information such as the date and time each file was last modified, and file sizes in bytes (to the left of the date).\nLet’s add another option, -h:\nls -lh\ntotal 1.8M\n-rw-rw----+ 1 jelmer PAS0471  562 Feb 24 20:30 Buzzard2015_about.txt\n-rw-rw----+ 1 jelmer PAS0471  39K Feb 24 20:30 Buzzard2015_data.csv\n-rw-rw----+ 1 jelmer PAS0471  447 Feb 24 20:30 Gesquiere2011_about.txt\n-rw-rw----+ 1 jelmer PAS0471  38K Feb 24 20:30 Gesquiere2011_data.csv\n-rw-rw----+ 1 jelmer PAS0471  756 Feb 24 20:30 Marra2014_about.txt\n-rw-rw----+ 1 jelmer PAS0471 553K Feb 24 20:30 Marra2014_data.fasta\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 miRNA\n-rw-rw----+ 1 jelmer PAS0471  520 Feb 24 20:30 Pacifici2013_about.txt\n-rw-rw----+ 1 jelmer PAS0471 1.1M Feb 24 20:30 Pacifici2013_data.csv\ndrwxrwx---+ 2 jelmer PAS0471 4.0K Feb 24 20:30 Saavedra2013\n-rw-rw----+ 1 jelmer PAS0471  322 Feb 24 20:30 Saavedra2013_about.txt\n\n\nWhat is different about the output, and what do you think that means? (Click to see the answer)\n\nThe only difference is in the format of the column reporting the sizes of the items listed.\nWe now have “Human-readable filesizes” (hence -h), where sizes on the scale of kilobytes will be shown with Ks, of megabytes with Ms, and of gigabytes with Gs. That can be really useful especially for very large files.\n\n\nWe can also list files in directories other than the one we are in, by specifying that dir as an argument:\nls miRNA\nggo_miR.fasta  hsa_miR.fasta  miR_about.txt  miRNA_about.txt  ppa_miR.fasta  ppy_miR.fasta  ptr_miR.fasta  ssy_miR.fasta\nAnd like we saw with cal, we can combine options and arguments:\nls -lh miRNA\ntotal 320K\n-rw-rw----+ 1 jelmer PAS0471  18K Feb 24 20:30 ggo_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471 131K Feb 24 20:30 hsa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miR_about.txt\n-rw-rw----+ 1 jelmer PAS0471  104 Feb 24 20:30 miRNA_about.txt\n-rw-rw----+ 1 jelmer PAS0471 4.0K Feb 24 20:30 ppa_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  33K Feb 24 20:30 ppy_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  29K Feb 24 20:30 ptr_miR.fasta\n-rw-rw----+ 1 jelmer PAS0471  495 Feb 24 20:30 ssy_miR.fasta\nLet’s move into the sandbox dir in preparation for the next sections:\ncd ../sandbox\n\nls\nPapers and reviews\n\n\n\n5.2 cp to copy files\nThe cp command copies files and/or dirs from one location to another. It has two required arguments: what you want to copy (source), and where you want to copy it to (destination). Its basic syntax is cp &lt;source&gt; &lt;destination&gt;.\nFor example, to copy a file to our current working directory using the . shortcut, keeping the original file name:\ncp ../data/Buzzard2015_about.txt .\nWe can also copy using a new name for the copy:\ncp ../data/Buzzard2015_about.txt buzz2.txt\ncp will by default refuse to copy directories and their contents — that is, it is not “recursive” by default. The -r option is needed for recursive copying:\ncp -r ../data/ . \nCheck the contents of the sandbox dir again now that we’ve copied several items there:\nls\nbuzz2.txt  Buzzard2015_about.txt  data  Papers and reviews\n\n\n\n5.3 mv to move and rename files\nUse mv both to move and rename files (this is fundamentally the same operation):\n# Same directory, different file name (\"renaming\")\nmv buzz2.txt buzz_copy.txt\n# Different directory, same file name (\"moving\")\nmv buzz_copy.txt data/\nUnlike cp, mv is recursive by default, so you won’t need the -r option.\n\n\n\n\n\n\nBoth the mv and cp commands will by default:\n\n\n\n\nNot report what they do: no output = success (use the -v option for verbose to make them report what they do).\nOverwrite existing files without reporting this (use the -i option for interactive to make them ask before overwriting).\n\n\n\n\n\n\n5.4 rm to remove files and dirs\nThe rm command removes files and optionally dirs — here, we’ll remove the file copy we made above:\nrm Buzzard2015_about.txt\nLike with cp, the -r option is needed to make the command work recursively:\n# First we create 3 levels of dirs - we need `-p` to make mkdir work recursively:\nmkdir -p d1/d2/d3\n\n# Then we try to remove the d1 dir - which fails:\nrm d1\nrm: cannot remove ‘d1’: Is a directory\n# But it does work (silently!) with the '-r' option:\nrm -r d1\n\n\n\n\n\n\nThere is no thrash bin when deleting files in the shell, so use rm with caution! (Click to expand)\n\n\n\n\n\nrm -r can be very dangerous — for example rm -r / would at least attempt to remove the entire contents of the computer, including the operating system.\nA couple ways to take precautions:\n\nYou can add the -i option, which will have you confirm each individual removal (can be tedious)\nWhen you intend to remove an empty dir, you can use the rmdir command which will do just (and only) that — that way, if the dir isn’t empty after all, you’ll get an error.\n\n\n\n\n\n Bonus exercise: cp and mv behavior\nFor both cp and mv, when operating on files (and this works equivalently for dirs):\n\nIf the destination is an existing dir, the file will go into that dir and keep its original name.\nIf the destination is not an existing dir, the (last bit of the) destination specifies the new file name.\nA trailing slash in the destination makes explicit that you are referring to a dir and not a file.\n\n\nWith that in mind, try to answer the following questions about this command:\ncp Buzzard2015_about.txt more_data/\n\nWhat do you think the command would do or attempt to do?\nDo you think the command will succeed?\nWhat would the command have done if we had omitted the trailing forward slash?\n\n\n\nClick to see the solution\n\n\n\nBecause we put a trailing forward slash in more_data/, we are making clear that we are referring to a directory. So the file should be copied into a dir more_data, and keep the same file name.\nHowever, the more_data/ dir does not exist, and cp will not create a dir on the fly, so this will fail:\ncp Buzzard2015_about.txt more_data/\ncp: cannot create regular file ‘more_data/’: Not a directory\nIf we had omitted the trailing forward slash, we would have created a copy of the file with file name more_data (note that no file extension is needed, per se).\nP.S: To make the original intention work, first create the destination dir:\nmkdir more_data\ncp Buzzard2015_about.txt more_data/\nNote also that once the more_data dir exists, it does not make a difference whether or not you using a trailing slash (!).\n\n\n\n\n\n\n5.5 Viewing and processing text files\ncd ../data   # Move to the data dir for the next commands\n\ncat\nThe cat command will print the entire contents of (a) file(s) to screen:\ncat Marra2014_about.txt\nData published by:\nMarra NJ, DeWoody JA (2014) Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. BMC Genomics 15: 929. http://dx.doi.org/10.1186/1471-2164-15-929\n\nData description:\nFile D_spec_spleen_filtered.fasta (57.01Mb) contains Dipodomys spectabilis spleen transcriptome data. Combined assembly of 454 reads and fragmented Illumina assembly (see methods of associated paper) in gsAssembler version 2.6.\nNote that we truncated the original file to 1% of its original size and named it Marra2014_data.txt\n\nData taken from:\nMarra NJ, DeWoody JA (2014) Data from: Transcriptomic characterization of the immunogenetic repertoires of heteromyid rodents. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.qn474\n\n\n\nhead and tail\nThe head and tail commands will print the first or last lines of a file:\n\nhead & tail’s defaults are to print 10 lines:\nhead Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\nUse the -n option to specify the number of lines to print:\nhead -n 3 Gesquiere2011_data.csv\nmaleID  GC      T\n1       66.9    64.57\n1       51.09   35.57\nA neat trick with tail is to start at a specific line, often used to skip the header line, like in this example6:\ntail -n +2 Gesquiere2011_data.csv\n1       66.9    64.57\n1       51.09   35.57\n1       65.89   114.28\n1       80.88   137.81\n1       32.65   59.94\n1       60.52   101.83\n1       65.89   65.84\n1       52.72   43.98\n1       84.85   102.31\n1       98.25   149.61\n[...output truncated...]\n\n\n\n\nwc -l\nThe wc command is different from the previous commands, which all printed file contents. By default, wc will count lines, words, and characters — but it is most commonly used to only count lines, with the -l option:\nwc -l Marra2014_about.txt\n9 Marra2014_about.txt"
  },
  {
    "objectID": "shell/01_shell1.html#advanced-unix-commands-ch.-1.6",
    "href": "shell/01_shell1.html#advanced-unix-commands-ch.-1.6",
    "title": "Intro to the Unix Shell",
    "section": "6 Advanced Unix commands (Ch. 1.6)",
    "text": "6 Advanced Unix commands (Ch. 1.6)\nStart by moving back to the sandbox dir:\ncd ../sandbox\n\n6.1 Standard output and redirection\nThe regular output of a command is also called “standard out” (“stdout”). As we’ve seen many times now, such output is by default printed to screen, but it can alternatively be “redirected”, such as into a file.\nWith “&gt;”, we redirect output to a file:\n\nIf the file doesn’t exist, it will be created.\nIf the file does exist, any contents will be overwritten.\n\nFirst, let’s remind ourselves what echo does without redirection:\necho \"My first line\"\nMy first line\nNow, let’s redirect to a new file test.txt — no output is printed, as it went into the file:\necho \"My first line\" &gt; test.txt\ncat test.txt\nMy first line\nLet’s redirect another line into that same file:\necho \"My second line\" &gt; test.txt\ncat test.txt\nMy second line\nThat may not have been what we intended! As explained above, the file was overwritten.\nWith “&gt;&gt;”, however, we append the output to a file:\necho \"My third line\" &gt;&gt; test.txt\ncat test.txt\nMy second line\nMy third line\n\n\n\n6.2 Standard input and pipes\nRecall from today’s previous examples that a file name can be given as an argument to many commands — for example, see the following sequence of commands:\n# First we redirect the ls output to a file\nls ../data/Saavedra2013 &gt; filelist.txt\n\n# Let's check what that looks like:\nhead -n 5 filelist.txt\nn10.txt\nn11.txt\nn12.txt\nn13.txt\nn14.txt\n# Then we count the nr. of lines, which is the number of files+dirs in Saavedra2013:\nwc -l filelist.txt\n59 filelist.txt\nHowever, most commands also accept input from so-called “standard input” (stdin) using the pipe, “|”:\nls ../data/Saavedra2013 | wc -l\n59\nWhen we use the pipe, the output of the command on the left-hand side (a file listing, in this case) is no longer printed to screen but is redirected into the wc command, which will gladly accept input that way instead of via an argument with a file name like in the earlier example.\nPipes are useful because they avoid having to write/read intermediate files — this saves typing, makes the operation quicker, and reduces file clobber.\n\n\n\n6.3 Selecting columns using cut\nWe’ll now turn to some commands that may be described as Unix “data tools”: cut, sort, uniq, tr, and grep.\nMost of the examples will use the file Pacifici2013_data.csv, so let’s have a look at the contents of that file first:\ncd ../data\nhead -n 3 Pacifici2013_data.csv\nTaxID;Order;Family;Genus;Scientific_name;AdultBodyMass_g;Sources_AdultBodyMass;Max_longevity_d;Sources_Max_longevity;Rspan_d;AFR_d;Data_AFR;Calculated_GL_d;GenerationLength_d;Sources_GL\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n\n\n\n\n\n\nTabular plain-text files (Click to expand)\n\n\n\n\n\nIn bioinformatics, we usually work with so-called “plain-text” files. These are simple files that can be opened by any text editor and Unix shell tool, as opposed to more complex “binary” formats like Excel or Word files. Almost all common genomics file formats are plain-text.\n“Tabular” files contain data that is arranged in a rows-and-columns format, like a table or an Excel worksheet.\nPlain-text tabular files, like Pacifici2013_data.csv, contain a specific “delimiter” to delimit columns — most commonly:\n\nA Tab, and such files are often stored with a .tsv extension for Tab-Separated Values (“TSV file”).\nA comma, and such files are often stored with a .csv extension for Comma-Separated Values (“CSV file”).\n\n\n\n\n\n\nWhat delimiter does the Pacifici2013_data.csv file appear to contain? (Click for the answer)\n\nFrom looking at the first couple of lines that we printed above, the delimiter is a semicolon ; — “even though” the file has a .csv extension.\n\n\nThe cut command will select/“cut out” one or more columns from a tabular file:\n\nWe’ll always have to use the -f option to specify the desired column(s) / “field”(s).\nBecause its default column delimiter is a Tab, for this file, we’ll have to specify the delimiter with -d.\n\n# Select the first column of the file:\ncut -d \";\" -f 1 Pacifici2013_data.csv\nTaxID\n7580\n42632\n42653\n42662\n16652\n[...output truncated...]\nThat worked, but a ton of output was printed, and we may find ourselves scrolling to the top to see the first few lines – in many cases, it can be useful to pipe the output to head to see if our command works:\ncut -d \";\" -f 1 Pacifici2013_data.csv | head -n 3\nTaxID\n7580\n42632\n\n\n\n\n\n\nSelecting multiple columns with cut (Click to expand)\n\n\n\n\n\nTo select multiple columns, use a range or comma-delimited list:\ncut -d \";\" -f 1-4 Pacifici2013_data.csv | head -n 3\nTaxID;Order;Family;Genus\n7580;Rodentia;Cricetidae;Eligmodontia\n42632;Rodentia;Cricetidae;Microtus\ncut -d \";\" -f 2,8 Pacifici2013_data.csv | head -n 3\nOrder;Max_longevity_d\nRodentia;292\nRodentia;456.25\n\n\n\n\n\n\n6.4 Combining cut, sort, and uniq to create a list\nLet’s say we want an alphabetically sorted list of animal orders from the Pacifici2013_data.csv file. To do this, we’ll need two new commands:\n\nsort to sort/order/arrange rows, by default in alphanumeric order.\nuniq to remove duplicates (i.e., keep all distinct) entries from a sorted file/list\n\nWe’ll build up a small “pipeline” to do this, step-by-step and piping the output into head every time. First, we get rid of the header line with our tail trick:\ntail -n +2 Pacifici2013_data.csv | head -n 5\n7580;Rodentia;Cricetidae;Eligmodontia;Eligmodontia typus;17.37;PanTHERIA;292;PanTHERIA;254.64;73.74;calculated;147.5856;147.5856;Rspan-AFR(SM+Gest)\n42632;Rodentia;Cricetidae;Microtus;Microtus oregoni;20.35;PanTHERIA;456.25;PanTHERIA;445.85;58.06;calculated;187.3565;187.3565;Rspan-AFR(SM+Gest)\n42653;Rodentia;Cricetidae;Peromyscus;Peromyscus gossypinus;27.68;PanTHERIA;471.45833335;PanTHERIA;444.87833335;72.58;calculated;201.59471667;201.5947166715;Rspan-AFR(SM+Gest)\n42662;Macroscelidea;Macroscelididae;Elephantulus;Elephantulus myurus;59.51;PanTHERIA;401.5;PanTHERIA;412.34;90.48;calculated;210.0586;210.0586;Rspan-AFR(SM+Gest)\n16652;Rodentia;Cricetidae;Peromyscus;Peromyscus boylii;23.9;PanTHERIA;547.5;PanTHERIA;514.13;79.97;calculated;229.0677;229.0677;Rspan-AFR(SM+Gest)\nSecond, we select our column of interest with cut:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | head -n 5\nRodentia\nRodentia\nRodentia\nMacroscelidea\nRodentia\nThird, we pipe to sort to sort the result:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | head -n 5\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nAfrosoricida\nFourth and finally, we use uniq to only keep unique rows (values):\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq\nAfrosoricida\nCarnivora\nCetartiodactyla\nChiroptera\nCingulata\nDasyuromorphia\nDermoptera\nDidelphimorphia\n[...output truncated...]\n\n\n\n\n\n\nGet a count table with uniq -c (Click to expand)\n\n\n\n\n\nWith a very small modification to our pipeline, using uniq’s -c option (for count), we can generate a “count table” instead of a simple list:\ntail -n +2 Pacifici2013_data.csv | cut -d \";\" -f 2 | sort | uniq -c\n     54 Afrosoricida\n    280 Carnivora\n    325 Cetartiodactyla\n   1144 Chiroptera\n     21 Cingulata\n[...output truncated...]\n\n\n\n\n\n\n6.5 grep to print lines matching a pattern\nThe grep command is useful to find specific text or patterns in a file. By default, it will print each line that contains a “match” in full. It’s basic syntax is grep \"&lt;pattern&gt;\" file.\nFor example, this will print all lines from Pacifici2013_data.csv that contain “Vombatidae”:\ngrep \"Vombatidae\" Pacifici2013_data.csv\n40555;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus latifrons;26163.8;PanTHERIA;9928;\"PanTHERIA;AnAge\";9317.5;652.24;calculated;3354.315;3354.315;Rspan-AFR(SM+Gest)\n40556;Diprotodontia;Vombatidae;Vombatus;Vombatus ursinus;26000;PanTHERIA;10238.25;\"PanTHERIA;AnAge\";9511.6;783.65;calculated;3542.014;3542.014;Rspan-AFR(SM+Gest)\n11343;Diprotodontia;Vombatidae;Lasiorhinus;Lasiorhinus krefftii;31849.99;PanTHERIA;10950;\"PanTHERIA;AnAge\";no information;no information;no information;no information;3354.315;Mean_congenerics_same_body_mass\nInstead of printing matching lines, we can also count them with the -c option:\ngrep -c \"Chiroptera\" Pacifici2013_data.csv\n1144\nThe option -v inverts grep’s behavior and will print all lines not matching the pattern — here, we’ll combine -v and -c to count the number of lines that do not contain “Vombatidae”:\ngrep -vc \"Vombatidae\" Pacifici2013_data.csv\n5424\n\n\n\n\n\n\ngrep tips\n\n\n\n\nIt is best to always use quotes around the pattern.7\nIncomplete matches, including in individual words, work: “Vombat” matches Vombatidae.\ngrep has many other useful options:\n\n-i to ignore case\n-r to search files recursively\n-w to match “words”\n\n\n\n\n\n\n\n6.6 Substituting characters using tr\ntr for translate will substitute characters – here, any a for a b:\necho \"aaaabbb\" | tr a b\nbbbbbbb\nOddly enough tr does not take a file name as an argument, so how can we provide it with input from a file? The easiest way is by using a pipe (note that the book also shows another method):\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" | head -n 3\nTaxID   Order   Family  Genus   Scientific_name AdultBodyMass_g Sources_AdultBodyMass   Max_longevity_d Sources_Max_longevity   Rspan_d AFR_d   Data_AFR        Calculated_GL_d   GenerationLength_d      Sources_GL\n7580    Rodentia        Cricetidae      Eligmodontia    Eligmodontia typus      17.37   PanTHERIA       292     PanTHERIA       254.64  73.74   calculated      147.5856  147.5856        Rspan-AFR(SM+Gest)\n42632   Rodentia        Cricetidae      Microtus        Microtus oregoni        20.35   PanTHERIA       456.25  PanTHERIA       445.85  58.06   calculated      187.3565  187.3565        Rspan-AFR(SM+Gest)\nThe example above converted the ; delimited to a Tab (i.e., a CSV file to a TSV file), where \\t is a regular expression meaning Tab. (Though note that we didn’t modify the original file nor saved the output in a new file.)\n\n\n\n\n\n\nDeletion and “squeezing” with tr (Click to expand)\n\n\n\n\n\n\nDelete all a’s:\necho \"aabbccddee\" | tr -d a\nbbccddee\nRemove consecutive duplicates a’s:\necho \"aabbccddee\" | tr -s a\nabbccddee\n\n\n\n\n\n\n\n\n\n\ntr also works well with multiple values and replacements (Click to expand)\n\n\n\n\n\n\nReplace multiple types of characters with one:\n\necho \"123456789\" | tr 1-5 0    # Replace any of 1-5 with 0\n000006789\n\nOne-to-one mapping of input and replacement!\n\necho \"ACtGGcAaTT\" | tr actg ACTG\nACTGGCAATT\necho \"aabbccddee\" | tr a-c 1-3\n112233ddee\n\n\n\n\n Exercise: Redirecting tr output\nModify our command in which we changed the delimiter to a Tab to redirect the output to a new file, Pacifici2013_data.tsv (note the extension) in the sandbox dir (that’s not where you are located yourself).\n\n\nSolution\n\ncat Pacifici2013_data.csv | tr \";\" \"\\t\" &gt; ../Pacifici2013_data.tsv"
  },
  {
    "objectID": "shell/01_shell1.html#wrap-up-the-unix-philosophy",
    "href": "shell/01_shell1.html#wrap-up-the-unix-philosophy",
    "title": "Intro to the Unix Shell",
    "section": "7 Wrap-up & the Unix philosophy",
    "text": "7 Wrap-up & the Unix philosophy\n\n7.1 The Unix philosophy\n\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. — Doug McIlory\n\nAdvantages of a modular approach:\n\nEasier to spot errors\nEasy to swap out components, including in other languages\nEasier to learn (?)\n\nText “streams”?\nRather than loading entire files into memory, process them one line at a time. Very useful with large files!\n# This command would combine all files in the working dir ending in `.fa`\n# (i.e. FASTA files) into a single file -- even if that's multiple GBs,\n# this will not be a heavy lift at all!\ncat *.fa &gt; combined.fa"
  },
  {
    "objectID": "shell/01_shell1.html#footnotes",
    "href": "shell/01_shell1.html#footnotes",
    "title": "Intro to the Unix Shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Technically, the latter terms are more correct, as Unix formally does refer to a specific operating system.↩︎\nCommand-line Interface (CLI), as opposed to Graphical User Interface (GUI)↩︎\n Though some commands are flexible and accept either order.↩︎\nThere really is only one proper additional options: two others reflect the defaults, and then there’s the version option.↩︎\nBecause of this, anytime you see a word/string that starts with a $ in the shell, you can safely assume that it is a variable.↩︎\nWe’ll see this in action in a bit↩︎\nEven though for “literal string” like in the examples above, as opposed to regular expressions, this is not strictly necessary, it’s good habit to always quote.↩︎"
  },
  {
    "objectID": "shell/02_shellfiles.html#overview-setting-up",
    "href": "shell/02_shellfiles.html#overview-setting-up",
    "title": "Managing files in the shell",
    "section": "1 Overview & setting up",
    "text": "1 Overview & setting up\nIn this session, we will learn some more Unix shell skills, focusing on commands to manage files with an eye on organizing your research projects.\nSpecifically, we will learn about:\n\nWildcard expansion to select and operate on multiple files at once\nBrace expansion to help create regular series of files and dirs\nCommand substitution to save the output of commands\nFor loops to repeat operations across, e.g., file\nRenaming multiple files using for loops\n\nAnd for those that are interested, there is some optional at-home reading about changing file permissions (e.g. to make your raw data read-only) and creating symbolic links (to access files across different projects).\n\n\n1.1 VS Code setup\n\n\n\n\n\n\nStarting VS Code at OSC - with a Terminal (Click to expand)\n\n\n\n\n\n\nLog in to OSC’s OnDemand portal at https://ondemand.osc.edu.\nIn the blue top bar, select Interactive Apps and then near the bottom of the dropdown menu, click Code Server.\nIn the form that appears on a new page:\n\nSelect OSC project PAS2700\nThe starting directory: /fs/ess/PAS2700/users/&lt;user&gt; (replace &lt;user&gt; with your OSC username)\nNumber of hours: 2\nClick Launch.\n\nOn the next page, once the top bar of the box has turned green and says Runnning, click Connect to VS Code.\nOpen a Terminal by clicking      =&gt; Terminal =&gt; New Terminal.\nType pwd to check where you are.\nIf you are not in /fs/ess/PAS2700/users/&lt;user&gt; click      =&gt;   File   =&gt;   Open Folder, then type/select /fs/ess/PAS2700/users/&lt;user&gt; and press OK.\n\n\n\n\n\n\n\n1.2 Create a dummy project – following Buffalo\nGo into the dir for this tutorial that you created earlier:\n# You should be in /fs/ess/PAS2700/users/$USER/\ncd week02\nFirst, we’ll create a set of directories representing a dummy research project:\nmkdir zmays-snps\ncd zmays-snps\n\n# The -p option for mkdir will allow for 'recursive' (nested) dir creation\nmkdir -p data/fastq scripts results/figs\nThe touch command will create one or more empty files. We will use it to create some empty files that are supposed to represent sequence files with forward (“R1”) and reverse (“R2”) DNA sequence reads for 3 samples:\ncd data/fastq\n\ntouch sample1_R1.fastq.gz sample1_R2.fastq.gz\ntouch sample2_R1.fastq.gz sample2_R2.fastq.gz\ntouch sample3_R1.fastq.gz sample3_R2.fastq.gz\nFor a nice recursive overview of your directory structure, use the tree command (with option -C to show colors):\n# \"../..\" tells tree to start two levels up\n# (Output colors are not shown on this webpage)\ntree -C ../..\n../..\n├── data\n│   └── fastq\n│       ├── sample1_R1.fastq.gz\n│       ├── sample1_R2.fastq.gz\n│       ├── sample2_R1.fastq.gz\n│       ├── sample2_R2.fastq.gz\n│       ├── sample3_R1.fastq.gz\n│       └── sample3_R2.fastq.gz\n├── results\n│   └── figs\n└── scripts\n\n5 directories, 6 files"
  },
  {
    "objectID": "shell/02_shellfiles.html#wildcard-expansion",
    "href": "shell/02_shellfiles.html#wildcard-expansion",
    "title": "Managing files in the shell",
    "section": "2 Wildcard expansion",
    "text": "2 Wildcard expansion\nShell wildcard expansion is a very useful technique to select files. Selecting files with wildcard expansion is called globbing. Wildcards are symbols that have a special meaning.\n\nThe * wildcard\nIn globbing, the * wildcard matches any number of any character, including nothing.\nWith the following files in our directory:\nls\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz\nsample2_R2.fastq.gz  sample3_R1.fastq.gz  sample3_R2.fastq.gz\nWe can match both “sample1” files as follows:\nls sample1*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\nls sample1*fastq.gz\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\n\nTo match only files with forward reads (contain “_R1”):\nls *_R1*\nsample1_R1.fastq.gz  sample2_R1.fastq.gz  sample3_R1.fastq.gz\nls *R1.fastq.gz\nsample1_R1.fastq.gz  sample2_R1.fastq.gz  sample3_R1.fastq.gz\n\nWhen globbing, the pattern has to match the entire file name, so this doesn’t match anything:\n# There are no files that _end in_ R1: we'd need another asterisk at the end\nls *R1\nls: cannot access *R1: No such file or directory\n\nIn summary:\n\n\n\n\n\n\n\nPattern\nMatches files whose names…\n\n\n\n\n*\nContain anything (matches all files)1\n\n\n*fastq.gz\nEnd in “.fastq.gz”\n\n\nsample1*\nStart with “sample1”\n\n\n*_R1*\nContain “_R1”\n\n\n\n\n\n\n Exercise: File matching 1\n\nList only the FASTQ files for sample 3.\n\n\n\nClick for the solution\n\nls sample3*\nsample3_R1.fastq.gz  sample3_R2.fastq.gz\n\n\nWhich files would ls samp*le* match?\n\n\n\nClick for the solution\n\nAll of them, since all file names start with sample, and because * also matches “zero characters”, there is no requirement for there to be a character between the p and the l.\n\n\n\n\nOther shell wildcards\nThere are two more shell wildcards, and here is a complete overview of shell wildcards:\n\n\n\n\n\n\n\nWildcard\nMatches\n\n\n\n\n*\nAny number of any character, including nothing\n\n\n?\nAny single character\n\n\n[] and [^]\nOne or none (^) of the “character set” within the brackets\n\n\n\n\nUsing the ? wildcard to match both R1 and R2:\nls sample1_R?.fastq.gz\nsample1_R1.fastq.gz  sample1_R2.fastq.gz\nTo match files for sample1 and sample2 using only a character class with []:\n\nMethod 1 — List all possible characters (1 and 2 in this case):\nls sample[12]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\nMethod 2 – Use a range like [0-9], [A-Z], [a-z]:\nls sample[1-2]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\nMethod 3 – Exclude the unwanted sample ID:\nls sample[^3]*\nsample1_R1.fastq.gz  sample1_R2.fastq.gz  sample2_R1.fastq.gz  sample2_R2.fastq.gz\n\n\n\n\n\n\n\n[] works on single character ranges only: 0-9 works but 10-13 does not.\n\n\n\n\n\n\nThe examples so far may seem trivial, but you can use these techniques to easily operate on selections among 100s or 1000s of files.\n\n\n\nExpansion is done by the shell itself\nThe expansion –to all matching file names– is done by the shell, not by ls or another command you might be using wildcards with. Therefore, ls will “see”/“receive” the list of files after the expansion has already happened.\nFor example: we can copy (cp command), move (mv) or delete (rm) files with shell expansion, and we can also first check which files those command will “see” by first using echo (or ls) with the exact same globbing pattern:\n# Check which files are selected \necho sample[12]*\nsample1_R1.fastq.gz sample1_R2.fastq.gz sample2_R1.fastq.gz sample2_R2.fastq.gz\n# Remove the files with rm\n# (The -v option will make rm report what it's removing)\nrm -v sample[12]*\nremoved ‘sample1_R1.fastq.gz’\nremoved ‘sample1_R2.fastq.gz’\nremoved ‘sample2_R1.fastq.gz’\nremoved ‘sample2_R2.fastq.gz’\n\n\n\n\n\n\nWildcards vs. regular expressions\n\n\n\nDon’t confuse wildcards with regular expressions! You may have used regular expressions before, for example with R or a text editor. They are similar to but not the same as shell wildcards."
  },
  {
    "objectID": "shell/02_shellfiles.html#brace-expansion",
    "href": "shell/02_shellfiles.html#brace-expansion",
    "title": "Managing files in the shell",
    "section": "3 Brace expansion",
    "text": "3 Brace expansion\nWhereas wildcard expansion looks for corresponding files and expands to whichever files are present, brace expansion with {}, is another type of shell expansion that expands to whatever you tell it to.\n# First move up to zmays-snps\ncd ../..\nUse .. within {} to indicate ranges of numbers or letters:\n# Here we'll create 31 _dirs_ for different dates\nmkdir -p data/obs/2024-03-{01..31}\n\nls data/obs\n2024-03-01  2024-03-04  2024-03-07  2024-03-10  2024-03-13  2024-03-16  2024-03-19  2024-03-22  2024-03-25  2024-03-28  2024-03-31\n2024-03-02  2024-03-05  2024-03-08  2024-03-11  2024-03-14  2024-03-17  2024-03-20  2024-03-23  2024-03-26  2024-03-29\n2024-03-03  2024-03-06  2024-03-09  2024-03-12  2024-03-15  2024-03-18  2024-03-21  2024-03-24  2024-03-27  2024-03-30\n# Here we'll create 6 empty _files_\ntouch results/figs/fig-1{A..F}.png\n\nls results/figs\nfig-1A.png  fig-1B.png  fig-1C.png  fig-1D.png  fig-1E.png  fig-1F.png\n\nFinally, you can also use a comma-separated list, and multiple brace expansions — with the latter, you will get all combinations among values in the expansions:\nmkdir -p data/obs2/treatment-{Kr,Df,Tr}_temp-{lo,med,hi}\n\nls data/obs2\ntreatment-Df_temp-hi   treatment-Kr_temp-hi   treatment-Tr_temp-hi\ntreatment-Df_temp-lo   treatment-Kr_temp-lo   treatment-Tr_temp-lo\ntreatment-Df_temp-med  treatment-Kr_temp-med  treatment-Tr_temp-med\n\n\n Exercise: File matching 2\n\nMove back into data/fastq/ and remove all (remaining) files in there in one go.\n\n\n\nClick for the solution\n\ncd data/fastq/   # Assuming you were still in /fs/ess/PAS2700/users/$USER/week02/zmays-snps\n\n# (You don't have to use the -v flag)\nrm -v *fastq.gz\nremoved ‘sample3_R1.fastq.gz’\nremoved ‘sample3_R2.fastq.gz’\n\n\nUsing brace expansion and the touch command, create empty R1 and R2 FASTQ files for 100 samples with IDs from 001 to 100: sample&lt;ID&gt;_R1.fastq and sample&lt;ID&gt;_R2.fastq.\n\n\n\nClick for the solution\n\ntouch sample{001..100}_R{1,2}.fastq\n\nls\nsample001_R1.fastq  sample026_R1.fastq  sample051_R1.fastq  sample076_R1.fastq\nsample001_R2.fastq  sample026_R2.fastq  sample051_R2.fastq  sample076_R2.fastq\nsample002_R1.fastq  sample027_R1.fastq  sample052_R1.fastq  sample077_R1.fastq\nsample002_R2.fastq  sample027_R2.fastq  sample052_R2.fastq  sample077_R2.fastq\nsample003_R1.fastq  sample028_R1.fastq  sample053_R1.fastq  sample078_R1.fastq\nsample003_R2.fastq  sample028_R2.fastq  sample053_R2.fastq  sample078_R2.fastq\nsample004_R1.fastq  sample029_R1.fastq  sample054_R1.fastq  sample079_R1.fastq\nsample004_R2.fastq  sample029_R2.fastq  sample054_R2.fastq  sample079_R2.fastq\nsample005_R1.fastq  sample030_R1.fastq  sample055_R1.fastq  sample080_R1.fastq\nsample005_R2.fastq  sample030_R2.fastq  sample055_R2.fastq  sample080_R2.fastq\nsample006_R1.fastq  sample031_R1.fastq  sample056_R1.fastq  sample081_R1.fastq\nsample006_R2.fastq  sample031_R2.fastq  sample056_R2.fastq  sample081_R2.fastq\nsample007_R1.fastq  sample032_R1.fastq  sample057_R1.fastq  sample082_R1.fastq\nsample007_R2.fastq  sample032_R2.fastq  sample057_R2.fastq  sample082_R2.fastq\nsample008_R1.fastq  sample033_R1.fastq  sample058_R1.fastq  sample083_R1.fastq\nsample008_R2.fastq  sample033_R2.fastq  sample058_R2.fastq  sample083_R2.fastq\nsample009_R1.fastq  sample034_R1.fastq  sample059_R1.fastq  sample084_R1.fastq\nsample009_R2.fastq  sample034_R2.fastq  sample059_R2.fastq  sample084_R2.fastq\nsample010_R1.fastq  sample035_R1.fastq  sample060_R1.fastq  sample085_R1.fastq\nsample010_R2.fastq  sample035_R2.fastq  sample060_R2.fastq  sample085_R2.fastq\nsample011_R1.fastq  sample036_R1.fastq  sample061_R1.fastq  sample086_R1.fastq\nsample011_R2.fastq  sample036_R2.fastq  sample061_R2.fastq  sample086_R2.fastq\nsample012_R1.fastq  sample037_R1.fastq  sample062_R1.fastq  sample087_R1.fastq\nsample012_R2.fastq  sample037_R2.fastq  sample062_R2.fastq  sample087_R2.fastq\nsample013_R1.fastq  sample038_R1.fastq  sample063_R1.fastq  sample088_R1.fastq\nsample013_R2.fastq  sample038_R2.fastq  sample063_R2.fastq  sample088_R2.fastq\nsample014_R1.fastq  sample039_R1.fastq  sample064_R1.fastq  sample089_R1.fastq\nsample014_R2.fastq  sample039_R2.fastq  sample064_R2.fastq  sample089_R2.fastq\nsample015_R1.fastq  sample040_R1.fastq  sample065_R1.fastq  sample090_R1.fastq\nsample015_R2.fastq  sample040_R2.fastq  sample065_R2.fastq  sample090_R2.fastq\nsample016_R1.fastq  sample041_R1.fastq  sample066_R1.fastq  sample091_R1.fastq\nsample016_R2.fastq  sample041_R2.fastq  sample066_R2.fastq  sample091_R2.fastq\nsample017_R1.fastq  sample042_R1.fastq  sample067_R1.fastq  sample092_R1.fastq\nsample017_R2.fastq  sample042_R2.fastq  sample067_R2.fastq  sample092_R2.fastq\nsample018_R1.fastq  sample043_R1.fastq  sample068_R1.fastq  sample093_R1.fastq\nsample018_R2.fastq  sample043_R2.fastq  sample068_R2.fastq  sample093_R2.fastq\nsample019_R1.fastq  sample044_R1.fastq  sample069_R1.fastq  sample094_R1.fastq\nsample019_R2.fastq  sample044_R2.fastq  sample069_R2.fastq  sample094_R2.fastq\nsample020_R1.fastq  sample045_R1.fastq  sample070_R1.fastq  sample095_R1.fastq\nsample020_R2.fastq  sample045_R2.fastq  sample070_R2.fastq  sample095_R2.fastq\nsample021_R1.fastq  sample046_R1.fastq  sample071_R1.fastq  sample096_R1.fastq\nsample021_R2.fastq  sample046_R2.fastq  sample071_R2.fastq  sample096_R2.fastq\nsample022_R1.fastq  sample047_R1.fastq  sample072_R1.fastq  sample097_R1.fastq\nsample022_R2.fastq  sample047_R2.fastq  sample072_R2.fastq  sample097_R2.fastq\nsample023_R1.fastq  sample048_R1.fastq  sample073_R1.fastq  sample098_R1.fastq\nsample023_R2.fastq  sample048_R2.fastq  sample073_R2.fastq  sample098_R2.fastq\nsample024_R1.fastq  sample049_R1.fastq  sample074_R1.fastq  sample099_R1.fastq\nsample024_R2.fastq  sample049_R2.fastq  sample074_R2.fastq  sample099_R2.fastq\nsample025_R1.fastq  sample050_R1.fastq  sample075_R1.fastq  sample100_R1.fastq\nsample025_R2.fastq  sample050_R2.fastq  sample075_R2.fastq  sample100_R2.fastq\n\n\n\nBonus: Count the number of “R1” files by first using ls with a globbing pattern that only selects R1 files, and then piping the ls output into wc -l.\n\n\n\nClick for the solution\n\n# wc -l will count the number of lines, i.e. the number of files\n# (Note that this works properly even though raw ls output may\n# put multiple files on 1 line.)\nls *R1*fastq | wc -l\n100\n\n\nBonus: Copy all files except the two for “sample100” into a new directory called selection — use a wildcard to do the move with a single command. (You will first need to create the new dir separately.)\n\n\n\nClick for the solution\n\nFirst create the selection dir:\nmkdir selection\nMethod 1 — Exclude sample numbers starting with a 1:\ncp sample[^1]* selection/\nMethod 2 — Other way around; include sample numbers starting with a 0:\ncp sample0* selection/"
  },
  {
    "objectID": "shell/02_shellfiles.html#variables-and-command-substitution",
    "href": "shell/02_shellfiles.html#variables-and-command-substitution",
    "title": "Managing files in the shell",
    "section": "4 Variables and command substitution",
    "text": "4 Variables and command substitution\n\n4.1 Variables\nIn programming, we use variables for things that:\n\nWe refer to repeatedly and/or\nAre subject to change.\n\nThese tend to be settings like the paths to input and output files, and parameter values for programs. Using variables makes it easier to change such settings. We also need to understand variables to work with loops and scripts.\n\nAssigning and referencing variables\nTo assign a value to a variable in the shell, use the syntax variable_name=value:\n# Assign the value \"beach\" to a variable with the name \"location\":\nlocation=beach\n\n# Assign the value \"200\" to a variable with the name \"n_lines\":\nn_lines=200\n\n\n\n\n\n\nRecall that there can’t be spaces around the equals sign (=)!\n\n\n\n\n\n\nTo reference a variable (i.e., to access its value):\n\nYou need to put a dollar sign $ in front of its name.\nIt is good practice to double-quote (\"...\") variable names2.\n\nAs before with the environment variable $HOME, we’ll use the echo command to see what values our variables contain:\necho \"$location\"\nbeach\necho \"$n_lines\"\n200\nConveniently, we can use variables in lots of contexts, as if we had directly typed their values:\ninput_file=results/figs/fig-1A.png\n\nls -lh \"$input_file\"\n-rw-rw----+ 1 jelmer PAS0471 0 Mar  7 13:17 results/figs/fig-1A.png\n\n\n\n\n\n\nRules and tips for naming variables (Click to expand)\n\n\n\n\n\nVariable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods, or other special symbols\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file above, as opposed to say $x and $myvar.\n\n\n\n\n\n\n\n4.2 Command substitution\nCommand substitution allows you to store and pass the output of a command to another command. Let’s see an example. As you know, the date command will print the current date and time:\ndate\nThu Mar  7 14:52:22 EST 2024\nIf we try to store the date in a variable directly, it doesn’t work: the literal string “date” is stored instead:\ntoday=date\necho \"$today\"\ndate\nThat’s why we need command substitution, which we can use by wrapping the command inside $():\ntoday=$(date)\necho \"$today\"\nThu Mar  7 14:53:11 EST 2024\n\nOne practical example of using command substitution is when you want to automatically include the current date in a file name. First, note that we can use date +%F to print the date in YYYY-MM-DD format, and omit the time:\ndate +%F\n2024-03-07\nLet’s use that in a command substitution — but a bit differently than before: we use the command substitution $(date +%F) directly in our touch command, rather than first assigning it to a variable:\ntouch README_\"$(date +%F)\".txt\n\nls\nREADME_2024-03-07.txt\n\n\n Bonus exercise: Command substitution\nSay we wanted to store and report the number of lines in a FASTQ file, which tells us how many sequence “reads” are in it (because FASTQ files contain 4 lines per read).\nHere is how we can get the number of lines of a compressed FASTQ file:\n\nUse zcat (instead of regular cat) to print the contents despite the file compression\nAs we’ve seen before, wc -l gets you the number of lines, but note here that if you pipe input into wc -l, it won’t include the file name in the output:\n\nzcat /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz | wc -l\n2000000\nUse command substitution to store the output of the last command in a variable, and then use an echo command to print the following:\nThe file has 2000000 lines\n\n\nClick for the solution\n\nn_lines=$(zcat /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz | wc -l)\n\necho \"The file has $n_lines lines\"\nThe file has 2000000 lines\nNote: You don’t have to quote variables inside a quoted echo call, since it’s, well, already quoted. If you also quote the variables, you will in fact unquote it, although that shouldn’t pose a problem inside echo statements.\n\n\n\n\n\n4.3 For loops\nLoops are a universal element of programming languages, and are used to repeat operations. Here, we’ll only cover the most common type of loop: the for loop.\nA for loop iterates over a collection, such as a list of files, and allows you to perform one or more actions for each element in the collection. In the example below, our “collection” is just a short list of numbers (1, 2, and 3):\nfor a_number in 1 2 3; do\n    echo \"In this iteration of the loop, the number is $a_number\"\n    echo \"--------\"\ndone\nIn this iteration of the loop, the number is 1\n--------\nIn this iteration of the loop, the number is 2\n--------\nIn this iteration of the loop, the number is 3\n--------\nThe indented lines between do and done contain the code that is being executed as many times as there are items in the collection: in this case 3 times, as you can tell from the output above.\nWhat was actually run under the hood is the following:\n# (Don't run this)\na_number=1\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=2\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\n\na_number=3\necho \"In this iteration of the loop, the number is $a_number\"\necho \"--------\"\nHere are two key things to understand about for loops:\n\nIn each iteration of the loop, one element in the collection is being assigned to the variable specified after for. In the example above, we used a_number as the variable name, so that variable contained 1 when the loop ran for the first time, 2 when it ran for the second time, and 3 when it ran for the third and last time.\nThe loop runs sequentially for each item in the collection, and will run exactly as many times as there are items in the collection.\n\n\n\n\n\n\n\nA further explanation of for loop syntax\n\n\n\nOn the first and last, unindented lines, for loops contain the following mandatory keywords:\n\n\n\n\n\n\n\nKeyword\nPurpose\n\n\n\n\nfor\nAfter for, we set the variable name (an arbitrary name; above we used a_number)\n\n\nin\nAfter in, we specify the collection (list of items) we are looping over\n\n\ndo\nAfter do, we have one ore more lines specifying what to do with each item\n\n\ndone\nTells the shell we are done with the loop\n\n\n\n\n\n\n\nCombining loops and globbing\nA very useful strategy is to loop over files with globbing, for example:\nfor fastq_file in data/fastq/*fastq; do\n    echo \"Running an analysis for file $fastq_file\"...\n    # Additional commands to process the FASTQ file\ndone\nRunning an analysis for file data/fastq/sample001_R1.fastq...\nRunning an analysis for file data/fastq/sample001_R2.fastq...\nRunning an analysis for file data/fastq/sample002_R1.fastq...\nRunning an analysis for file data/fastq/sample002_R2.fastq...\nRunning an analysis for file data/fastq/sample003_R1.fastq...\nRunning an analysis for file data/fastq/sample003_R2.fastq...\n# [...output truncated...]\n\n\n\n Exercise: A simple loop\nCreate a loop that will print:\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom\n\n\nClick for the solution\n\nfor mushroom in morel destroying_angel eyelash_cup; do\n    echo \"$mushroom is an Ohio mushroom\"\ndone\nmorel is an Ohio mushroom  \ndestroying_angel is an Ohio mushroom  \neyelash_cup is an Ohio mushroom"
  },
  {
    "objectID": "shell/02_shellfiles.html#renaming-files-with-loops",
    "href": "shell/02_shellfiles.html#renaming-files-with-loops",
    "title": "Managing files in the shell",
    "section": "5 Renaming files with loops",
    "text": "5 Renaming files with loops\nThere are many different ways to rename many files in a programmatic way in the shell – admittedly none as easy as one might have hoped.\nHere, we’ll use the basename command and a for loop. for loops are a verbose method for tasks like renaming, but are relatively intuitive and good to get practice with.\n\nbasename\nFirst, we’ll have to learn about the basename command, which removes any dir name that may be present in a file name (path), and optionally, removes a suffix too:\n# Just remove the directories:\nbasename data/fastq/sample001_R1.fastq\nsample001_R1.fastq\n# Also remove a suffix by specifying it after the file name:\nbasename data/fastq/sample001_R1.fastq .fastq\nsample001_R1\n\n\n\n\n\n\nDon’t have these FASTQ files? (Click to expand)\n\n\n\n\n\nWe made these in one of the exercises above, but if you don’t have them:\n# You should be in /fs/ess/PAS2700/users/$USER/week02\ntouch data/fastq/sample{001..100}_R{1,2}.fastq\n\n\n\n\n\n\nRenaming a single file\nLet’s say that we wanted to rename these files so that they have the suffix .fq instead of .fastq. Here’s how we could do that for one file in a way that we can use in a loop:\nThe original file name will be contained in a variable:\noldname=sample001_R1.fastq\nWe can also save the new name in a variable\nnewname=$(basename \"$oldname\" .fastq).fq\nBefore actually renaming, note this trick with echo to just print the command instead of executing it:\necho mv -v \"$oldname\" \"$newname\"\nmv -v sample001_R1.fastq sample001_R1.fq\nLooks good? Then we remove echo and rename the file (we’re using the -v to make mv report what it’s doing):\nmv -v \"$oldname\" \"$newname\"\nsample001_R1.fastq -&gt; sample001_R1.fq\n\n\n\nLooping over all files\nHere’s how we can loop over these files, saving each file name (one at a time) in the variable $oldname:\nfor oldname in *.fastq; do\n    # ...\ndone\nNext, we assign a new name for each file:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\ndone\nWe build and check the renaming command:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\n    echo mv -v \"$oldname\" \"$newname\"\ndone\nmv -v sample001_R1_001.fastq sample001_R1_001.fq\nmv -v sample001_R2_001.fastq sample001_R2_001.fq\nmv -v sample002_R1_001.fastq sample002_R1_001.fq\nmv -v sample002_R2_001.fastq sample002_R2_001.fq\n# [...output truncated...]\nWe do the renaming by removing echo:\nfor oldname in *.fastq; do\n    newname=$(basename \"$oldname\" .fastq).fq\n    mv -v \"$oldname\" \"$newname\"\ndone\n‘sample001_R1_001.fastq’ -&gt; ‘sample001_R1_001.fq’\n‘sample001_R2_001.fastq’ -&gt; ‘sample001_R2_001.fq’\n‘sample002_R1_001.fastq’ -&gt; ‘sample002_R1_001.fq’\n‘sample002_R2_001.fastq’ -&gt; ‘sample002_R2_001.fq’\n‘sample003_R1_001.fastq’ -&gt; ‘sample003_R1_001.fq’\n‘sample003_R2_001.fastq’ -&gt; ‘sample003_R2_001.fq’\n# [...output truncated...]"
  },
  {
    "objectID": "shell/02_shellfiles.html#bonus-content",
    "href": "shell/02_shellfiles.html#bonus-content",
    "title": "Managing files in the shell",
    "section": "6 Bonus content",
    "text": "6 Bonus content\n\n6.1 Viewing and modifying file permissions\nFile “permissions” are the types of things (e.g. reading, writing) that different groups of users (creator, group, anyone else) are permitted to do with files and dirs.\nThere are a couple of reasons you may occasionally need to view and modify file permissions:\n\nYou may want to make your data read-only\nYou may need to share files with other users at OSC\n\n\n\nViewing file permissions\nTo show file permissions, use ls with the -l (long format) option that we’ve seen before. The command below also uses the -a option to show all files, including hidden ones (and -h to show file sizes in human-readable format):\n\n\n\n\n\nHere is an overview of the file permission notation in ls -l output:\n\n\n\n\n\nIn the two lines above:\n\nrwxrwxr-x means:\nread + write + execute permissions for both the owner (first rwx) and the group (second rwx), and read + execute but not write permissions for others (r-x at the end).\nrw-rw-r-- means:\nread + write but not execute permissions for both the owner (first rw-) and the group (second rw-), and only read permissions for others (r-- at the end).\n\nLet’s create a file to play around with file permissions:\n# Create a test file\ntouch testfile.txt\n\n# Check the default permissions\nls -l testfile.txt\n-rw-rw----+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\n\n\n\nChanging file permissions\nThis can be done in two different ways with the chmod command. Here, we’ll focus on the method with = (set permission to), + (add permission), and - (remove permission).\nFor example, to add read (r) permissions for all (a):\n# chmod &lt;who&gt;+&lt;permission-to-add&gt;:\nchmod a+r testfile.txt\n\nls -l testfile.txt\n-rw-rw-r--+ 1 jelmer PAS0471 0 Mar  7 13:40 testfile.txt\nTo set read + write + execute (rwx) permissions for all (a):\n# chmod &lt;who&gt;=&lt;permission-to-set&gt;`:\nchmod a=rwx testfile.txt\n\nls -l testfile.txt\n-rwxrwxrwx+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\nTo remove write (w) permissions for others (o):\n# chmod &lt;who&gt;-&lt;permission-to-remove&gt;:\nchmod o-w testfile.txt\n\nls -l testfile.txt\n-rwxrwxr-x+ 1 jelmer PAS2700 0 Mar  7 13:36 testfile.txt\n\n\n\n\n\n\nAlternative: changing file permissions with numbers (Click to expand)\n\n\n\n\n\nYou can also use a series of 3 numbers (for user, group, and others) to set permissions, where each number can take on the following values:\n\n\n\nNr\nPermission\nNr\nPermission\n\n\n\n\n1\nx\n5\nr + x\n\n\n2\nw\n6\nr + w\n\n\n4\nr\n7\nr + w + x\n\n\n\nFor example, to set read + write + execute permissions for all:\nchmod 777 testfile.txt\nTo set read + write + execute permissions for yourself, and only read permission for the group and others:\nchmod 744 file.txt\n\n\n\n\n\n\nMaking your data read-only\nSo, if you want to make your raw data (here: the files in the data/fastq dir) read-only, you can use:\n\nSet only read permissions for everyone:\nchmod a=r data/fastq/*\nTake away write permissions for yourself (no-one else should have it by default):\nchmod u-w data/fastq/*\n\n\n\n\n\n\n\n\nRead/execute permissions for directories\n\n\n\nOne tricky and confusing aspect of file permissions is that to list a directory’s content, you need execute permissions for the dir! This is something to take into account when you want to grant others access to your project e.g. at OSC.\nTo set execute permissions for everyone but only for dirs throughout a dir hierarchy, use an X (uppercase x):\nchmod -R a+X my-dir\n\n\n\nAfter running one or both of the above commands, let’s check the permissions:\nls -l data/fastq\ntotal 0\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample001_R1.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample001_R2.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample002_R1.fastq\n-r--r--r--+ 1 jelmer PAS0471 0 Mar  7 13:41 sample002_R2.fastq\n# [...output truncated...]\nWhat happens when we try to remove write-protected files?\nrm data/fastq/*fastq\nrm: remove write-protected regular empty file ‘data/fastq/sample001_R1.fastq’?\nYou’ll be prompted for every file! If you answer y (yes), you can still remove them. (But note that people other than the file’s owners cannot overried file permissions; only if they are system administrators.)\n\n\n\n\n6.2 Using files across projects: Using symbolic links\n\nSingle files\nA symbolic (or soft) links only links to the path of the original file, whereas a hard link directly links to the contents of the original file. Note that modifying a file via either a hard or soft link will modify the original file.\nCreate a symlink to a file using ln -s &lt;source-file&gt; [&lt;link-name&gt;]:\n# Only provide source =&gt; create link of the same name in the wd:\nln -s /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz\n  \n# The link can also be given an arbitrary name/path:\nln -s /fs/ess/PAS2700/share/garrigos/data/fastq/ERR10802863_R1.fastq.gz shared-fastq.fastq.gz\n\n\n\n\n\n\nUse an absolute path to refer to the source file when creating links\n\n\n\nAt least at OSC, you have to use an absolute path for the source file(s), or the link will not work. The $PWD environment variable, which contains your current working directory can come in handy to do so:\n# (Fictional example, don't run this)\nln -s $PWD/shared-scripts/align.sh project1/scripts/\n\n\n\n\n\nMultiple files\nLink to multiple files in a directory at once:\n# (Fictional example, don't run this)\nln -s $PWD/shared_scripts/* project1/scripts/ \nLink to a directory:\n# (Fictional example, don't run this)\nln -s $PWD/shared_scripts/ project1/scripts/\nln -s $PWD/shared_scripts/ project1/scripts/ln-shared-scripts\n\n\n\n\n\n\nBe careful not to remove source files!\n\n\n\nBe careful when linking to directories: you are creating a point of entry to the original dir. Therefore, even if you enter via the symlink, you are interacting with the original files.\nThis means that a command like the following would remove the original directory!\nrm -r symlink-to-dir\nInstead, use rm symlink-to-dir (the link itself is a file, not a dir, so you don’t need -r!) or unlink symlink-to-dir to only remove the link.\n\n\n\n\n Exercise: Creating symbolic links\n\nCreate a symbolic link in your $HOME dir that points to your personal dir in the project dir (/fs/ess/PAS2700/users/$USER).\nIf you don’t provide a name for the link, it will be your username (why?), which is not particularly informative about its destination. Therefore, give it a name that makes sense to you, like PLNTPTH6193-SP24 or pracs-sp24.\n\n\n\nClick for the solution\n\nln -s /fs/ess/PAS1855/users/$USER ~/PLNTPTH6193-SP24\n\n\nWhat would happen if you do rm -rf ~/PLNTPTH8300-SP21? Don’t try this.\n\n\n\nClick for the solution\n\nThe content of the original dir will be removed."
  },
  {
    "objectID": "shell/02_shellfiles.html#footnotes",
    "href": "shell/02_shellfiles.html#footnotes",
    "title": "Managing files in the shell",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcept so-called hidden files.↩︎\nWe’ll talk more about quoting later.↩︎"
  },
  {
    "objectID": "shell/03_scripts.html#overview-and-setting-up",
    "href": "shell/03_scripts.html#overview-and-setting-up",
    "title": "Shell scripting",
    "section": "Overview and setting up",
    "text": "Overview and setting up\n\nThis tutorial\nIn this tutorial, we will talk about:\n\nThe basics of shell scripts\nBoilerplate shell script header lines: shebang and safe settings\nCommand-line arguments to scripts\nSome more details on shell variables ($myvar etc)\nConditionals (if statements) — if we get to that\n\n\n\n\n\n\n\n\nVS Code improvements\n\n\n\nThese two settings will make life easier when writing shell scripts in VS Code.\nFirst, we’ll add a keyboard shortcut to send code from your editor to the terminal. This is the same type of behavior that you may be familiar with from RStudio, and will mean that won’t have to copy-and-paste code into the terminal:\n\nClick the  (bottom-left) =&gt; Keyboard Shortcuts.\nFind Terminal: Run Selected Text in Active Terminal, click on it, then add a shortcut, e.g. Ctrl+Enter1.\n\nIn VS Code’s editor pane, the entire line that your cursor is on is selected by default. As such, your keyboard shortcut will send the line that your cursor is in to the terminal; you can also send multiple lines to the terminal after selecting them.\n\nSecond, we’ll add the ShellCheck VS Code extension. This extension checks shell scripts for errors like referencing variables that have not been assigned. Potential problems show up as colored squiggly lines. It also provided links with more information about the error and how to improve your code. This extension is incredibly useful!\n\nClick on the Extensions icon in the far left (narrow) sidebar in VS Code.\nType “shellcheck” and click the small purple “Install” button next to the entry of this name (the description should include “Timon Wong”, who is the author)."
  },
  {
    "objectID": "shell/03_scripts.html#introduction-to-shell-scripts",
    "href": "shell/03_scripts.html#introduction-to-shell-scripts",
    "title": "Shell scripting",
    "section": "1 Introduction to shell scripts",
    "text": "1 Introduction to shell scripts\nMany bioinformatics tools (programs/software) that are used to analyze omics data are run from the command line. We can run them using command line expressions that are structurally very similar to how we’ve been using basic Unix shell commands.\nHowever, we’ve been running shell commands in a manner that we may call “interactive”, by typing or pasting them into the shell, and then pressing Enter. But when you run bioinformatics tools, it is in most cases a much better idea to run them via shell scripts, which are plain-text files that contain shell code.\n\n“Most Bash scripts in bioinformatics are simply commands organized into a rerunnable script with some added bells and whistles to check that files exist and ensuring any error causes the script to abort.” — Buffalo Ch. 12\n\nTherefore, shell scripts are relatively straightforward to write with what you already know! We will learn about those bells and whistles from the quote above in this session.\n\n\n\n\n\n\nBash vs. shell\n\n\n\nSo far, we’ve mostly used talked about the Unix shell and shell scripts. The quote above uses the word “Bash”, and you’ll see that term more often in other tutorials. The difference is this: there are multiple Unix shell (language) variants and the specific one we have been using, which is also by far the most common, is the Bash shell. Our shell scripts are therefore in the Bash language and can be specifically called Bash scripts.\n\n\n\n\nRunning commands interactively vs. via scripts\nBefore we see why it’s often a better idea to use scripts than to run code interactively, let’s go through a minimal example of both approaches with the tool FastQC, which performs FASTQ file quality control (QC; more on FastQC in the next session).\n\nHere’s how you can run FastQC on one FASTQ file — the command fastqc followed by a file name:\nfastqc data/fastq/A_R1.fastq.gz\nThis is what a minimal shell script to do the same thing would look like:\n#!/bin/bash\nfastqc data/fastq/A_R1.fastq.gz\nIf the above shell script is saved as fastqc.sh in our working dir, it can be executed as follows:\nbash fastqc.sh\n\n\n\n\nWhy use shell scripts\nThere are several general reasons why it can be beneficial to use shell scripts instead of running code interactively line-by-line:\n\nIt is a good way to save and organize your code.\nYou can easily rerun scripts and re-use them in similar contexts.\nRelated to the point above, they provide a first step towards automating the set of analyses in your project.\nWhen your code is tucked away in a shell script, you only have to call the script to run what is potentially a large set of commands.\n\nAnd very importantly for our purposes at OSC, we can submit scripts as “batch jobs” to the compute job scheduling program (which is called Slurm), and this allows us to:\n\nRun scripts remotely without needing to stay connected to the running process, or even to be connected at all to it: we can submit a script, log out from OSC and shut down our computer, and it will still run.\nEasily run analyses that take many hours or even multiple days.\nRun a script many times simultaneously, such as for different files/samples.\n\n\n\n\nSummary of what we need to learn about\n\nWriting shell scripts\nSubmitting scripts to the Slurm job scheduler\nMaking software available at OSC"
  },
  {
    "objectID": "shell/03_scripts.html#a-basic-shell-script",
    "href": "shell/03_scripts.html#a-basic-shell-script",
    "title": "Shell scripting",
    "section": "2 A basic shell script",
    "text": "2 A basic shell script\n\n2.1 A one-line script to start\nCreate your first script, printname.sh (note that shell scripts usually have the extension .sh) as follows:\n# First, let's create and move into a new dir\nmkdir -p week04/scripts\ncd week04\n# Create an empty file\ntouch scripts/printname.sh\nA nice VS Code trick is that is if you hold Ctrl (Cmd on Mac) while hovering over a file path in the terminal, the path should become underlined and you can click on it to open the file. Try that with the printname.sh script2.\nOnce the file is open in your editor pane, type or paste the following inside the script:\necho \"This script will print a first and a last name\"\nShell scripts mostly contain the same regular Unix shell code that we have gotten familiar with, but have so far directly typed in the terminal. As such, our single line with an echo command constitutes a functional shell script!\nOne way of running the script is by typing bash followed by the path to the script:\nbash scripts/printname.sh\nThis script will print a first and a last name\nThat worked! The script doesn’t yet print any names like it “promises” to do, but we will add that functionality in a little bit. But first, we’ll learn about two header lines that are good practice to add to every shell script.\n\n\n\n\n\n\n\nAuto Save in VS Code (Click to expand)\n\n\n\n\n\nAny changes you make to this and other files in the editor pane should be immediately, automatically saved by VS Code. If that’s not happening for some reason, you should see an indication of unsaved changes like a large black dot next to the script’s file name in the editor pane tab header.\nIf the file is not auto-saving, you can always save it manually (including with Ctrl/Cmd+S) like you would do in other programs. However, it may be convenient to turn Auto Save on: press Ctrl/Cmd+Shift+S to open the Command Palette and type “Auto Save”. You should see an option “Toggle Auto Save”: click on that.\n\n\n\n\n\n\n2.2 Shebang line\nWe use a so-called “shebang” line as the first line of a script to indicate which computer language our script uses. More specifically, this line tell the computer where to find the binary (executable) that will run our script.\n#!/bin/bash\nSuch a line starts with #! (hash-bang), basically marking it as a special type of comment. After those two characters comes the file path of the relevant program: in our case Bash, which itself is just a program with an executable file that is located at /bin/bash on Linux and Mac computers.\nWhile not always strictly necessary, adding a shebang line to every shell script is good practice, especially when you submit your script to OSC’s Slurm queue, as we’ll do next week.\n\n\n\n2.3 Shell script settings\nAnother best-practice line you should add to your shell scripts will change some default settings to safer alternatives.\n\nBad default shell settings\nThe following two default settings of the Bash shell are bad ideas inside scripts:\n\nWhen you reference a non-existent (“unset”) variable, the shell replaces that with nothing without complaint:\necho \"Hello, my name is $myname. What is yours?\"\nHello, my name is . What is yours?\nIn scripts, this can lead to all sorts of downstream problems, because you very likely tried and failed to do something with an existing variable (e.g. you misspelled its name, or forgot to assign it altogether). Even more problematically, this can lead to potentially very destructive file removal, as the box below illustrates.\nA Bash script keeps running after encountering errors. That is, if an error is encountered when running, say, line 2 of a script, any remaining lines in the script will nevertheless be executed.\nIn the best case, this is a waste of computer resources, and in worse cases, it can lead to all kinds of unintended consequences. Additionally, if your script prints a lot of output, you might not notice an error somewhere in the middle if it doesn’t produce more errors downstream. But the downstream results from what we at that point might call a “zombie script” can still be completely wrong.\n\n\n\n\n\n\n\n\nAccidental file removal with unset variables\n\n\n\nThe shell’s default behavior of ignoring the referencing of unset variables can lead to accidental file removal as follows:\n\nUsing a variable, we try to remove some temporary files whose names start with tmp_:\n# NOTE: DO NOT run this!\ntemp_prefix=\"temp_\"\nrm \"$tmp_prefix\"*\nUsing a variable, we try to remove a temporary directory:\n# NOTE: DO NOT run this!\ntempdir=output/tmp\nrm -r $tmpdir/*\n\n\n\nAbove, the text specified the intent of the commands. What would have actually happened? (Click to expand)\n\nIn both examples, there is a similar typo: temp vs. tmp, which means that we are referencing a (likely) non-existent variable.\n\nIn the first example, rm \"$tmp_prefix\"* would have been interpreted as rm *, because the non-existent variable is simply ignored. Therefore, we would have removed all files in the current working directory.\nIn the second example, along similar lines, rm -rf $tmpdir/* would have been interpreted as rm -rf /*. Horrifyingly, this would attempt to remove the entire filesystem: recall that a leading / in a path is a computer’s root directory. (-r makes the removal recursive and -f makes forces removal).\n\n\nNote this is especially likely to happen inside scripts, where it is common to use variables and to work non-interactively.\nBefore you get too scared of creating terrible damage, note that at OSC, you would not be able to remove any essential files3, since you don’t have the permissions to do so. On your own computer, this could be more genuinely dangerous, though even there, you would not be able to remove operating system files without specifically requesting “admin” rights.\n\n\n\n\n\nSafer settings\nThe following three settings will make your shell scripts more robust and safer. With these settings, the script terminates with an appropriate error message if:\n\nset -u — an “unset” (non-existent) variable is referenced.\nset -e — almost any error occurs.\nset -o pipefail — an error occurs in a shell “pipeline” (e.g., sort | uniq).\n\nWe can change all of these settings in one line in a script:\nset -e -u -o pipefail\nOr even more concisely:\nset -euo pipefail\n\n\n\n\n2.4 Adding the header lines to our script\nAdd the discussed header lines to your printname.sh script, so it will now contain the following:\n#!/bin/bash\nset -euo pipefail\n\necho \"This script will print a first and a last name\"\nAnd run the script again:\nbash scripts/printname.sh\nThis script will print a first and a last name\nThat didn’t change anything to the output, but at least we confirmed that the script still works.\n\n\n\n\n\n\nCan I run scripts without the bash command? (Click to expand)\n\n\n\n\n\nBecause our script has a shebang line, we have taken one step towards being able to execute the script without the bash command, or in other words, to run the script basically “as a command”. With that method, we could run a script using just its path:\nsandbox/printname.sh\n(Or if the script was in our current working dir, using ./printname.sh. In that case the ./ is necessary to make it explicit that we are referring to a file name: otherwise, when running just printname.sh, the shell would look for a command or program of that name, and wouldn’t be able to find it.)\nHowever, this would also require us to “make the script executable”, which we won’t talk about. But I’m mentioning it here because you might see this way of running scripts being used elsewhere."
  },
  {
    "objectID": "shell/03_scripts.html#command-line-arguments-for-scripts",
    "href": "shell/03_scripts.html#command-line-arguments-for-scripts",
    "title": "Shell scripting",
    "section": "3 Command-line arguments for scripts",
    "text": "3 Command-line arguments for scripts\n\n3.1 Calling a script with arguments\nWhen you call a script to run it, you can pass command-line arguments to it, such as a file to operate on. This is much like when you provide a command like ls with arguments:\n# [Don't run any of this, these are just syntax examples]\n\n# Run ls without arguments:\nls\n\n# Pass 1 filename as an argument to ls:\nls data/sampleA.fastq.gz\n\n# Pass 2 filenames as arguments to ls, separated by spaces:\nls data/sampleA.fastq.gz data/sampleB.fastq.gz\nAnd here is what it looks like to pass arguments to scripts:\n# [Don't run any of this, these are just syntax examples]\n\n# Run scripts without any arguments:\nbash scripts/fastqc.sh\nbash scripts/printname.sh\n\n# Run scripts with 1 or 2 arguments:\nbash scripts/fastqc.sh data/sampleA.fastq.gz  # 1 argument: a filename\nbash scripts/printname.sh John Doe            # 2 arguments: strings representing names\nIn the next section, we’ll see what happens with the arguments we pass to a script inside that script.\n\n\n\n3.2 Placeholder variables\nInside the script, any command-line arguments that you pass to it are automatically available in “placeholder” variables. Specifically:\n\nAny first argument will be assigned to the variable $1\nAny second argument will be assigned to $2\nAny third argument will be assigned to $3, and so on.\n\n\n\n\n In the calls to fastqc.sh and printname.sh above, what are the placeholder variables and their values? (Click for the solution)\n\n\n\nIn bash scripts/fastqc.sh data/sampleA.fastq.gz, a single argument, data/sampleA.fastq.gz, is passed to the script, and will be assigned to $1.\nIn bash scripts/printname.sh John Doe, two arguments are passed to the script: the first one (John) will be stored in $1, and the second one (Doe) in $2.\n\n\n\nHowever, while they are made available, these placeholder variables are not “automagically” used. So, unless we explicitly include code in the script to do something with these variables, nothing extra really happens.\nTherefore, let’s add some code to our printname.sh script to “process” any first and last name that are passed to the script. For now, our script will simply echo the placeholder variables, so that we can see what happens:\n#!/bin/bash\nset -euo pipefail\n\necho \"This script will print a first and a last name\"\necho \"First name: $1\"\necho \"Last name: $2\"\n\n# [Paste this into you script - don't enter this directly in your terminal.]\nNext, we’ll run the script, passing the arguments John and Doe:\nbash scripts/printname.sh John Doe\nThis script will print a first and a last name\nFirst name: John\nLast name: Doe\n\n\n Exercise: Command-line arguments\nIn each scenario that is described below, think about what might happen. Then, run the script as instructed in the scenario to test your prediction.\n\nRunning the script printname.sh without passing arguments to it.\n\n\n\nClick here for the solution\n\nThe script will error out because we are referencing variables that don’t exist: since we didn’t pass command-line arguments to the script, the $1 and $2 have not been set.\nbash scripts/printname.sh\nprintname.sh: line 5: $1: unbound variable\n\n\nAfter commenting out the line with set settings, running the script again without passing arguments to it.\n\n\n\nClick here to learn what “commenting out” means\n\nYou can deactivate a line of code without removing it (because perhaps you’re not sure you may need this line in the end) by inserting a # as the first character of that line. This is often referred to as “commenting out” code.\nFor example, below I’ve commented out the ls command, and nothing will happen if I run this line:\n#ls\n\n\n\nClick here for the solution\n\nThe script will run in its entirety and not throw any errors, because we are now using default Bash settings such that referencing non-existent variables does not throw an error. Of course, no names are printed either, since we didn’t specify any:\nbash scripts/printname.sh\necho \"First name:\"\necho \"Last name:\"\nBeing “commented out”, the set line should read:\n#set -euo pipefail\n\n\nDouble-quoting the entire name when you run the script, e.g.: bash scripts/printname.sh \"John Doe\".\n\n\n\nClick here for the solution\n\nBecause we are quoting \"John Doe\", both names are passed as a single argument and both names end up in $1, the “first name”:\nbash scripts/printname.sh \"John Doe\"\necho \"First name: John Doe\"\necho \"Last name:\"\n\nTo get back to where you were, remove the # you inserted in the script in step 2 above to reactive the set line.\n\n\n\n\n3.3 Copying placeholders to variables with descriptive names\nWhile you can use the $1-style placeholder variables throughout your script, I find it very useful to copy them to more descriptively named variables — for example:\n#!/bin/bash\nset -euo pipefail\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\nUsing descriptively named variables in your scripts has several advantages, such as:\n\nIt will make your script easier to understand for others and for your future self.\nIt will make it less likely that you make errors in your script in which you use the wrong variable in the wrong place.\n\n\n\n\n\n\n\nOther variables that are automatically available inside scripts\n\n\n\n\n$0 contains the script’s file name.\n$# contains the number of command-line arguments passed to the script."
  },
  {
    "objectID": "shell/03_scripts.html#more-on-shell-variables",
    "href": "shell/03_scripts.html#more-on-shell-variables",
    "title": "Shell scripting",
    "section": "4 More on shell variables",
    "text": "4 More on shell variables\n\n4.1 Why use variables\nAbove, we saw that variables are useful to be able to pass arguments to a script, so you can easily rerun a script with a different input file / settings / etc. Let’s take a step back and think about variables and their uses a bit more.\n\n“Processing pipelines having numerous settings that should be stored in variables (e.g., which directories to store results in, parameter values for commands, input files, etc.).\nStoring these settings in a variable defined at the top of the file makes adjusting settings and rerunning your pipelines much easier.\nRather than having to change numerous hardcoded values in your scripts, using variables to store settings means you only have to change one value—the value you’ve assigned to the variable.”\n— Buffalo ch. 12\n\nIn brief, use variables for things that:\n\nYou refer to repeatedly and/or\nAre subject to change.\n\n\n\n\n4.2 Quoting variables\nI have mentioned that it is good practice to quote variables (i.e. to use \"$myvar\" instead of $myvar). So what can happen if you don’t do this?\n# Start by making and moving into a dir to create some messy files\nmkdir sandbox\ncd sandbox\nIf a variable’s value contains spaces:\n# Assign a string with spaces to variable 'today', and print its value:\ntoday=\"Tue, Mar 26\"\necho $today\nTue, Mar 26\n# Try to create a file with a name that includes this variable: \ntouch README_$today.txt\n\n# (Using the -1 option to ls will print each entry on its own line)\nls -1\n26.txt\nMar\nREADME_Tue,\nOops! The shell performed “field splitting” to split the value into three separate units — as a result, three files were created. This can be avoided by quoting the variable:\ntouch README_\"$today\".txt\nls -1\nREADME_Tue, Mar 26.txt\nAdditionally, without quoting, we can’t explicitly indicate where a variable name ends:\n# We intend to create a file named 'README_Tue, Mar 26_final.txt'\ntouch README_$today_final.txt\nls -1\nREADME_.txt\n\n\nDo you understand what happened here? (Click for the solution)\n\nWe have assigned a variable called $today, but the shell will instead look for a variable called $today_final. This is because we have not explicitly indicated where the variable name ends, so the shell will include all characters until it hits a character that cannot be part of a shell variable name: in this case a period, ..\n\nQuoting solves this, too:\ntouch README_\"$today\"_final.txt\nls -1\nREADME_Tue, Mar 26_final.txt\n\n\n\n\n\n\n\nCurly braces notation: ${myvar} (Click to expand)\n\n\n\n\n\nThe $var notation to refer to a variable in the shell is actually an abbreviation of the full notation, which includes curly braces:\necho ${today}\nTue, Mar 26\nPutting variable names between curly braces will also make it clear where the variable name begins and ends, although it does not prevent field splitting:\ntouch README_${today}_final.txt\n\nls\n26_final.txt  Mar  README_Tue,\nBut you can combine curly braces and quoting:\ntouch README_\"${today}\"_final.txt\n\nls\n'README_Tue, Mar 26_final.txt'\n\n\n\n\n\n\n\n\n\nQuoting as “escaping” special meaning & double vs. single quotes (Click to expand)\n\n\n\n\n\nBy double-quoting a variable, we are essentially escaping (or “turning off”) the default special meaning of the space as a separator, and are asking the shell to interpret it as a literal space.\nSimilarly, double quotes will escape other “special characters”, such as shell wildcards. Compare:\n# Due to shell expansion, this will echo/list all files in the current working dir\necho *\n18.txt Aug README_Thu, README_Thu, Aug 18.txt\n# This will simply print the literal \"*\" character \necho \"*\"\n*\nHowever, double quotes not turn off the special meaning of $ (which is to denote a string as a variable):\necho \"$today\"\nThu, Aug 18\n…but single quotes will:\necho '$today'\n$today\n\n\n\n\n\n\n\n4.3 Variable names\nIn the shell, variable names:\n\nCan contain letters, numbers, and underscores\nCannot contain spaces, periods (.), dashes (-), or other special symbols4.\nCannot start with a number\n\nTry to make your variable names descriptive, like $input_file above, as opposed to say $x and $myvar.\nThere are multiple ways of distinguishing words in the absence of spaces, such as $inputFile and $input_file: I prefer the latter, which is called “snake case”.\n\n\n\n\n\n\nCase and environment variables\n\n\n\nAll-uppercase variable names are pretty commonly used — and recall that so-called environment variables are always in uppercase (we’ve seen $USER and $HOME). Alternatively, you can use lowercase for variables and uppercase for “constants”, like when you include certain file paths or settings in a script without allowing them to be set from outside of the script.\n\n\n# Move out of the 'sandbox' dir (back to /fs/ess/PAS2700/users/$USER/week04)\ncd .."
  },
  {
    "objectID": "shell/03_scripts.html#conditionals",
    "href": "shell/03_scripts.html#conditionals",
    "title": "Shell scripting",
    "section": "5 Conditionals",
    "text": "5 Conditionals\nWith conditionals like if statements, we can run one or more commands only if some condition is true. Also, we can run a different set of commands if the condition is not true. This can be useful in shell scripts because we may, for instance, want to process a file differently depending on its file type.\n\n5.1 Basic syntax\nThis is the basic syntax of an if statement in Bash (note that similarities with for loop syntax):\nif &lt;test&gt;; then\n    # Command(s) to run if the condition is true\nfi\nWe’ll have to add an else clause to run alternative command(s) if the condition is false:\nif &lt;test&gt;; then\n    # Command(s) to run if the condition is true\nelse\n    # Commands(s) to run if the condition is false\nfi\n\n\n\n5.2 String comparisons\nFirst, an if statement that tests the file type of say an input file, and runs different code depending on the result:\n# [Hypothetical example - don't run this]\n# Say we have a variable $filetype that contains a file's type\n\nif [[ \"$filetype\" == \"fastq\" ]]; then\n    echo \"Processing FASTQ file...\"\n    # Commands to process the FASTQ file\nelse\n    echo \"Unknown filetype!\"\n    exit 1\nfi\nIn the code above, note that:\n\nThe double square brackets [[ ]] represent a test statement5.\nThe spaces bordering the brackets on the inside are necessary: [[\"$filetype\" == \"fastq\"]] would fail!\nDouble equals signs (==) are common in programming to test for equality — this is to contrast it with a single =, which is used for variable assignment.\nWhen used inside a script, the exit command will stop the execution of the script. With exit 1, the exit status of our script is 1: in bash, an exit status of 0 means success — any other integer, including 1, means failure.\n\n\n\n\n\n\n\n\n\nString comparison\nEvaluates to true if\n\n\n\n\nstr1 == str2\nStrings str1 and str2 are identical6\n\n\nstr1 != str2        \nStrings str1 and str2 are different                \n\n\n-z str\nString str is null/empty (useful with variables)\n\n\n\n\n\n\n5.3 File tests\nThe code below tests whether an input file exists using the file test -f and if it does not (hence the !), it will stop the execution of the script:\n# [Hypothetical example - don't run this]\n\n# '-f' is true if the file exists,\n# and '! -f' is true if the file doesn't exist\nif [[ ! -f \"$fastq_file\" ]]; then\n    echo \"Error: Input file $fastq_file not found!\"\n    exit 1\nfi\n\n\n\n\n\n\n\n\nFile/dir test\nEvaluates to true if\n\n\n\n\n-f file\nfile exists and is a regular file (not a dir or link)\n\n\n-d dir\ndir exists and is a directory          \n\n\n-e file/dir\nfile/dir exists\n\n\n\n\n\n\n5.4 Integer (number) comparisons\nTo avoid unexpected or hard-to-understand errors later on in a shell script, we may choose to test at the beginning whether the correct number of arguments was passed to the script, and abort the script if this is not the case:\n# [Hypothetical example - don't run this]\n\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 2 are required.\"\n    echo \"Usage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\"\n    exit 1\nfi\n\n\n\n\n\n\n\n\nInteger comparisons\nEvaluates to true if\n\n\n\n\nint1 -eq int2\nIntegers int1 and int2 are equal\n\n\nint1 -ne int2\nIntegers int1 and int2 are not equal\n\n\nint1 -lt int2\nInteger int1 is less than int2 (-le for less than or equal to)\n\n\nint1 -gt int2\nInteger int1 is greater than int2 (-ge for greater than or equal to)\n\n\n\n\n\n\n\n\n\n\nAnother integer comparison example (Click to expand)\n\n\n\n\n\nSay that we want to run a program with options that depend on our number of samples. With the number of samples determined from the number of lines in a hypothetical file samples.txt and stored in a variable $n_samples, we can test if the number is greater than 9 with \"$n_samples\" -gt 9, where gt stands for “greater than”:\n# [Hypothetical example - don't run this]\n\n# Store the number of samples in variable $n_samples:\nn_samples=$(cat samples.txt | wc -l)\n\n# With '-gt 9', the if statement tests whether the number of samples is greater than 9:\nif [[ \"$n_samples\" -gt 9 ]]; then\n    # Commands to run if nr of samples &gt;9:\n    echo \"Processing files with algorithm A\"\nelse\n    # Commands to run if nr of samples is &lt;=9:\n    echo \"Processing files with algorithm B...\"\nfi\n\n\n\n\n\n\n\n\n\nCombining multiple expressions with && and || (Click to expand)\n\n\n\n\n\nTo test for multiple conditions at once, use the && (“and”) and || (“or”) shell operators — for example:\n\nIf the number of samples is less than 100 and at least 50 (i.e. 50-99):\nif [[ \"$n_samples\" -lt 100 && \"$n_samples\" -ge 50 ]]; then\n    # Commands to run if the number of samples is 50-99\nfi\nIf either one of two FASTQ files don’t exist:\nif [[ ! -f \"$fastq_R1\" || ! -f \"$fastq_R2\" ]]; then\n    # Commands to run if either file doesn't exist - probably report error & exit\nfi\n\n\n\n\n\n\n Exercise: No middle names allowed!\nIn your printname.sh script, add the if statement from above that tests whether the correct number of arguments were passed to the script. Then, try running the script consecutively with 1, 2, or 3 arguments.\n\n\nStart with this printname.sh script we wrote above.\n\n#!/bin/bash\nset -euo pipefail\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\n\n\n\nClick for the solution\n\nNote that the if statement should come before you copy the variables to first_name and last_name, otherwise you get the “unbound variable error” before your descriptive custom error, when you pass 0 or 1 arguments to the script.\nThe final script:\n#!/bin/bash\nset -euo pipefail\n\nif [[ ! \"$#\" -eq 2 ]]; then\n    echo \"Error: wrong number of arguments\"\n    echo \"You provided $# arguments, while 2 are required.\"\n    echo \"Usage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\"\n    exit 1\nfi\n\nfirst_name=$1\nlast_name=$2\n\necho \"This script will print a first and a last name\"\necho \"First name: $first_name\"\necho \"Last name: $last_name\"\nRun it with different numbers of arguments:\nbash scripts/printname.sh Jelmer\nError: wrong number of arguments\nYou provided 1 arguments, while 2 are required.\nUsage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\nbash scripts/printname.sh Jelmer Poelstra\nFirst name: Jelmer\nLast name: Poelstra\nbash scripts/printname.sh Jelmer Wijtze Poelstra\nError: wrong number of arguments\nYou provided 3 arguments, while 2 are required.\nUsage: printname.sh &lt;first-name&gt; &lt;last-name&gt;\n\n\n\n Exercise: Conditionals II\nOpen a new script sandbox.sh and in it, write an if statement that tests whether the script scripts/printname.sh exists and is a regular file, and:\n\nIf it is (then block), report the outcome with echo (e.g. “The file is found”).\nIf it is not (else block), also report that outcome with echo (e.g. “The file is not found”).\n\nThen:\n\nRun your if statement by pasting the code into the terminal — it should report that the file is found.\nIntroduce a typo in the file name in the if statement, and run it again, to check that the file is not indeed not found.\n\n(Note that your new script isn’t meant to be run per se, but it is much easier to write multi-line statements in a text file than directly in the terminal.)\n\n\nClick for the solution\n\n# Note: you need single quotes when using exclamation marks with echo!\nif [[ -f scripts/printname.sh ]]; then\n    echo 'Phew! The file is found.'\nelse\n    echo 'Oh no! The file is not found!'\nfi\nPhew! The file is found.\nAfter introducing a typo:\nif [[ -f scripts/printnames.sh ]]; then\n    echo 'Phew! The file is found.'\nelse\n    echo 'Oh no! The file is not found!'\nfi\nOh no! The file is not found!"
  },
  {
    "objectID": "shell/03_scripts.html#footnotes",
    "href": "shell/03_scripts.html#footnotes",
    "title": "Shell scripting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Don’t worry about the warning that other keybindings exist for this shortcut.↩︎\n Alternatively, find the script in the file explorer in the side bar and click on it there.↩︎\n And more generally, you can’t remove or edit files that are not yours unless you’ve explicitly been given permission for this.↩︎\n Compare this with the situation for file names, which ideally do not contain spaces and special characters either, but in which - and . are recommended.↩︎\n You can also use single square brackets [ ] but the double brackets have more functionality and I would recommend to always use these.↩︎\nA single = also works but == is clearer.↩︎"
  },
  {
    "objectID": "shell/04_cli-tools.html#overview-and-setting-up",
    "href": "shell/04_cli-tools.html#overview-and-setting-up",
    "title": "Running command-line (CLI) tools with shell scripts",
    "section": "Overview and setting up",
    "text": "Overview and setting up\nThis session focuses on using programs, like various bioinformatics tools, with command-line interfaces (CLIs), and on running these inside shell scripts.\n\nThe strategy that you’ll learn is to write scripts that run a single program a single time, even if you need to run the program many times: in that case, you’ll loop over files outside of that script.\nThis means you’ll also need a higher-level “runner” script in which you save the looping code and more. We will talk about why this strategy makes sense, especially when you have a supercomputer at your disposal.\n\nWe’ll start with a quick intro to FASTQ files and the FastQC program, which will be our first example of a CLI tool.\n\nOur practice data set\nOur practice data set is from the paper “Two avian Plasmodium species trigger different transcriptional responses on their vector Culex pipiens”, published last year in Molecular Ecology (link):\n\n\n\n\n\nThis paper uses RNA-seq data to study gene expression in Culex pipiens mosquitos infected with malaria-causing Plasmodium protozoans — specifically, it compares mosquitos according to:\n\nInfection status: Plasmodium cathemerium vs. P. relictum vs. control\nTime after infection: 24 h vs. 10 days vs. 21 days\n\nHowever, to keep things manageable for our practice, I have subset the data to omit the 21-day samples and only keep 500,000 reads per FASTQ file. All in all, our set of files consists of:\n\n44 paired-end Illumina FASTQ files for 22 samples.\nCulex pipiens reference genome file from NCBI: assembly in FASTA format and annotation in GTF format.\nA metadata file in TSV format with sample IDs and treatment & time point info.\nA README file describing the data set.\n\n\n\nGet your own copy of the data\n# (Assuming you are in /fs/ess/PAS2700/users/$USER)\ncd week04\n\ncp -rv /fs/ess/PAS2700/share/garrigos_data .\n‘/fs/ess/PAS2700/share/garrigos_data’ -&gt; ‘./garrigos_data’\n‘/fs/ess/PAS2700/share/garrigos_data/meta’ -&gt; ‘./garrigos_data/meta’\n‘/fs/ess/PAS2700/share/garrigos_data/meta/metadata.tsv’ -&gt; ‘./garrigos_data/meta/metadata.tsv’\n‘/fs/ess/PAS2700/share/garrigos_data/ref’ -&gt; ‘./garrigos_data/ref’\n‘/fs/ess/PAS2700/share/garrigos_data/ref/GCF_016801865.2.gtf’ -&gt; ‘./garrigos_data/ref/GCF_016801865.2.gtf’\n‘/fs/ess/PAS2700/share/garrigos_data/ref/GCF_016801865.2.fna’ -&gt; ‘./garrigos_data/ref/GCF_016801865.2.fna’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq’ -&gt; ‘./garrigos_data/fastq’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802868_R2.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802868_R2.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802863_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802863_R1.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802880_R2.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802880_R2.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802880_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802880_R1.fastq.gz’\n‘/fs/ess/PAS2700/share/garrigos_data/fastq/ERR10802870_R1.fastq.gz’ -&gt; ‘./garrigos_data/fastq/ERR10802870_R1.fastq.gz’\n# [...output truncated...]\nTake a look at the files you just copied with the tree command, a sort of recursive ls with simple tree-like output:\n# -C will add colors (not shown in the output below)\ntree -C garrigos_data\ngarrigos_data\n├── fastq\n│   ├── ERR10802863_R1.fastq.gz\n│   ├── ERR10802863_R2.fastq.gz\n│   ├── ERR10802864_R1.fastq.gz\n│   ├── ERR10802864_R2.fastq.gz\n│   ├── [...other FASTQ files not shown...]\n├── meta\n│   └── metadata.tsv\n├── README.md\n└── ref\n    ├── GCF_016801865.2.fna\n    └── GCF_016801865.2.gtf\n\n3 directories, 48 files"
  },
  {
    "objectID": "shell/04_cli-tools.html#fastq-files-and-fastqc",
    "href": "shell/04_cli-tools.html#fastq-files-and-fastqc",
    "title": "Running command-line (CLI) tools with shell scripts",
    "section": "1 FASTQ files and FastQC",
    "text": "1 FASTQ files and FastQC\n\n1.1 The FASTQ format\nFASTQ is a very common output format of high-throughput sequencing machines. Like most genomic data files, these are plain text files. Each sequence that is read by the sequencer (i.e., each “read”) forms one FASTQ entry represented by four lines. The lines contain, respectively:\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe nucleotide sequence itself\nA + (plus sign)\nOne-character quality scores for each base in the sequence\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than what is typical.\n\n\nThe “Q” in FASTQ stands for “quality”, to contrast this format with FASTA, a more basic and generic sequence data format that does not include base quality scores. FASTQ files have the extension .fastq or .fq, but they are very commonly gzip-compressed, in which case their name ends in .fastq.gz or .fq.gz.\n\n\n\n\n\n\nFASTQ quality scores (Click to expand)\n\n\n\n\n\nThe quality scores we saw in the read above represent an estimate of the error probability of the base call. Specifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\nFor some specific probabilities and their rough qualitative interpretations for Illumina data:\n\n\n\n\n\n\n\n\n\nPhred quality score\nError probability\nRough interpretation\nASCII character\n\n\n\n\n10\n1 in 10\nterrible\n+\n\n\n20\n1 in 100\nbad\n5\n\n\n30\n1 in 1,000\ngood\n?\n\n\n40\n1 in 10,000\nexcellent\n?\n\n\n\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character” (last column in the table). This allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read. (For your reference, here is a complete lookup table — look at the top table, “BASE=33”).\n\n\n\n\n\n\n1.2 Our FASTQ files\nTake a look at a file listing of your FASTQ files:\nls -lh garrigos_data/fastq\ntotal 941M\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802863_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802863_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802864_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802864_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802865_R1.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 22M Mar 21 09:38 ERR10802865_R2.fastq.gz\n-rw-r-----+ 1 jelmer PAS0471 21M Mar 21 09:38 ERR10802866_R1.fastq.gz\n# [...output truncated...]\nNote that:\n\nThere are two files per sample: _R1 (forward reads) and _R2 (reverse reads).\nThe files all have a .gz extension, indicating they have been compressed with the gzip utility.\nThe files are ~21-22 Mb in size — considerably smaller than the original file sizes (around 1-2 Gb, which is typical) because they were subsampled.\n\n\n\n\n1.3 Viewing the contents of FASTQ files\nNext, try to take a peak inside one of these FASTQ files. Use -n 8 with head to print the first 8 lines (2 reads):\nhead -n 8 garrigos_data/fastq/ERR10802863_R1.fastq.gz\n�\nԽے�8�E��_1f�\"�QD�J��D�fs{����Yk����d��*��\n|��x���l޴�j�N������?������ٔ�bUs�Ng�Ǭ���i;_��������������|&lt;�v����3��������|���ۧ��3ĐHyƕ�bIΟD�%����Sr#~��7��ν��1y�Ai,4\nw\\]\"b�#Q����8��+[e�3d�4H���̒�l�9LVMX��U*�M����_?���\\[\"��7�s\\&lt;_���:�$���N��v�}^����sw�|�n;&lt;�&lt;�oP����\ni��k��q�ְ(G�ϫ��L�^��=��&lt;���K��j�_/�[ۭV�ns:��U��G�z�ݎ�j����&��~�F��٤ZN�'��r2z}�f\\#��:�9$�����H�݂�\"�@M����H�C�\n�0�pp���1�O��I�H�P됄�.Ȣe��Q�&gt;���\n�'�;@D8���#��St�7k�g��|�A䉻���_���d�_c������a\\�|�_�mn�]�9N������l�٢ZN�c�9u�����n��n�`��\n\"gͺ�\n    ���H�?2@�FC�S$n���Ԓh�       nԙj��望��f      �?N@�CzUlT�&�h�Pt!�r|��9~)���e�A�77�h{��~��     ��\n# [...output truncated...]\n\n\nOuch! 😳 What went wrong here? (Click for the solution)\n\nWe were directly presented with the contents of the compressed file, which isn’t human-readable.\n\n\n\n\n\n\n\n\nNo need to decompress\n\n\n\nTo get around the problem we just encountered with head, we might be inclined to uncompress these files, which we could do with the gunzip command. However, uncompressed files take up several times as much disk storage space as compressed ones. Fortunately, we don’t need to decompress them:\n\nAlmost any bioinformatics tool will accept compressed FASTQ files.\nWe can still view these files in compressed form, as shown below.\n\n\n\nInstead, we’ll use the less command, which automatically displays gzip-compressed files in human-readable form:\nless -S garrigos_data/fastq/ERR10802863_R1.fastq.gz\n@ERR10802863.8435456 8435456 length=74\nCAACGAATACATCATGTTTGCGAAACTACTCCTCCTCGCCTTGGTGGGGATCAGTACTGCGTACCAGTATGAGT\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.27637245 27637245 length=74\nGCCACACTTTTGAAGAACAGCGTCATTGTTCTTAATTTTGTCGGCAACGCCTGCACGAGCCTTCCACGTAAGTT\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE&lt;EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.10009244 10009244 length=73\nCTCGGCGTTAACTTCATCACGCAGATCATTCCGTTCCAGCAGCTGAAGCAAGACTACCGTCAGTACGAGATGA\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.6604176 6604176 length=74\nAACTACAAATCTTCCTGTGCCGTTTCCAGCAAGTACGTCGATACCTTCGATGGACGCAACTACGAGTACAACAT\n+\nAAAAAEAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.11918285 11918285 length=35\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n###################################\n\n\n\n\n\n\nThe -S option to less suppresses line-wrapping: lines in the file will not be “wrapped” across multiple lines.\n\n\n\n\n\n\n\n\n Exercise: Explore the file with less\nAfter running the command above, you should be viewing the file inside the less pager.\nYou can move around in the file in several ways: by scrolling with your mouse, with up and down arrows, or, if you have them, PgUp and PgDn keys (also, u will move up half a page and d down half a page).\nRecall that you won’t get your shell prompt back until you press q to quit less.\nDo you notice anything that strikes you as potentially unusual, some reads that look different from others?\n\n\nClick for the solution\n\nThere are a number of reads that are much shorter than the others and only consist of N, i.e. uncalled bases. For example:\n@ERR10802863.11918285 11918285 length=35\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n###################################\n\n\n\n\n\n1.4 FastQC\nFastQC is a ubiquitous tool for quality control of FASTQ files. Running FastQC or a similar program is the first step in nearly any high-throughput sequencing project. FastQC is also a good introductory example of a tool with a command-line interface.\nFor each FASTQ file, FastQC outputs an HTML file that you can open in your browser with about a dozen graphs showing different QC metrics. The most important one is the per-base quality score graph:\n\n\n\nA FastQC per-base quality score graph for files with reasonably good quality reads. The y-axis shows Phred quality scores (higher is better, see also the color-coding) and the x-axis shows the position along the read."
  },
  {
    "objectID": "shell/04_cli-tools.html#running-fastqc-interactively",
    "href": "shell/04_cli-tools.html#running-fastqc-interactively",
    "title": "Running command-line (CLI) tools with shell scripts",
    "section": "2 Running FastQC interactively",
    "text": "2 Running FastQC interactively\nThe command fastqc will run the FastQC program. If you want to analyze a FASTQ file with default FastQC settings, a complete FastQC command would simply be fastqc followed by the file name:\n# (Don't run this)\nfastqc garrigos_data/fastq/ERR10802863_R1.fastq.gz\nHowever, an annoying FastQC default behavior is that it writes its output files in the same dir that contains the input FASTQ files — this means mixing your raw data with your results, which we don’t want!\nTo figure out how we can change that behavior, first consider that many commands and bioinformatics tools alike have an option -h and/or --help to print usage information to the screen. Let’s try that:\nfastqc -h\nbash: fastqc: command not found...\nHowever, there is a wrinkle — while FastQC is installed at OSC1, we have to first “load it”2:\nmodule load fastqc/0.11.8\n\n\n Exercise: FastQC help and output dir\nPrint FastQC’s help info, and figure out which option you can use to specify a custom output directory.\n\n\nClick for the solution\n\nRunning fastqc -h or fastqc --help will work to show the help info. You’ll get quite a bit of output printed to screen, including the snippet about output directories that is reproduced below:\nfastqc -h\n  -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\nSo, you can use -o or equivalently, --outdir to specify an output dir.\n\n\n\nWith the added --outdir (or -o) option, let’s try to run the following FastQC command:\n# We'll have to first create the outdir ourselves, in this case\nmkdir -p results/fastqc\n\n# Now we run FastQC\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R1.fastq.gz\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\n[...truncated...]\nAnalysis complete for ERR10802863_R1.fastq.gz\nIn the output dir we specified, we have a .zip file, which contains tables with FastQC’s data summaries, and an .html (HTML) file, which contains the graphs:\nls -lh results/fastqc\ntotal 512K\n-rw-rw----+ 1 jelmer PAS0471 241K Mar 21 09:53 ERR10802863_R1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 256K Mar 21 09:53 ERR10802863_R1_fastqc.zip\n\n\n\n\n\n\nSpecifying an output dir vs. output file(s)\n\n\n\nFastQC allows us to specify the output directory, but not the output file names: these will be automatically determined based on the input file name(s). This kind of behavior is fairly common for bioinformatics programs, since they will often produce multiple output files.\n\n\n\n\n Exercise: Another FastQC run\nRun FastQC for the corresponding R2 FASTQ file. Would you use the same output dir or a separate one?\n\n\nClick for the solution\n\nYes, it makes sense to use the same output dir, since as you could see above, the output file names have the input file identifiers in them. As such, we don’t need to worry about overwriting files, and it will be more convenient to have all results in a single dir.\nTo run FastQC for the R2 (reverse-read) file:\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R2.fastq.gz\nStarted analysis of ERR10802863_R2.fastq.gz\nApprox 5% complete for ERR10802863_R2.fastq.gz\nApprox 10% complete for ERR10802863_R2.fastq.gz\nApprox 15% complete for ERR10802863_R2.fastq.gz\n[...truncated...]\nAnalysis complete for ERR10802863_R2.fastq.gz\nls -lh results/fastqc\ntotal 1008K\n-rw-rw----+ 1 jelmer PAS0471 241K Mar 21 09:53 ERR10802863_R1_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 256K Mar 21 09:53 ERR10802863_R1_fastqc.zip\n-rw-rw----+ 1 jelmer PAS0471 234K Mar 21 09:55 ERR10802863_R2_fastqc.html\n-rw-rw----+ 1 jelmer PAS0471 244K Mar 21 09:55 ERR10802863_R2_fastqc.zip\nNow, we have four files: two for each of our preceding successful FastQC runs."
  },
  {
    "objectID": "shell/04_cli-tools.html#running-fastqc-with-a-shell-script",
    "href": "shell/04_cli-tools.html#running-fastqc-with-a-shell-script",
    "title": "Running command-line (CLI) tools with shell scripts",
    "section": "3 Running FastQC with a shell script",
    "text": "3 Running FastQC with a shell script\nInstead of running FastQC interactively, we’ll want to write a script that runs it. Specifically, our script will deliberately run FastQC on only one FASTQ file.\nIn bioinformatics, you commonly need to run a CLI tool many times, because most tools can or have to be run separately run for each file or sample. Instead of writing a script that runs one file or sample, a perhaps more intuitive approach would be writing a script that processes all files/samples in a single run. That can be accomplished by:\n\nLooping over files/samples inside the script; or\nPassing many file names or a glob with * to a single run of the tool (this can be done with some tools).\n\nHowever, given that we have access to OSC’s clusters, it will save running time -potentially a lot of it- when we submit a separate batch job for each FASTQ file. This is why we will write a script such that runs only one file, and then we’ll run that script many times using a loop outside of te script.\nFor now, we’ll practice with writing scripts this way, and running them interactively. In the next tutorial, we will take the next step and submit each run of the script as a batch job.\n\n\n3.1 Arguments to the script\nOur favored approach of running the script for one FASTQ file at a time means that our script needs to accept a FASTQ file name as an argument. So instead of using a line like the one we ran above…\n# [Don't copy or run this]\nfastqc --outdir results/fastqc garrigos_data/fastq/ERR10802863_R2.fastq.gz\n…we would use a variable for the file name — for example:\n# [Don't copy or run this]\nfastqc --outdir results/fastqc \"$fastq_file\"\nAnd while we’re at it, we may also want to use a variable for the output dir:\n# [Don't copy or run this]\nfastqc --outdir \"$outdir\" \"$fastq_file\"\nOf course, these variables don’t appear out of thin air completely — we need to pass arguments to the script, and copy the placeholder variables inside the script:\n# [Don't copy or run this]\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\n\n\n\n\n\n\n\nRunning the script\n\n\n\nAnd such a script would be run for a single file as follows:\n# [Don't copy or run this]\n# Syntax: 'bash &lt;script-path&gt; &lt;argument1&gt; &lt;argument2&gt;'\nbash scripts/fastqc.sh garrigos_data/fastq/ERR10802863_R2.fastq.gz results/fastqc\nAnd it would be run for all files by looping over all them as follows:\n# [Don't copy or run this]\n# Run the script separately for each FASTQ file\nfor fastq_file in data/fastq/*fastq.gz; do\n    bash scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n\n\n\n\n\n3.2 Creating an initial script\nWe saw some code to run FastQC inside a script, to which we should add a number of “boilerplate” bits of code:\n\nThe shebang line and strict Bash settings:\n#!/bin/bash\nset -euo pipefail\nA line to load the relevant OSC software module:\nmodule load fastqc/0.11.8\nA line to create the output directory if it doesn’t yet exist:\nmkdir -p \"$outdir\"\n\n\n\n\n\n\n\nRefresher: the -p option to mkdir (Click to expand)\n\n\n\n\n\nUsing the -p option does two things at once, and both are necessary for a foolproof inclusion of this command in a script:\n\nIt will enable mkdir to create multiple levels of directories at once (i.e., to act recursively): by default, mkdir errors out if the parent directory/ies of the specified directory don’t yet exist.\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: No such file or directory\n# This successfully creates both directories\nmkdir -p newdir1/newdir2\nIf the directory already exists, it won’t do anything and won’t return an error. Without this option, mkdir would return an error in this case, which would in turn lead the script to abort at that point with our set settings:\nmkdir newdir1/newdir2\nmkdir: cannot create directory ‘newdir1/newdir2’: File exists\n# This does nothing since the dirs already exist\nmkdir -p newdir1/newdir2\n\n\n\n\nWith those additions, our partial script would look like this:\n# [Don't copy or run this - we'll add to it later]\n\n#!/bin/bash\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\nNotice that this script to run a CLI tool is very similar to our “toy scripts” from the previous sessions: mostly standard (“boilerplate”) code with just a single command to run our program of interest. Therefore, you can adopt this script as a template for scripts that run other command-line programs, and will generally only need minor modifications!\n\n\n\n3.3 Add some “logging” statements\nIt is often useful to have your scripts “report” or “log” what is going on. For instance:\n\nAt what date and time did we run this script.\nWhich arguments were passed to the script.\nWhat are the designated output dirs/files.\nPerhaps even summaries of the output (we won’t do this here).\n\nAll of this can help with troubleshooting and record-keeping3. Let’s try this with our FastQC script:\n#!/bin/bash\nset -euo pipefail\n\n# Load the OSC module for FastQC\nmodule load fastqc\n\n# Copy the placeholder variables\nfastq_file=$1\noutdir=$2\n\n# Initial reporting\necho \"# Starting script fastqc.sh\"\ndate\necho \"# Input FASTQ file:   $fastq_file\"\necho \"# Output dir:         $outdir\"\necho\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Run FastQC\nfastqc --outdir \"$outdir\" \"$fastq_file\"\n\n# Final reporting\necho\necho \"# Done with script fastqc.sh\"\ndate\nA couple of notes about the lines that were added to the script above:\n\nWe printed a “marker line” Done with script that indicates the end of the script was reached. This is handy due to our set settings: seeing this line printed means that no errors were encountered.\nRunning date at the beginning and end of the script is one way to check the running time of the script.\nPrinting the input files can be particularly useful for troubleshooting.\nThe lines that only have echo will simply print a blank line, basically as a separator between sections.\n\n Create a script to run FastQC:\ntouch scripts/fastqc.sh\nOpen it and paste in the code in the box above."
  },
  {
    "objectID": "shell/04_cli-tools.html#a-runner-script",
    "href": "shell/04_cli-tools.html#a-runner-script",
    "title": "Running command-line (CLI) tools with shell scripts",
    "section": "4 A “runner” script",
    "text": "4 A “runner” script\n\n4.1 Running our FastQC script for 1 file\nLet’s run our FastQC script for one FASTQ file:\nbash scripts/fastqc.sh garrigos_data/fastq/ERR10802863_R1.fastq.gz results/fastqc\n# Starting script fastqc.sh\nWed Mar 27 21:53:13 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\nApprox 20% complete for ERR10802863_R1.fastq.gz\nApprox 25% complete for ERR10802863_R1.fastq.gz\nApprox 30% complete for ERR10802863_R1.fastq.gz\nApprox 35% complete for ERR10802863_R1.fastq.gz\nApprox 40% complete for ERR10802863_R1.fastq.gz\nApprox 45% complete for ERR10802863_R1.fastq.gz\nApprox 50% complete for ERR10802863_R1.fastq.gz\nApprox 55% complete for ERR10802863_R1.fastq.gz\nApprox 60% complete for ERR10802863_R1.fastq.gz\nApprox 65% complete for ERR10802863_R1.fastq.gz\nApprox 70% complete for ERR10802863_R1.fastq.gz\nApprox 75% complete for ERR10802863_R1.fastq.gz\nApprox 80% complete for ERR10802863_R1.fastq.gz\nApprox 85% complete for ERR10802863_R1.fastq.gz\nApprox 90% complete for ERR10802863_R1.fastq.gz\nApprox 95% complete for ERR10802863_R1.fastq.gz\nApprox 100% complete for ERR10802863_R1.fastq.gz\nAnalysis complete for ERR10802863_R1.fastq.gz\n\n# Done with script fastqc.sh\nWed Mar 27 21:53:19 EDT 2024\nHowever, as discussed above, we’ll want to run the script for each FASTQ file. We’ll have to write a loop to do so, and that loop will go in a “runner script”.\n\n\n\n4.2 What are runner scripts and why do we need them\nThe loop code could be directly typed in the terminal, but it’s better to save this in a file/script as well.\nWe will now create such a file, which has the overall purpose of documenting the steps we took. You can think of this file as akin to an analysis lab notebook4. Because it will contain shell code, we will save it as a shell script (.sh) just like the script to run fastqc.sh and other individual analysis steps.\nHowever, it is important to realize that this script is conceptually different from the scripts that run individual steps of your analysis. The latter are meant to be run/submitted in their entirety by the runner script, whereas commands in the former typically have to be run one-by-one, i.e. interactively. This kind of script is sometimes called a “runner” or “master” script.\nTo summarize, we’ll separate our code into two hierarchical levels of scripts:\n\nScripts that run individual steps of your analysis, like fastqc.sh. These will eventually be submitted a batch jobs.\nAn overarching “runner” script with code that we run interactively, to orchestrates batch job submission of the individual steps.\n\nTo make this division clearer, we’ll also save these scripts in separate directories:\n\nscripts for the analysis scripts.\nrun for the runner script(s).\n\n\n\n\n\n\n\n\nKeep the scripts for individual steps simple (Click to expand)\n\n\n\n\n\nIt is a good idea to keep the shell scripts you will submit (e.g., fastqc.sh) simple in the sense that they should generally just run one program, and not a sequence of programs.\nOnce you get the hang of writing these scripts, it may seem appealing to string a series of programs/steps together in a single script, so that it’s easier to rerun everything at once — but in practice, that will often end up leading to more difficulties than convenience. If you do want to develop a workflow that can be easily run and rerun from start to finish, you should learn a workflow management system like Snakemake or Nextflow — see the Nextflow tutorials.\n\n\n\n\n\n\n\n\n\nWhy the runner script generally can’t itself be run at once in its entirety (Click to expand)\n\n\n\n\n\nFirst off, not that this applies only once we start submitting our scripts as batch jobs.\nOnce we’ve added multiple batch job steps, and the input of a later step uses the output of an earlier step, we won’t be able to just run the script as is. This is because the runner script would then submit jobs from different steps all at once, and that later step would start running before the earlier step has finished.\nFor example, consider the following series of two steps, in which the second step uses the output of the first step:\n# This script would create a genome \"index\" for STAR, that will be used in the next step\n# ('my_genome.fa' = input genome FASTA, 'results/star_index' = output index dir)\nsbatch scripts/star_index.sh my_genome.fa results/star_index\n\n# This script would align a FASTQ file to the genome index created in the previous step\n# ('results/star_index' = input index dir, 'sampleA.fastq.gz' = input FASTQ file,\n# 'results/star_align' = output dir)\nsbatch scripts/star_align.sh results/star_index sampleA.fastq.gz results/star_align \nIf these two lines were included in your runner script, and you would run that script in its entirety all at once, the script in the second step would be submitted just a split-second after the first one (when using sbatch, you get your prompt back immediately – there is no waiting). As such, it would fail because of the missing output from the first step.\nIt is possible to make sbatch batch jobs wait for earlier steps to finish (e.g. with the --dependency option), but this quickly gets tricky. If you want to create a workflow/pipeline that can run from start to finish in an automated way, you should consider using a workflow management system like Snakemake or NextFlow — see the Nextflow tutorials!\n\n\n\n\n\n\n4.3 Creating our runner script\nCreate a new file, and open it after running these commands:\nmkdir run\ntouch run/run.sh\nIn this script, add the code that runs our fastqc.sh script for each FASTQ file, and then run that code:\n# Run FastQC for each FASTQ file\nfor fastq_file in garrigos_data/fastq/*fastq.gz; do\n    bash scripts/fastqc.sh \"$fastq_file\" results/fastqc\ndone\n# Starting script fastqc.sh\nThu Mar 21 10:06:46 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\nApprox 20% complete for ERR10802863_R1.fastq.gz\nApprox 25% complete for ERR10802863_R1.fastq.gz\nApprox 30% complete for ERR10802863_R1.fastq.gz\nApprox 35% complete for ERR10802863_R1.fastq.gz\nApprox 40% complete for ERR10802863_R1.fastq.gz\nApprox 45% complete for ERR10802863_R1.fastq.gz\nApprox 50% complete for ERR10802863_R1.fastq.gz\nApprox 55% complete for ERR10802863_R1.fastq.gz\nApprox 60% complete for ERR10802863_R1.fastq.gz\nApprox 65% complete for ERR10802863_R1.fastq.gz\nApprox 70% complete for ERR10802863_R1.fastq.gz\nApprox 75% complete for ERR10802863_R1.fastq.gz\nApprox 80% complete for ERR10802863_R1.fastq.gz\nApprox 85% complete for ERR10802863_R1.fastq.gz\nApprox 90% complete for ERR10802863_R1.fastq.gz\nApprox 95% complete for ERR10802863_R1.fastq.gz\nApprox 100% complete for ERR10802863_R1.fastq.gz\nAnalysis complete for ERR10802863_R1.fastq.gz\n\n# Done with script fastqc.sh\nThu Mar 21 10:06:51 EDT 2024\n\n# Starting script fastqc.sh\nThu Mar 21 10:06:51 EDT 2024\n# Input FASTQ file:   garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output dir:         results/fastqc\n\nStarted analysis of ERR10802863_R2.fastq.gz\nApprox 5% complete for ERR10802863_R2.fastq.gz\nApprox 10% complete for ERR10802863_R2.fastq.gz\n# [...output truncated...]"
  },
  {
    "objectID": "shell/04_cli-tools.html#looping-over-samples-rather-than-files",
    "href": "shell/04_cli-tools.html#looping-over-samples-rather-than-files",
    "title": "Running command-line (CLI) tools with shell scripts",
    "section": "5 Looping over samples rather than files",
    "text": "5 Looping over samples rather than files\nIn some cases, we can’t simply loop over all files like we have done so far. For example, in many tools that process paired-end FASTQ files, the corresponding R1 and R2 files for each sample must be processed together. That is, we don’t run the tool separately for each FASTQ file, but separately for each sample i.e. each pair of FASTQ files.\nHow can we loop over pairs of FASTQ files instead? There are two main ways:\n\nCreate a list of sample IDs, loop over these IDs, and find the pair of FASTQ files with matching names.\nLoop over the R1 files only and then infer the name of the corresponding R2 file within the loop. This is generally straightforward because the file names should be identical other than the read-direction identifier (R1/R2).\n\nBelow, we will use the second method — but first, we’ll recap/learn a few prerequisites.\n\n\n5.1 Recap of basename, and dirname\nRunning the basename command on a filename will strip any directories in its name:\nbasename garrigos_data/fastq/ERR10802863_R1.fastq.gz\nERR10802863_R1.fastq.gz\nYou can also provide any arbitrary suffix to also strip from the file name:\nbasename garrigos_data/fastq/ERR10802863_R1.fastq.gz .fastq.gz\nERR10802863_R1\nIf you instead want the directory part of the path, use the dirname command:\ndirname garrigos_data/fastq/ERR10802863_R1.fastq.gz\ngarrigos_data/fastq\n\n\n\n5.2 Parameter expansion\nYou can use so-called “parameter expansion”, with parameter basically being another word for variable, to search-and-replace text in your variable’s values. For example:\n\nAssign a short DNA sequence to a variable:\ndna_seq=\"AAGTTCAT\"\necho \"$dna_seq\"\nAAGTTCAT\nUse parameter expansion to replace all Ts with U to convert the DNA to RNA:\necho \"${dna_seq//T/U}\"\nAAGUUCAT\nYou can also assign the result of the parameter expansion back to a variable:\nrna_seq=\"${dna_seq//T/U}\"\necho \"$rna_seq\"\nAAGUUCAT\n\nSo, the syntax for this type of parameter expansion is {var_name//&lt;search&gt;/replace} — let’s deconstruct that:\n\nReference the variable, using the full notation, with braces (${var_name})\nAfter the first two forward slashes, enter the search pattern: (T)\nAfter another forward slash, enter the replacement: (U).\n\nIf you needed to replace at most one of the search patterns, a single backslash after the variable name would suffice: {var_name/&lt;search&gt;/replace}.\n\n\n Exercise: Get the R2 file name with parameter expansion\nFile names of corresponding R1 and R2 FASTQ files should be identical other than the marker indicating the read direction, which is typically R1/R2 (and in some cases just 1 and 2).\nAssign the file name garrigos_data/fastq/ERR10802863_R1.fastq.gz to a variable and use parameter expansion to get the name of the corresponding R2 file name. Also save the R2 file name in a variable.\n\n\nSolutions\n\nfastq_R1=garrigos_data/fastq/ERR10802863_R1.fastq.gz\nfastq_R2=${fastq_R1/_R1/_R2}\nAbove, e.g. ${fastq_R1/R1/R2}, that is without underscores, would have also worked. But note that it’s generally good to avoid unwanted search-pattern matches by making the search string as specific as possible. So perhaps ${fastq_R1/_R1.fastq.gz/_R2.fastq.gz} would have been even better.\nTest that it worked:\necho \"$fastq_R2\"\ngarrigos_data/fastq/ERR10802863_R2.fastq.gz\n\n\n\n\n\n5.3 A per-sample loop\nWe will now create a loop that:\n\nLoops over R1 FASTQ files only, and then infers the corresponding R2 file name.\nDefines output file names that are the same as the input file names but in a different dir.\n\nTo stay focused just on the shell syntax here, we won’t include code to run an actual bioinformatics tool, but will use a fictional tool trimmer:\n# [Don't run or copy this]\n\n# Loop over the R1 files - our glob is `*_R1.fastq.gz` to only select R1 files\nfor R1_in in garrigos_data/fastq/*_R1.fastq.gz; do\n    # Get the R2 file name with parameter expansion\n    R2_in=${R1_in/_R1/_R2}\n    \n    # Define the output files (assume that a variable $outdir exists)\n    R1_out=\"$outdir\"/$(basename \"$R1_in\")\n    R2_out=\"$outdir\"/$(basename \"$R2_in\")\n    \n    # Report\n    echo \"Input files: $R1_in $R2_in\"\n    echo \"Output files: $R1_out $R2_out\"\n    \n    # Use the imaginary program 'trimmer' with options --in1/--in2 for the R1/R2 input files,\n    # and --out1/--out2 for the R1/R2 output files:\n    trimmer --in1 \"$R1_in\" --in2 \"$R2_in\" --out1 \"$R1_out\" --out2 \"$R2_out\"\ndone\n\n\n\n5.4 Converting to the single-sample script format\nBut wait! Aren’t we supposed to write a script that only processes one sample at a time, and then run/submit that script with a loop? That’s right, so now that we know what to do, let’s switch to that setup.\nCreate a new script scripts/trim_mock.sh and paste the following code into it:\n#!/bin/bash\nset -euo pipefail\n\n# Copy the placeholder variables\nR1_in=$1\noutdir=$2\n\n# Create the output dir if needed\nmkdir -p \"$outdir\"\n\n# Infer the R2_in file name\nR2_in=${R1_in/_R1/_R2}\n    \n# Define the output file names\nR1_out=\"$outdir\"/$(basename \"$R1_in\")\nR2_out=\"$outdir\"/$(basename \"$R2_in\")\n\n# Initial reporting\necho \"# Starting script trim_mock.sh\"\ndate\necho \"# Input R1 file:       $R1_in\"\necho \"# Input R2 file:       $R2_in\"\necho \"# Output R1 file:      $R1_out\"\necho \"# Output R2 file:      $R2_out\"\necho\n\n# Mock-run the tool: I preface the command with 'echo' so this will just report\n# and not try to run a program that doesn't exist\necho trimmer --in1 \"$R1_in\" --in2 \"$R2_in\" --out1 \"$R1_out\" --out2 \"$R2_out\"\n\n# Final reporting\necho\necho \"# Done with script trim_mock.sh\"\ndate\nNow, add the following code to our run.sh script, and run that:\n# Run the trim_mock.sh script for each sample\nfor R1_in in garrigos_data/fastq/*_R1.fastq.gz; do\n    bash scripts/trim_mock.sh \"$R1_in\" results/trim_mock\ndone\n# Starting script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n# Input R1 file:       garrigos_data/fastq/ERR10802863_R1.fastq.gz\n# Input R2 file:       garrigos_data/fastq/ERR10802863_R2.fastq.gz\n# Output R1 file:      results/trim_mock/ERR10802863_R1.fastq.gz\n# Output R2 file:      results/trim_mock/ERR10802863_R2.fastq.gz\n\ntrimmer --in1 garrigos_data/fastq/ERR10802863_R1.fastq.gz --in2 garrigos_data/fastq/ERR10802863_R2.fastq.gz --out1 results/trim_mock/ERR10802863_R1.fastq.gz --out2 results/trim_mock/ERR10802863_R2.fastq.gz\n\n# Done with script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n\n# Starting script trim_mock.sh\nThu Mar 21 10:12:17 EDT 2024\n# Input R1 file:       garrigos_data/fastq/ERR10802864_R1.fastq.gz\n# Input R2 file:       garrigos_data/fastq/ERR10802864_R2.fastq.gz\n# Output R1 file:      results/trim_mock/ERR10802864_R1.fastq.gz\n# Output R2 file:      results/trim_mock/ERR10802864_R2.fastq.gz\n# [...output truncated...]\nCheck the files in the output dir:\nls -lh results/trim_mock\ntotal 0\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802863_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802863_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802864_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802864_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802865_R1.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802865_R2.fastq.gz\n-rw-rw----+ 1 jelmer PAS0471 0 Mar 21 10:12 ERR10802866_R1.fastq.gz\n# [...output truncated...]"
  },
  {
    "objectID": "shell/04_cli-tools.html#footnotes",
    "href": "shell/04_cli-tools.html#footnotes",
    "title": "Running command-line (CLI) tools with shell scripts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a full list of installed software at OSC: https://www.osc.edu/resources/available_software/software_list.↩︎\n Find out more about loading (and installing) software at OSC in this tutorial.↩︎\nWe’ll see in the upcoming Slurm module that we when submit scripts to the OSC queue (rather than running them directly), the output of scripts that is normally printed to screen, will instead go to a sort of “log” file. So, your script’s reporting will end up in this file.↩︎\n Or depending on how you use this exactly, as your notebook entry that contains the final protocol you followed.↩︎"
  },
  {
    "objectID": "shell/05_bonus.html#while-loops",
    "href": "shell/05_bonus.html#while-loops",
    "title": "Shell script bonus: while loops, arrays and more",
    "section": "1 While loops",
    "text": "1 While loops\nIn bash, while loops are mostly useful in combination with the read command, to loop over each line in a file. If you use while loops, you’ll very rarely need Bash arrays (next section), and conversely, if you like to use arrays, you may not need while loops much.\nwhile loops will run as long as a condition is true and this condition can include constructs such as read -r which will read input line-by-line, and be true as long as there is a line left to be read from the file. In the example below, while read -r will be true as long as lines are being read from a file fastq_files.txt — and in each iteration of the loop, the variable $fastq_file contains one line from the file:\n# [ Don't run this - hypothetical example]\ncat fastq_files.txt\nseq/zmaysA_R1.fastq\nseq/zmaysA_R2.fastq\nseq/zmaysB_R1.fastq\n# [ Don't run this - hypothetical example]\ncat fastq_files.txt | while read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq\nA more elegant but perhaps confusing syntax variant used input redirection instead of cat-ing the file:\n# [ Don't run this - hypothetical example]\nwhile read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; fastq_files.txt\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq\nWe can also process each line of the file inside the while loop, like when we need to select a specific column:\n# [ Don't run this - hypothetical example]\nhead -n 2 samples.txt\nzmaysA  R1      seq/zmaysA_R1.fastq\nzmaysA  R2      seq/zmaysA_R2.fastq\n# [ Don't run this - hypothetical example]\nwhile read -r my_line; do\n    echo \"Have read line: $my_line\"\n    fastq_file=$(echo \"$my_line\" | cut -f 3)\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; samples.txt\nHave read line: zmaysA  R1      seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R1.fastq\nHave read line: zmaysA  R2      seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysA_R2.fastq\nAlternatively, you can operate on file contents before inputting it into the loop:\n# [ Don't run this - hypothetical example]\nwhile read -r fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; &lt;(cut -f 3 samples.txt)\nFinally, you can extract columns directly as follows:\n# [ Don't run this - hypothetical example]\nwhile read -r sample_name readpair_member fastq_file; do\n    echo \"Processing file: $fastq_file\"\n    # More processing...\ndone &lt; samples.txt\nProcessing file: seq/zmaysA_R1.fastq\nProcessing file: seq/zmaysA_R2.fastq\nProcessing file: seq/zmaysB_R1.fastq"
  },
  {
    "objectID": "shell/05_bonus.html#arrays",
    "href": "shell/05_bonus.html#arrays",
    "title": "Shell script bonus: while loops, arrays and more",
    "section": "2 Arrays",
    "text": "2 Arrays\nBash “arrays” are basically lists of items, such as a list of file names or samples IDs. If you’re familiar with R, they are like R vectors1.\nArrays are mainly used with for loops: you create an array and then loop over the individual items in the array. This usage represents an alternative to looping over files with a glob. Looping over files with a glob is generally easier and preferable, but sometimes this is not the case; or you are looping e.g. over samples and not files.\n\nCreating arrays\nYou can create an array “manually” by typing a space-delimited list of items between parentheses:\n# The array will contain 3 items: 'zmaysA', 'zmaysB', and 'zmaysC'\nsample_names=(zmaysA zmaysB zmaysC)\nMore commonly, you would populate an array from a file, in which case you also need command substitution:\n\nSimply reading in an array from a file with cat will only work if the file simply contains a list of items:\nsample_files=($(cat fastq_files.txt))\nFor tabular files, you can include e.g. a cut command to extract the focal column:\nsample_files=($(cut -f 3 samples.txt))\n\n\n\n\n\n\n\n\nAlternatively, use the mapfile command\n\n\n\nTODO\n\n\n\n\n\nAccessing arrays\nFirst off, it is useful to realize that arrays are closely related to regular variables, and to recall that the “full” notation to refer to a variable includes curly braces: ${myvar}. When referencing arrays, the curly braces are always needed.\n\nUsing [@], we can access all elements in the array (and arrays are best quoted, like regular variables):\necho \"${sample_names[@]}\"\nzmaysA zmaysB zmaysC\nWe can also use the [@] notation to loop over the elements in an array:\nfor sample_name in \"${sample_names[@]}\"; do\n    echo \"Processing sample: $sample_name\"\ndone\nProcessing sample: zmaysA\nProcessing sample: zmaysB\nProcessing sample: zmaysC\n\n\n\n\n\n\n\n\nOther array operations (Click to expand)\n\n\n\n\n\n\nExtract specific elements (note: Bash arrays are 0-indexed!):\n# Extract the first item\necho ${sample_names[0]}\nzmaysA\n# Extract the third item\necho ${sample_names[2]}\nzmaysC\nCount the number of elements in the array:\necho ${#sample_names[@]}\n3\n\n\n\n\n\n\n\nArrays and filenames with spaces\nThe file files.txt contains a short list of file names, the last of which has a space in it:\ncat files.txt\nfile_A\nfile_B\nfile_C\nfile D\nWhat will happen if we read this list into an array, and then loop over the array?\n# Populate an array with the list of files from 'files.txt'\nall_files=($(cat files.txt))\n\n# Loop over the array:\nfor file in \"${all_files[@]}\"; do\n    echo \"Current file: $file\"\ndone\nCurrent file: file_A\nCurrent file: file_B\nCurrent file: file_C\nCurrent file: file\nCurrent file: D\nUh-oh! The file name with the space in it was split into two items! And note that we did quote the array in \"${all_files[@]}\", so clearly, this doesn’t solve that problem.\nFor this reason, it’s best not to use arrays to loop over filenames with spaces (though there are workarounds). Direct globbing and while loops with the read function (while read ..., see below) are easier choices for problematic file names.\nAlso, this example once again demonstrates you should not have spaces in your file names!\n\n\n\n Exercise: Bash arrays\n\nCreate an array with the first three file names (lines) listed in samples.txt.\nLoop over the contents of the array with a for loop.\nInside the loop, create (touch) the file listed in the current array element.\nCheck whether you created your files.\n\n\n\nSolutions\n\n\nCreate an array with the first three file names (lines) listed in samples.txt.\n\ngood_files=($(head -n 3 files.txt))\n\nLoop over the contents of the array with a for loop.\nInside the loop, create (touch) the file listed in the current array element.\nfor good_file in \"${good_files[@]}\"; do\n    touch \"$good_file\"\ndone\nCheck whether you created your files.\nls\nfile_A  file_B  file_C"
  },
  {
    "objectID": "shell/05_bonus.html#miscellaneous",
    "href": "shell/05_bonus.html#miscellaneous",
    "title": "Shell script bonus: while loops, arrays and more",
    "section": "3 Miscellaneous",
    "text": "3 Miscellaneous\n\n3.1 More on the && and || operators\nAbove, we saw that we can combine tests in if statements with && and ||. But these shell operators can be used to chain commands together in a more general way, as shown below.\n\nOnly if the first command succeeds, also run the second:\n# Move into the data dir and if that succeeds, then list the files there:\ncd data && ls data\n# Stage all changes =&gt; commit them =&gt; push the commit to remote:\ngit add --all && git commit -m \"Add README\" && git push\nOnly if the first command fails, also run the second:\n# Exit the script if you can't change into the output dir:\ncd \"$outdir\" || exit 1\n# Only create the directory if it doesn't already exist:\n[[ -d \"$outdir\" ]] || mkdir \"$outdir\"\n\n\n\n\n3.2 Parameter expansion to provide default values\nIn scripts, it may be useful to have optional arguments that have a default value if they are not specified on the command line. You can use the following “parameter expansion” syntax for this.\n\nAssign the value of $1 to number_of_lines unless $1 doesn’t exist: in that case, set it to a default value of 10:\nnumber_of_lines=${1:-10}\nSet true as the default value for $3:\nremove_unpaired=${3:-true}\n\nAs a more worked out example, say that your script takes an input dir and an output dir as arguments. But if the output dir is not specified, you want it to be the same as the input dir. You can do that like so:\ninput_dir=$1\noutput_dir=${2:-$input_dir}\nNow you can call the script with or without the second argument, the output dir:\n# Call the script with 2 args: input and output dir\nsort_bam.sh results/bam results/bam\n# Call the script with 1 arg: input dir (which will then also be the output dir)\nsort_bam.sh results/bam\n\n\n\n3.3 Standard output and standard error\nAs you’ve seen, when commands run into errors, they will print error messages. Error messages are not part of “standard out”, but represent a separate output stream: “standard error”.\nWe can see this when we try to list a non-existing directory and try to redirect the output of the ls command to a file:\nls -lhr solutions/ &gt; solution_files.txt \nls: cannot access solutions.txt: No such file or directory\nEvidently, the error was printed to screen rather than redirected to the output file. This is because &gt; only redirects standard out, and not standard error. Was anything at all printed to the file?\ncat solution_files.txt\n# We just get our prompt back - the file is empty\nNo, because there were no files to list, only an error to report.\nThe figure below draws the in- and output streams without redirection (a) versus with &gt; redirection (b):\n\n\n\nFigure from Buffalo.\n\n\nTo redirect the standard error, use 2&gt; 2:\nls -lhr solutions/ &gt; solution_files.txt 2&gt; errors.txt\nTo combine standard out and standard error, use &&gt;:\n# (&&gt; is a bash shortcut for 2&gt;&1)\nls -lhr solutions/ &&gt; out.txt\ncat out.txt\nls: cannot access solutions.txt: No such file or directory\nFinally, if you want to “manually” designate an echo statement to represent standard error instead of standard out in a script, use &gt;&2:\necho \"Error: Invalid line number\" &gt;&2\necho \"Number should be &gt;0 and &lt;= the file's nr. of lines\" &gt;&2\necho \"File contains $(wc -l &lt; $2) lines; you provided $1.\" &gt;&2\nexit 1"
  },
  {
    "objectID": "shell/05_bonus.html#footnotes",
    "href": "shell/05_bonus.html#footnotes",
    "title": "Shell script bonus: while loops, arrays and more",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Or if you’re familiar with Python, they are like Python lists.↩︎\n Note that 1&gt; is the full notation to redirect standard out, and the &gt; we’ve been using is merely a shortcut for that.↩︎"
  },
  {
    "objectID": "osc/03_osc2.html",
    "href": "osc/03_osc2.html",
    "title": "A closer look at OSC",
    "section": "",
    "text": "In this short session, we will touch on some additional aspects of the Ohio Supercomputer Center (OSC)."
  },
  {
    "objectID": "osc/03_osc2.html#self-study-material-more-on-file-transfer",
    "href": "osc/03_osc2.html#self-study-material-more-on-file-transfer",
    "title": "A closer look at OSC",
    "section": "Self-study material: More on file transfer",
    "text": "Self-study material: More on file transfer\n\nRemote transfer commands\nFor small transfers, an alternative to the OnDemand Files menu is using remote transfer commands like scp, rsync, or rclone. These commands can be more convenient than OnDemand especially if you want to keep certain directories synced between OSC and your computer.\nBecause at OSC, these transfers will happen using a login node, these commands are unfortunately not recommended for large transfers3.\n\nscp\nOne option is scp (secure copy), which works much like the regular cp command, including the need for the -r option for recursive transfers. The key difference is that we have to somehow refer to a path on a remote computer, and we do so by starting with the remote computer’s address, followed by :, and then the path:\n# Copy from remote (OSC) to local (your computer):\nscp &lt;user&gt;@pitzer.osc.edu:&lt;remote-path&gt; &lt;local-path&gt;\n\n# Copy from local (your computer) to remote (OSC)\nscp &lt;local-path&gt; &lt;user&gt;@pitzer.osc.edu:&lt;remote-path&gt;\nHere are two examples of copying from OSC to your local computer:\n# Copy a file from OSC to a local computer - namely, to your current working dir ('.'):\nscp jelmer@pitzer.osc.edu:/fs/ess/PAS0471/jelmer/mcic-scripts/misc/fastqc.sh .\n\n# Copy a directory from OSC to a local computer - namely, to your home dir ('~'):\nscp -r jelmer@pitzer.osc.edu:/fs/ess/PAS0471/jelmer/mcic-scripts ~\nAnd two examples of copying from your local computer to OSC:\n# Copy a file from your computer to OSC --\n# namely, a file in from your current working dir to your home dir at OSC:\nscp fastqc.sh jelmer@pitzer.osc.edu:~\n\n# Copy a file from my local computer's Desktop to the Scratch dir for PAS0471:\nscp /Users/poelstra.1/Desktop/fastqc.sh jelmer@pitzer.osc.edu:/fs/scratch/PAS0471\nSome nuances for remote copying:\n\nFor both transfer directions (remote-to-local and local-to-remote), you issue the copying commands from your local computer.\nThe path for the remote computer (OSC) should always be absolute but that for your local computer can be relative or absolute.\nSince all files can be accessed at the same paths at Pitzer and at Owens, it doesn’t matter whether you use @pitzer.osc.edu or @owens.osc.edu in the scp command.\n\n\n\n\n\n\n\nTransferring directly to and from OneDrive (Click to expand)\n\n\n\n\n\nIf your OneDrive is mounted on or synced to your local computer (i.e., if you can see it in your computer’s file brower), you can also transfer directly between OSC and OneDrive. For example, the path to my OneDrive files on my laptop is:\n/Users/poelstra.1/Library/CloudStorage/OneDrive-TheOhioStateUniversity.\nSo if I had a file called fastqc.sh in my top-level OneDrive dir, I could transfer it to my Home dir at OSC as follows:\nscp /Users/poelstra.1/Library/CloudStorage/OneDrive-TheOhioStateUniversity jelmer@pitzer.osc.edu:~\n\n\n\n\n\n\nrsync\nAnother option, which I can recommend, is the rsync command, especially when you have directories that you repeatedly want to sync: rsync won’t copy any files that are identical in source and destination.\nA useful combination of options is -avz --progress:\n\n-a enables archival mode (among other things, this makes it work recursively).\n-v increases verbosity — tells you what is being copied.\n-z enables compressed file transfer (=&gt; generally faster).\n--progress to show transfer progress for individual files.\n\nThe way to refer to remote paths is the same as with scp. For example, I could copy a dir_with_results in my local Home dir to my OSC Home dir as follows:\nrsync -avz --progress ~/dir_with_results jelmer@owens.osc.edu:~\n\n\n\n\n\n\n\nTrailing slashes in rsync (Click to expand)\n\n\n\n\n\nOne tricky aspect of using rsync is that the presence/absence of a trailing slash for source directories makes a difference for its behavior. The following commands work as intended — to create a backup copy of a scripts dir inside a dir called backup4:\n# With trailing slash: copy the *contents* of source \"scripts\" into target \"scripts\":\nrsync -avz scripts/ backup/scripts\n\n# Without trailing slash: copy the source dir \"scripts\" into target dir \"backup\"\nrsync -avz scripts backup\nBut these commands don’t:\n# This would result in a dir 'backup/scripts/scripts':\nrsync -avz scripts backup/scripts\n\n# This would copy the files in \"scripts\" straight into \"backup\":\nrsync -avz scripts/ backup\n\n\n\n\n\n\n\nSFTP\nThe first of two options for larger transfers is SFTP. You can use the sftp command when you have access to a Unix shell on your computer, and this what I’ll cover below.\n\n\n\n\n\n\nSFTP with a GUI\n\n\n\nIf you have Windows without e.g. WSL or Git Bash, you can use a GUI-based SFTP client instead like WinSCP, Cyberduck, or FileZilla. CyberDuck also works on Mac, and FileZilla works on all operating systems, if you prefer to do SFTP transfers with a GUI, but I won’t cover their usage here.\n\n\n\nLogging in\nTo log in to OSC’s SFTP server, issue the following command in your local computer’s terminal, substituting &lt;user&gt; by your OSC username:\nsftp &lt;user&gt;@sftp.osc.edu   # E.g., 'jelmer@sftp.osc.edu'\nThe authenticity of host 'sftp.osc.edu (192.148.247.136)' can't be established.\nED25519 key fingerprint is SHA256:kMeb1PVZ1XVDEe2QiSumbM33w0SkvBJ4xeD18a/L0eQ.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nIf this is your first time connecting to OSC SFTP server, you’ll get a message like the one shown above: you should type yes to confirm.\nThen, you may be asked for your OSC password, and after that, you should see a “welcome” message like this:\n******************************************************************************\n\nThis system is for the use of authorized users only.  Individuals using\nthis computer system without authority, or in excess of their authority,\nare subject to having all of their activities on this system monitored\nand recorded by system personnel.  In the course of monitoring individuals\nimproperly using this system, or in the course of system maintenance,\nthe activities of authorized users may also be monitored.  Anyone using\nthis system expressly consents to such monitoring and is advised that if\nsuch monitoring reveals possible evidence of criminal activity, system\npersonnel may provide the evidence of such monitoring to law enforcement\nofficials.\n\n******************************************************************************\nConnected to sftp.osc.edu.\nNow, you will have an sftp prompt (sftp&gt;) instead of a regular shell prompt.\nFamiliar commands like ls, cd, and pwd will operate on the remote computer (OSC, in this case), and there are local counterparts for them: lls, lcd, lpwd — for example:\n# NOTE: I am prefacing sftp commands with the 'sftp&gt;' prompt to make it explicit\n#       these should be issued in an sftp session; but don't type that part.\nsftp&gt; pwd\nRemote working directory: /users/PAS0471/jelmer\nsftp&gt; lpwd\nLocal working directory: /Users/poelstra.1/Desktop\n\n\n\nUploading files to OSC\nTo upload files to OSC, use sftp’s put command.\nThe syntax is put &lt;local-path&gt; &lt;remote-path&gt;, and unlike with scp etc., you don’t need to include the address to the remote (because in an stfp session, you are simultaneously connected to both computers). But like with cp and scp, you’ll need the -r flag for recursive transfers, i.e. transferring a directory and its contents.\n# Upload fastqc.sh in a dir 'scripts' on your local computer to the PAS0471 Scratch dir:\nsftp&gt; put scripts/fastqc.sh /fs/scratch/PAS0471/sandbox\n\n# Use -r to transfer directories:\nsftp&gt; put -r scripts /fs/scratch/PAS0471/sandbox\n\n# You can use wildcards to upload multiple files:\nsftp&gt; put scripts/*sh /fs/scratch/PAS0471/sandbox\n\n\n\n\n\n\nsftp is rather primitive\n\n\n\nThe ~ shortcut to your Home directory does not work in sftp! sftp is generally quite primitive and you also cannot use, for example, tab completion or the recalling of previous commands with the up arrow.\n\n\n\n\n\nDownloading files from OSC\nTo download files from OSC, use the get command, which has the syntax get &lt;remote-path&gt; &lt;local-path&gt; (this is the other way around from put in that the remote path comes first, but the same in that both use the order &lt;source&gt; &lt;target&gt;, like cp and so on).\nFor example:\nsftp&gt; get /fs/scratch/PAS0471/mcic-scripts/misc/fastqc.sh .\n\nsftp&gt; get -r /fs/scratch/PAS0471/sandbox/ .\n\n\nClosing the SFTP connection\nWhen you’re done, you can type exit or press Ctrl+D to exit the sftp prompt.\n\n\n\n\nGlobus\nThe second option for large transfers is Globus, which has a browser-based GUI, and is especially your best bet for very large transfers. Some advantages of using Globus are that:\n\nIt checks whether all files were transferred correctly and completely\nIt can pause and resume automatically when you e.g. turn off your computer for a while\nIt can be used to share files from OSC directly with collaborators even at different institutions.\n\nGlobus does need some setup, including the installation of a piece of software that will run in the background on your computer.\n\nGlobus installation and configuration instructions: Windows / Mac / Linux\nGlobus transfer instructions\nOSC’s page on Globus"
  },
  {
    "objectID": "osc/03_osc2.html#footnotes",
    "href": "osc/03_osc2.html#footnotes",
    "title": "A closer look at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n And file sharing / collaborating is also a bit more difficult with home dirs.↩︎\n Copying of files back-and-forth, and making sure your results are not lost upon some kind of failure.↩︎\n This may be different at other supercomputer centers: there are no inherent transfer size limitations to these commands.↩︎\nFor simplicity, these commands are copying between local dirs, which is also possible with rsync.↩︎"
  },
  {
    "objectID": "osc/04_software.html#loading-software-at-osc-with-lmod-modules",
    "href": "osc/04_software.html#loading-software-at-osc-with-lmod-modules",
    "title": "Using software at OSC",
    "section": "1 Loading software at OSC with Lmod modules",
    "text": "1 Loading software at OSC with Lmod modules\nOSC administrators manage software with the “Lmod” system of software modules. For us users, this means that even though a lot of software is installed, most of it can only be used after we explicitly load it. That may seem like a drag, but on the upside, this practice enables the use of different versions of the same software, and of mutually incompatible software on a single system.\nWe can load, unload, and search for available software modules using the module command and its sub-commands.\n\n1.1 Checking whether a program is available\nThe OSC website has a list of installed software. You can also search for available software in the shell using two subtly different module commands:\n\nmodule spider lists all installed modules.\nmodule avail lists modules that can be directly loaded given the current environment (i.e., taking into account which other software has been loaded).\n\nSimply running module spider or module avail spits out complete lists of installed/available programs — it is more useful to add a search term as an argument. Below, we’ll search for the Conda distribution “miniconda”:\nmodule spider miniconda\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  miniconda3:\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        miniconda3/4.10.3-py37\n        miniconda3/4.12.0-py38\n        miniconda3/4.12.0-py39\n        miniconda3/23.3.1-py310\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"miniconda3\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider miniconda3/4.12.0-py39\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nmodule avail miniconda\n------------------------------------------------------------------------------------------------------ /apps/lmodfiles/Core -------------------------------------------------------------------------------------------------------\n   miniconda3/4.10.3-py37 (D)    miniconda3/4.12.0-py38    miniconda3/4.12.0-py39    miniconda3/23.3.1-py310\n\n  Where:\n   D:  Default Module\nAs stated at the bottom of the output below, the (D) in the module avail output above marks the default version of the program: this is the version of the program that will be loaded if we don’t specify a version ourselves (see examples below). The module spider command does not provide this information.\n\n\n\n\n\n\nBoth of these search commands are case-insensitive, but module load (below) is not\n\n\n\n\n\n\n\n\n\n1.2 Loading and unloading software\nAll other Lmod software functionality is also accessed using module commands. For instance, to make a program available to you, use the load command:\n# Load a module:\nmodule load miniconda3               # Load the default version\nmodule load miniconda3/23.3.1-py310  # Load a specific version\n\n\n\n\n\n\nIt can be good to specify the version even when you want the default (Click to expand)\n\n\n\n\n\nThis is especially true inside a shell script — when using the module load command, specifying a version would:\n\nEnsure that when you run the same script a year later, the same version would be used (assuming it hasn’t been removed) — otherwise, it’s possible a newer version would has been installed in the meantime, which might produce different results.\nMake it easy to see which version you used, which is something you typically want to know and report in your paper.\n\n\n\n\n\n\n\n\n\n\nModules do not remain loaded across separate shell sessions\n\n\n\nModule loading does not persist across shell sessions. Whenever you get a fresh shell session (including but not limited to after logging into OSC again), you will have to reload any modules you want to use!\n\n\n\nTo check which modules are loaded, use module list. Its output also includes automatically loaded modules — for example, after loading miniconda3/23.3.1-py310, it should list miniconda3 as the 9th entry2:\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest               3) intel/19.0.5     5) modules/sp2020     7) git/2.18.0              9) miniconda3/23.3.1-py310\n  2) gcc-compatibility/8.4.0   4) mvapich2/2.3.3   6) project/ondemand   8) app_code_server/4.8.3\nOccasionally, when you run into conflicting (mutually incompatible) modules, it can be useful to unload modules, which you can do with module unload or module purge:\nmodule unload miniconda3    # Unload a specific module\nmodule purge                # Unload all modules\n\n\n\n\n\n\n\nAlways include the module load command in your shell script\n\n\n\nWhen you run a program that is loaded with Lmod in your shell script, always include the module load command in the script, and it is best to do so way at the top of the script:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load fastqc/0.11.8\n\n\n\n\n Exercise: Load a BLAST module\nBLAST is a very widely used alignment tool, often used to identify sequences that are similar to a query sequence. There is not just a web version on NCBI’s website, but also a BLAST command-line tool.\n\nUse module avail to check if BLAST is installed at OSC, and if so, which versions. (Note: you’ll also see results for the related module blast-database — ignore those.)\nLoad the default BLAST version by not specifying a version, and then check which version was loaded and if that matches the module avail output.\nLoad the latest version of BLAST without unloading the earlier version first. What output do you get?\n\n\n\nClick here for the solutions\n\n\nCheck the BLAST modules:\nmodule avail BLAST\n---------------------------------------------------------------------------------- /apps/lmodfiles/Core -- --------------------------------------------------------------------------------\n   blast-database/2018-08 (D)    blast-database/2020-04    blast-database/2022-06    blast/2.8.0+         blast/2.11.0+\n   blast-database/2019-09        blast-database/2021-05    blast-database/2023-06    blast/2.10.0+ (D)    blast/2.13.0+\n\n  Where:\n   D:  Default Module\nLoad the default version:\nmodule load BLAST\nmodule list\nCurrently Loaded Modules:\n  1) xalt/latest               3) intel/19.0.5     5) modules/sp2020     7) git/2.18.0              9) miniconda3/23.3.1-py310\n  2) gcc-compatibility/8.4.0   4) mvapich2/2.3.3   6) project/ondemand   8) app_code_server/4.8.3  10) blast/2.10.0+\nblast/2.10.0+ was loaded, which matches what module avail claimed with its (D) marker for the default version.\nLoad the latest version:\nmodule load blast/2.13.0+\nThe following have been reloaded with a version change:\n  1) blast/2.10.0+ =&gt; blast/2.13.0+\nLmod detected that you tried to load a different version of a software that was already loaded, so it changes the version and tells you about it.\n\n\n\n\n Bonus exercise: STAR and module availability\n\nUse module spider to check which versions of STAR, an RNA-seq read alignment program, have been installed at OSC. Compare this output with that of module avail.\nTry to load the most recent version of STAR that module spider listed (this should fail).\nFollow the instructions in the error message to again try and load OSC’s most recent version of STAR.\nSearch the internet to see what the most recent version of STAR is.\n\n\n\nClick here for the solutions\n\n\nCheck the versions of STAR — it looks like 2.7.9a is installed but we can’t load it for some reason:\nmodule spider star\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n  star:\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n     Versions:\n        star/2.5.2a\n        star/2.7.9a\nmodule avail star\n---------------------------------------------------------------------------------- /apps/lmodfiles/Core    ----------------------------------------------------------------------------------\n   star/2.5.2a\nA first stubborn attempt to load the most recent one:\nmodule load star/2.7.9a\nLmod has detected the following error:  These module(s) exist but cannot be loaded as requested: \"star/2.7.9a\"\n   Try: \"module spider star/2.7.9a\" to see how to load the module(s).\nFollow the instructions in the above error message to try and load it again:\nmodule spider star/2.7.9a\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n  star: star/2.7.9a\n--------------------------------------------------------------------------------------------------------------------------------   ------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"star/2.7.9a\" module is available to load.\n\n      gnu/10.3.0\nmodule load gnu/10.3.0\nLmod is automatically replacing \"intel/19.0.5\" with \"gnu/10.3.0\".\n\nThe following have been reloaded with a version change:\n  1) mvapich2/2.3.3 =&gt; mvapich2/2.3.6\nmodule load star/2.7.9a\nThe last command prints no output, which is generally good news, and indeed, it seems to have worked:\nSTAR --version\n2.7.9a\nMost recent version of STAR:\nAs of March 2024, it looks like that’s version 2.7.11b (https://github.com/alexdobin/STAR)."
  },
  {
    "objectID": "osc/04_software.html#when-software-isnt-installed-at-osc",
    "href": "osc/04_software.html#when-software-isnt-installed-at-osc",
    "title": "Using software at OSC",
    "section": "2 When software isn’t installed at OSC",
    "text": "2 When software isn’t installed at OSC\nIt’s not uncommon that software you need for your project is not installed at OSC, or that you need a more recent version of the software than what is available. In that case, the following two are generally your best options:\n\nConda, which creates software “environments” you can activate much like we did with Lmod modules.\nContainers, which are self-contained software environments that include operating systems, akin to mini virtual machines. Docker containers are most well-known, but OSC uses Apptainer (formerly known as Singularity).\n\nConda and containers are useful not only at OSC, where they bypass issues with dependencies and administrator privileges, but more generally for reproducible software environments. They also make it easy to access different versions of the same software, or use mutually incompatible software.\nIn this session, you will learn how to use Conda, and the self-study reading at the bottom of the page covers using containers.\n\n\n\n\n\n\n\nOther options to install software / get it installed\n\n\n\n\nSend an email to OSC Help. They might be able to help you with your installation, or in case of commonly used software, might be willing to perform a system-wide installation (that is, making it available through Lmod / module commands).\n“Manually” install the software, which in the best case involves downloading a directly functioning binary (executable), but more commonly requires you to “compile” (build) the program. This is sometimes straightforward but can also become extremely tricky, especially at OSC where you don’t have “administrator privileges”3 and will often have difficulties with “dependencies”4."
  },
  {
    "objectID": "osc/04_software.html#conda-basics",
    "href": "osc/04_software.html#conda-basics",
    "title": "Using software at OSC",
    "section": "3 Conda basics",
    "text": "3 Conda basics\nThe Conda software can create so-called environments in which you can install one or more software packages.\nAs you’ll learn below, as long as a program is available in one of the online Conda repositories (and this is nearly always the case for open-source bioinformatics programs), then installing it is quite straightforward, doesn’t require admin privileges, and is done with a procedure that is nearly identical regardless of the program you are installing.\nA Conda environment is “just” a directory that includes the executable (binary) files for the program(s) in question. I have a collection of Conda environments that anyone can use, and we can list these environments simply with ls:\nls /fs/ess/PAS0471/jelmer/conda\nabricate-1.0.1  clonalframeml        kraken2            picard                        salmon\nagat-0.9.1      codan-1.2            kraken-biom        pilon-1.24                    samtools\nalv             cogclassifier        krona              pkgs                          scoary\namrfinderplus   cutadapt             liftoff-1.6.3      plasmidfinder-2.1.6           seqkit\nantismash       deeploc              links-2.0.1        plink2                        seqtk\nariba-2.14.6    deeptmhmm            lissero            porechop                      shoot\nastral-5.7.8    deeptmhmm2           longstitch-1.0.3   prokka                        signalp-6.0\naswcli          diamond              mafft              pseudofinder                  sistr-1.1.1\nbactopia        dwgsim               maskrc-svg         purge_dups-1.2.6              smap\nbactopia3       eggnogmapper         mbar24             pycoqc-2.5.2                  smap_dev\nbactopia-dev    emboss               medaka-1.7.2       qiime2-2023.7                 smartdenovo-env\nbakta           entap-0.10.8         metaxa-2.2.3       qiime2-amplicon-2024.2        snippy-4.6.0\nbase            entrez-direct        methylpy           qualimap-env                  snpeff\nbbmap           evigene              minibusco          quast-5.0.2                   snp-sites-2.5.1\nbcftools        fastp                minimap2-2.24      quickmerge-env                soapdenovo-trans-1.0.4\nbedops          fastqc               mlst               racon-1.5.0                   sortmerna-env\nbedtools        fastq-dl             mlst_check         ragtag-2.1.0                  sourmash\nbioawk          fasttree-2.1.11      mobsuite           rascaf                        spades-3.15.5\nbioconvert      filtlong-env         multiqc            rcorrector-1.0.5              sra-tools\nbiopython       flye-2.9.1           mummer4            r-dartr                       star\nbit             fmlrc2-0.1.7         muscle             r-deseq                       subread-2.0.1\nblast           gcta                 nanolyse-1.2.1     recognizer-1.8.3              taxonkit\nbowtie1         geofetch             nanoplot           repeatmasker-4.1.2.p1         tgsgapcloser\nbowtie2         gffread-0.12.7       nanopolish-0.13.2  repeatmodeler-2.0.3           tracy-0.7.1\nbracken         gget                 ncbi-datasets      resfinder                     transabyss-2.0.1\nbraker2-env     gubbins              nextdenovo-env     resistomeanalyzer-2018.09.06  transdecoder-5.5.0\nbusco           hisat2               nextflow           rgi-5.2.1                     treetime\nbusco2          hmmer                nextflow-22.10     r-metabar                     trimgalore\nbusco3          interproscan-5.55    nf-core            rnaquast-2.2.1                trimmomatic-0.39\nbwa-0.7.17      iqtree               orna-2.0           roary-3.13                    trinity-2.13.2\ncabana          justorthologs-0.0.2  orthofinder        r-rnaseq                      unicycler\ncactus          kallisto-0.48.0      orthofisher        rsem-1.3.3                    virema\ncgmlst          kat-2.4.2            panaroo            rseqc-env                     virulencefinder\ncheckm-1.2.0    knsp-3.1             parsnp             r_tree                        wtdbg-2.5\nclinker         kofamscan            phylofisher        sabre-1.0\nThis is organized similarly to the Lmod modules in that there’s generally one separate environment for one program, and the environment is named after that program.\n(The naming of these environments is unfortunately not entirely consistent: many environments include the version number of the program, but others do not. For environments without version numbers, I try to have them contain the most recent version of a software5.)\n\n\n3.1 Activating Conda environments\nBefore you can activate Conda environments, you always need to load OSC’s Miniconda module first, and we will load the most recent one:\nmodule load miniconda3/23.3.1-py310\nAs mentioned above, these environments are (de)activated much like with the Lmod system. But while the term “load” is used for Lmod modules, the term “activate” is used for Conda environments — it means the same thing.\nAlso like Lmod, there is a main command (conda) and several sub-commands (deactivate, create, install, update). For example, to activate an environment:\nconda activate /fs/ess/PAS0471/jelmer/conda/multiqc\n(/fs/ess/PAS0471/jelmer/conda/multiqc) [jelmer@p0085 jelmer]$\n\n\n\n\n\n\nConda environment indicator!\n\n\n\nWhen you have an active Conda environment, its name is displayed in front of your prompt, as depicted above with (multiqc).\nBecause the MultiQC environment you just loaded is not your own, the full path to the environment is shown (making the prompt rather long…). But when you load your own environment, only the name will be shown, like so:\n(multiqc) [jelmer@p0085 jelmer]$\n\n\nAfter you have activated the MultiQC environment, you should be able to use the program. To test this, simply run the multiqc command with the --help option:\nmultiqc --help\n /// MultiQC 🔍 | v1.17\n                                                                                              \n Usage: multiqc [OPTIONS] [ANALYSIS DIRECTORY]\n \n MultiQC aggregates results from bioinformatics analyses across many samples into a    \n single report.                                                                        \n[...output truncated...]\n\nUnlike Lmod / module load, Conda will by default only keep a single environment active. Therefore, when you have one environment activate and then activate another. For example, after activating the TrimGalore environment, the MultiQC environment is no longer active:\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n  \nmultiqc --help\nbash: multiqc: command not found...\n\n\n\n\n\n\n\nThe --stack option does enable you having multiple Conda environments active (Click to expand)\n\n\n\n\n\n\nActivate the TrimGalore environment, if it isn’t already active:\nconda activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n“Stack” the MultiQC environment:\nconda activate --stack /fs/ess/PAS0471/jelmer/conda/multiqc\nCheck that you can use both programs — output not shown, but both should successfully print help info:\nmultiqc --help\n\ntrim_galore --help\n\n\n\n\n\n\n\n3.2 Lines to add to your shell script\nLike for Lmod modules, you’ll have to load Conda environments in every shell session that you want to use them — they don’t automatically reload.\nConda environments loaded in your interactive shell environment do “carry over” to the environment in which your script runs (even when you submit them to the Slurm queue with sbatch). However, it is good practice to always include the necessary code to load/activate programs in your shell scripts:\n#!/bin/bash\nset -euo pipefail\n\n# Load software\nmodule load miniconda3/23.3.1-py310\nconda activate /fs/ess/PAS0471/jelmer/conda/multiqc\n\n\n\n\n\n\n\nPerils of Conda environments inside scripts\n\n\n\nProblems can occur when you have a Conda environment active in your interactive shell while you submit a script as a batch job that activates a different environment. Therefore, it is generally a good idea not to have any Conda environments active in your interactive shell when submitting batch jobs6. To deactivate the currently active Conda environment, simply type conda deactivate without any arguments:\nconda deactivate"
  },
  {
    "objectID": "osc/04_software.html#creating-your-own-conda-environments",
    "href": "osc/04_software.html#creating-your-own-conda-environments",
    "title": "Using software at OSC",
    "section": "4 Creating your own Conda environments",
    "text": "4 Creating your own Conda environments\n\n4.1 One-time Conda configuration\nBefore you can create our own environments, you first have to do some one-time configuration7. The configuration will set the Conda “channels” (basically, software repositories) that we want to use when we install programs, including the relative priorities among channels (since one program may be available from multiple channels).\nWe can do this configuration with the config sub-command — run the following in your shell:\nconda config --add channels defaults     # Added first =&gt; lowest priority\nconda config --add channels bioconda\nconda config --add channels conda-forge  # Added last =&gt; highest priority\nLet’s check whether the configuration was successfully saved:\nconda config --get channels\n--add channels 'defaults'   # lowest priority\n--add channels 'bioconda'\n--add channels 'conda-forge'   # highest priority\n\n\n\n4.2 Example: Creating an environment for TrimGalore\nWe will now create a Conda environment with the program TrimGalore installed, which does not have a system-wide installation at OSC. Here is the command to all at once create a new Conda environment and install TrimGalore into that environment:\n# [Don't run this - we'll modify this a bit below]\nconda create -y -n trim-galore -c bioconda trim-galore\nLet’s break that command down:\n\ncreate is the Conda sub-command to create a new environment.\nWhen adding -y, Conda will not ask us for confirmation to install.\nFollowing the -n option, you can specify the name you would like the environment to have: we used trim-galore. You can use whatever name you like for the environment, but a descriptive yet concise name is a good idea. For single-program environments, it makes sense to simply name it after the program.\nThe -c option is to specify a “channel” (repository) from which to install, here bioconda8.\nThe trim-galore argument at the end of the line simply tells Conda to install the package of that name.\n\n\nBy default, Conda will install the latest available version of a program. If you create an entirely new environment for a program, like we’re doing here, that default should always apply — but if you’re installing into an environment that already contains programs, it’s possible that due to compatibility issues, it will install a different version.\nIf you want to be explicit about the version you want to install, add the version number after = following the package name, and you may then also want to include that version number in the Conda environment’s name — try this:\nconda create -y -n trim-galore-0.6.10 -c bioconda trim-galore=0.6.10\nCollecting package metadata (current_repodata.json): done  \nSolving environment: done\n# [...truncated...]\nThere should be a lot of output, with many packages that are being downloaded (these are all “dependencies” of TrimGalore), but if it works, you should see this before you get your prompt back:\nDownloading and Extracting Packages\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done                                                                                                                   \n#                        \n# To activate this environment, use                          \n#\n#     $ conda activate trim-galore-0.6.10                          \n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\nNow, you should be able to activate the environment (using just its name – see the box below):\nconda activate trim-galore-0.6.10  \nLet’s test if we can run TrimGalore — note, the command is trim_galore:\ntrim_galore --help\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\n\nSpecifying the full path to the environment dir (Click to expand)\n\n\n\n\n\nYou may have noticed above that we merely gave the environment a name (trim-galore or trim-galore-0.6.10), and did not tell it where to put this environment. Similarly, we were able to activate the environment with just its name. Conda assigns a personal default directory for its environments, somewhere in your Home directory.\nYou can install environments in a different location with the -p (instead of -n) option — for example:\n# [Don't run this]\nconda create -y -p /fs/scratch/PAS2700/$USER/conda/trim-galore -c bioconda trim-galore\nAnd when you want to load someone else’s Conda environments, you’ll always have to specify the full path to environment’s dir, like you did when loading one of my Conda environments above.\n\n\n\n\n\n\n4.3 Finding Conda installation info online\nMinor variations on the conda create command above can be used to install almost any program for which a Conda package is available, which is the vast majority of open-source bioinformatics programs!\nHowever, you may wonder how you would know:\n\nWhether the program is available and what the name of its Conda package is\nWhich Conda channel we should use\nWhich versions are available\n\nTo find this out, my strategy is to simply Google the program name together with “conda”, e.g. “cutadapt conda” if I wanted to install the Cutadapt program. Let’s see that in action:\n\n\n\n\n\nClick on that first link (in my experience, it is always the first Google hit):\n\n\n\n\n\n\n\n\n4.4 Building the installation command from the online info\nYou can take the top of the two example installation commands as a template, here: conda install -c bioconda cutadapt. You may notice the install subcommand, which we haven’t yet seen. This would install Cutadapt into the currently activated Conda environment. Since our strategy here is to create a separate environment for each program, just installing a program into whatever environment is currently active is not a great idea.\nYou can use the install command with a new environment, but then you would first have to create an “empty” environment, and then run the install command. However, we saw above that we can do all of this in a single command. To build this create-plus-install command, all we need to do is replace install in the example command on the Conda website by create -y -n &lt;env-name&gt;. Then, our full command (without version specification) will be:\n# [Don't run this - example command]\nconda create -y -n cutadapt -c bioconda cutadapt\nTo see which version of the software will be installed by default, and to see which older versions are available:\n\n\n\n\n\nFor almost any other program, you can use the exact same procedure to find the Conda package and install it!\n\n\n\n\n\n\nMore Conda commands to manage your environments (Click to expand)\n\n\n\n\n\n\nRemove an environment entirely:\nconda env remove -n cutadapt\nList all your conda environments:\nconda env list\nList all packages (programs) installed in an environment — due to dependencies, this can be a long list, even if you only actively installed one program:\nconda list -p /fs/ess/PAS0471/jelmer/conda/multiqc\nExport a plain-text “YAML” file that contains the instructions to recreate your currently-active environment (useful for reproducibility!)\nconda env export &gt; my_env.yml\nAnd you can use the following to create a Conda environment from such a YAML file:\nconda env create -n my_env --force --file my_env.yml\n\n\n\n\n\n\n\n4.5 Organizing your Conda environments\nThere are two reasonable alternative way to organize your Conda environments:\n\nHave one environment per program (my preference)\n\nEasier to keep an overview of what you have installed\nNo need to reinstall the same program across different projects\nLess risk of running into problems with your environment due to mutually incompatible software and complicated dependency situations\n\nHave one environment per research project\n\nYou just need to activate that one environment when you’re working on your project.\nEasier when you need to share your entire project with someone else (or yourself) on a different (super)computer.\n\n\nEven though it might seem easier, a third alternative, to simply install all programs across all projects in one single environment, is not recommended. This doesn’t benefit reproducibility, and your environment is likely to stop functioning properly sooner or later."
  },
  {
    "objectID": "osc/04_software.html#self-study-using-apptainer-containers",
    "href": "osc/04_software.html#self-study-using-apptainer-containers",
    "title": "Using software at OSC",
    "section": "5 Self-study: Using Apptainer containers",
    "text": "5 Self-study: Using Apptainer containers\nContainers are an alternative to Conda to use programs that don’t have system-wide installations at OSC.\nContainers are similar to Virtual Machines and different from Conda environments in that they come with an entire operating system. This makes creating your own container “image” (see box below on terminology) much more involved than creating a Conda environment, and we will not cover that here.\nHowever, pre-existing container images are available for most bioinformatics programs, and these can be easily found, downloaded, and used.\n\n\n\n\n\n\nContainer terminology\n\n\n\n\nContainer image: File (Apptainer) or files (Docker) that contain the container application.\nContainer (sensu stricto): A running container image.\nDefinition file (Apptainer) / Dockerfile (Docker): A plain text file that contains the recipe to build a container image.\n\n\n\nAmong container platforms, Apptainer (formerly known as Singularity) and especially Docker are the most widely used ones. At supercomputers like OSC, however, only Apptainer containers can be used. Luckily, the Apptainer program can work with Docker container images: it will convert them on the fly.\n\n\n5.1 Finding container images online\nThere are several online repositories with publicly available container images, but I would recommend BioContainers (https://biocontainers.pro/registry) or Quay.io (https://quay.io/biocontainers).\nFor example, let’s look on the BioContainers website for a TrimGalore container image:\n\n\n\n\n\n\nThe search result on the BioContainers website after entering “trim galore” in the search box.\n\n\nClick on the only entry that is shown, trim-galore, which will get you to a page like this:\n\n\n\n\n\nThe website also includes Conda installation instructions — to see the container results, scroll down to:\n\n\n\n\n\n\nAfter scrolling down on the results page, you should see a recent available container image.\nNote that the command shown is singularity run, but we will use the more up-to-date apptainer run.\n\n\nThe version tag that is shown (0.6.9--hdfd78af_0 above) pertains to the version of TrimGalore, but the result that is shown here is not will always the container image(s) with the most recent version. To see a list of all available images, click on the Packages and Containers tab towards the top, and then sort by Last Update:\n\n\n\nThe logo with the large S depicts Singularity/Apptainer containers.\n\n\nWhenever you find both a Singularity/Apptainer and a Docker image for your program, use the Singularity/Apptainer image. This is because those don’t have to be converted, while Docker images do. But when the version you want is only available as a Docker image, that will work too: as mentioned above, it will be automatically converted to the proper format.\n\n\n\n5.2 Running a container image\nWhen you’ve found a container image that you want to use, copy its URL from the BioContainers website. For example, for the most recent TrimGalore version as of March 2024: https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0.\nYou could also copy the full command — however, we will modify that in two ways, using:\n\nThe more up-to-date apptainer command9\nThe exec subcommand instead of run, allowing us to enter a custom command to run in the container10.\n\nAs such, our “base” command to run TrimGalore in the container will be as follows:\n# [Don't run this, we'll need to add a TrimGalore command]\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0\n\n\n\n\n\n\nNeed to use a Docker container? You can’t use the Docker URL as-is. (Click to expand)\n\n\n\n\n\nIf you want to use a Docker container, the listed quasi-URL on BioContainers will start with “quay.io”. In your apptainer exec command, you need to preface this URL with docker://. For instance:\napptainer exec docker://quay.io/biocontainers/trim-galore:0.6.10--hdfd78af_0\n\n\n\nAfter the code above, we would finish our command by simply entering a TrimGalore command in the exact same way as we would when running TrimGalore outside of the context of a container. For example, to just print the help info like we’ve been doing before, the TrimGalore command is:\ntrim_galore --help\nAnd to run that inside the container, our full command will be:\napptainer exec https://depot.galaxyproject.org/singularity/trim-galore:0.6.10--hdfd78af_0 \\\n    trim_galore --help\nINFO:    Downloading network image\n321.4MiB / 321.4MiB [===================================================================================================================================] 100 % 3.0 MiB/s 0s\nWARNING: Environment variable LD_PRELOAD already has value [], will not forward new value [/apps/xalt/xalt/lib64/libxalt_init.so] from parent process environment\n\n USAGE:\n\ntrim_galore [options] &lt;filename(s)&gt;\n\n-h/--help               Print this help message and exits.\n# [...truncated...]\n\n\n\n\n\n\nThe Apptainer/Singularity software does not need to be loaded at OSC, it is always automatically loaded.\n\n\n\n\n\n\nSo, all that is different from running a program inside a container instead of a a locally installed program, is that you prefix your command with apptainer exec &lt;URL&gt;.\nThe first time you run this command, the container will be downloaded, which can take a few minutes (by default it will be downloaded to ~/.apptainer/cache, but you can change this by setting the $APPTAINER_CACHEDIR environment variable). After that, the downloaded image will be used and the command should be executed about as instantaneously as when running TrimGalore outside of a container.\n(You will keep seeing the warning WARNING: Environment variable LD_PRELOAD [...] whenever you run a container, but this is nothing to worry about.)\n\n\n\n\n\n\nWhen to use a Container versus Conda\n\n\n\nWhen you need multiple programs in quick succession or in a single command (e.g., you’re piping the output of one program into a second program), it can be more convenient to have those programs installed in a single environment or container. Pre-built multi-program containers are not as easy to find. And since building your own Conda environment is easier than building your own container, this is a situation where you might prefer Conda."
  },
  {
    "objectID": "osc/04_software.html#footnotes",
    "href": "osc/04_software.html#footnotes",
    "title": "Using software at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n And with Git we saw another kind of behavior, where the automatically available version is very old, but we can load a more recent version.↩︎\n This may vary over time and also depends on whether you run this in the VS Code Server terminal — some of the loaded modules are related to that.↩︎\nWhen your personal computer asks you to “authenticate” while you are installing something, you are authenticating yourself as a user with administrator privileges. At OSC, you don’t have such privileges.↩︎\nOther software upon which the software that you are trying to install depends.↩︎\n It isn’t feasible to keep separate environments around for many different versions of a program, mostly because Conda environments contain a very large number of files, and OSC has file number quotas. This is why I have in many cases chosen the strategy of just updating the version within the same environment.↩︎\n Unless you first deactivate any active environments in your script.↩︎\nThat is, these settings will be saved somewhere in your OSC home directory, and you never have to set them again unless you need to make changes.↩︎\n Given that you’ve done some config above, this is not always necessary, but it can be good to be explicit.↩︎\nThough note that as of March 2024, the singularity command does still work, and it will probably continue to work for a while.↩︎\n The run subcommand would only run some preset default action, which is rarely useful for our purposes.↩︎"
  },
  {
    "objectID": "osc/05_slurm.html#basics-of-slurm-batch-jobs",
    "href": "osc/05_slurm.html#basics-of-slurm-batch-jobs",
    "title": "Slurm batch jobs at OSC",
    "section": "1 Basics of Slurm batch jobs",
    "text": "1 Basics of Slurm batch jobs\nWhen you request a batch job, you ask the Slurm scheduler to run a script “out of sight” on a compute node. While that script will run on a compute node, you stay in your current shell at your current node regardless of whether that is on a login or compute node. After submitting a batch job, it will continue running even if you log off from OSC and shut down your computer.\n\n\n1.1 The sbatch command\nYou can use Slurm’s sbatch command to submit a batch job. But first, recall that you can directly run a Bash script as follows:\nbash scripts/printname.sh Jane Doe\nThis script will print a first and a last name\nFirst name: Jane\nLast name: Doe\nThe above command ran the script on our current node. To instead submit the script to the Slurm queue, simply replace bash by sbatch:\nsbatch scripts/printname.sh Jane Doe\nsrun: error: ERROR: Job invalid: Must specify account for job  \nsrun: error: Unable to allocate resources: Unspecified error\nHowever, as the above error message “Must specify account for job” tells us, you need to indicate which OSC Project (or as Slurm puts it, “account”) you want to use for this compute job. Use the --account= option to sbatch to do this:\nsbatch --account=PAS2700 scripts/printname.sh Jane Doe\nSubmitted batch job 12431935\nThis output line means your job was successfully submitted (no further output will be printed to your screen — more about that below). The job has a unique identifier among all compute jobs by all users at OSC, and we can use this number to monitor and manage it. Each of us will therefore see a different job number pop up.\n\n\n\n\n\n\n\nsbatch options and script arguments\n\n\n\nAs you perhaps noticed in the command above, we can use sbatch options and script arguments in one command like so:\nsbatch [sbatch-options] myscript.sh [script-arguments]\nBut, depending on the details of the script itself, all combinations of using sbatch options and script arguments are possible:\nsbatch scripts/printname.sh                             # No options/arguments for either\nsbatch scripts/printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2700 scripts/printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2700 scripts/printname.sh Jane Doe  # Both sbatch option and script arguments\n(Omitting the --account option is possible when we specify this option inside the script, as we’ll see below.)\n\n\n\n\n\n1.2 Adding sbatch options in scripts\nThe --account= option is just one of many options you can use when reserving a compute job, but is the only required one. Defaults exist for all other options, such as the amount of time (1 hour) and the number of cores (1 core).\nInstead of specifying sbatch options on the command-line when submitting the script, you can also add these options inside the script. This is a useful alternative because:\n\nYou’ll often want to specify several options, which can lead to very long sbatch commands.\nIt allows you to store a script’s typical Slurm options as part of the script, so you don’t have to remember them.\n\nThese options are added in the script using another type of special comment line akin to the shebang (#!/bin/bash) line, marked by #SBATCH. Just like the shebang line, the #SBATCH line(s) should be at the top of the script. Let’s add one such line to the printname.sh script, such that the first few lines read:\n#!/bin/bash\n#SBATCH --account=PAS2700\n\nset -euo pipefail\nSo, the equivalent of adding --account=PAS2700 after sbatch on the command line is a line in your script that reads #SBATCH --account=PAS2700.\nAfter adding this to the script, you are now able to run the sbatch command without options (which failed earlier):\nsbatch scripts/printname.sh Jane Doe\nSubmitted batch job 12431942\nAfter submitting a batch job, you immediately get your prompt back. The job will run outside of your immediate view, and you can continue doing other things in the shell while it does (or log off). This behavior allows you to submit many jobs at the same time, because you don’t have to wait for other jobs to finish, or even to start.\n\n\n\n\n\n\nsbatch option precedence!\n\n\n\nAny sbatch option provided on the command line will override the equivalent option provided inside the script. This is sensible because it allows you to provide “defaults” inside the script, and change one or more of those when needed “on the go” on the command line.\n\n\n\n\n\n\n\n\nRunning a script with #SBATCH lines in non-Slurm contexts (Click to expand)\n\n\n\n\n\nBecause #SBATCH lines are special comment lines, they will simply be ignored (and not throw any errors) when you run a script with such lines in other contexts: for example, when not running it as a batch job at OSC, or even when running it on a computer without Slurm installed.\n\n\n\n\n\n\n1.3 Where does the script’s output go?\nAbove, we saw that when you ran printname.sh directly with bash, its output was printed to the screen, whereas when you submitted it as a batch job, only Submitted batch job &lt;job-number&gt; was printed to screen. Where did your output go?\nThe output ended up in a file called slurm-&lt;job-number&gt;.out (e.g., slurm-12431942.out; since each job number is unique to a given job, each file has a different number). We will call this type of file a Slurm log file.\n\n\nAny idea why we may not want batch job output printed to screen, even if it was possible? (Click for the answer)\n\nThe power of submitting batch jobs is that you can submit many at once — e.g. one per sample, running the same script. If the output from all those scripts ends up on your screen, things become a big mess, and you have no lasting record of what happened.\n\n\nYou should already have two of these Slurm log files if you ran all the above code:\nls\nscripts slurm-12431935.out slurm-12431942.out\nLet’s take a look at the contents of one of these:\n# (Replace the number in the file name with whatever you got! - check with 'ls')\ncat slurm-12431935.out\nThis script will print a first and a last name\nFirst name: Jane  \nLast name: Doe\nThis file contains the script’s output that was printed to screen when we ran it with bash — nothing more or less.\n\nTwo types of output files\nIt’s important to realize the distinction between two broad types of output a script may have:\n\nOutput that is printed to screen when you directly run a script (bash myscript.sh), and that ends up in the Slurm log file when you submit the script as a batch job. This includes output produced by echo statements, by any errors that may occur, and logging output by any program that we run in the script1.\nOutput of commands inside the script that is redirected to a file or that a program writes to an output file. This type of output will end up in the exact same files regardless of whether we run the script directly (with bash) or as a batch job (with sbatch).\n\nOur script above only had the first type of output, but typical scripts have both, and we’ll see examples of this below.\n\n\nCleaning up the Slurm logs\nWhen using batch jobs, your working dir can easily become a confusing mess of anonymous-looking Slurm log files. Two strategies help to prevent this:\n\nChanging the default Slurm log file name to include a one- or two-word description of the job/script (see below).\nCleaning up your Slurm log files, by:\n\nRemoving them when no longer needed — as is e.g. appropriate for our current Slurm log file.\nMoving them into a Results dir, which is often appropriate after you’ve run a bioinformatics tool, since the Slurm log file may contain some info you’d like to keep. For example, we may move any Slurm log files for jobs that ran FastQC to a dir results/fastqc/logs.\n\n\n# In this case, we'll simply remove the Slurm log files\nrm slurm*out\n\n\n\n\n\n\n\nThe working directory stays the same\n\n\n\nBatch jobs start in the directory that they were submitted from: that is, your working directory remains the same."
  },
  {
    "objectID": "osc/05_slurm.html#monitoring-batch-jobs",
    "href": "osc/05_slurm.html#monitoring-batch-jobs",
    "title": "Slurm batch jobs at OSC",
    "section": "2 Monitoring batch jobs",
    "text": "2 Monitoring batch jobs\nWhen submitting batch jobs for your research, you’ll often have jobs that run for a while, and/or you’ll submit many jobs at once. In addition, longer-running jobs and that ask for many cores sometimes remain queued for a while before they start. It’s therefore important to know how you can monitor your batch jobs.\n\n2.1 A sleepy script for practice\nWe’ll use another short shell script to practice monitoring and managing batch jobs. First create a new file:\ntouch scripts/sleep.sh\nOpen the file in the VS Code editor and copy the following into it:\n#!/bin/bash\n#SBATCH --account=PAS2700\n\necho \"I will sleep for 30 seconds\" &gt; sleep.txt\nsleep 30s\necho \"I'm awake! Done with script sleep.sh\"\n\n\n Exercise: Batch job output recap\nPredict what would happen if you submit the sleep.sh script as a batch job using sbatch scripts/sleep.sh:\n\nHow many output files will this batch job produce?\nWhat will be in each of those files?\nIn which directory will the file(s) appear?\nIn terms of output, what would have been different if we had run the script directly, using the command bash scripts/sleep.sh?\n\nThen, test your predictions by running the script.\n\n\nClick for the solutions\n\n\nThe job will produce 2 files:\n\nslurm-&lt;job-number&gt;.out: The Slurm log file, containing output normally printed to screen.\nsleep.txt: Containing output that was redirected to this file in the script.\n\nThe those files will contain the following:\n\nslurm-&lt;job-number&gt;.out: I’m awake! Done with script sleep.sh\nsleep.txt: “I will sleep for 30 seconds”\n\nBoth files will end up in your current working directory. Slurm log files always go to the directory from which you submitted the job. Slurm jobs also run from the directory from which you submitted your job, and since we redirected the output simply to sleep.txt, that file was created in our working directory.\nIf we had run the script directly, sleep.txt would have also been created with the same content, but “All done!” would have been printed to screen.\n\nRun the script and check the outputs:\nsbatch scripts/sleep.sh\nSubmitted batch job 27935840\ncat sleep.txt\nI will sleep for 30 seconds\ncat slurm-27935840.out\nI'm awake! Done with script sleep.sh\n\n\n\n\n\n2.2 Checking the job’s status\nAfter you submit a job, it may be initially be waiting to be allocated resources: i.e., it may be queued (“pending”). Then, the job will start running — you’ve seen all of this with the VS Code Interactive App job as well.\nWhereas Interactive App jobs will keep running until they’ve reached the end of the allocated time2, batch jobs will stop as soon as the script has finished. And if the script is still running when the job runs out of its allocated time, it will be killed (stopped) right away.\n\nThe squeue command\nYou can check the status of your batch job using the squeue Slurm command:\nsqueue -u $USER -l\nThu Apr 4 15:47:51 2023\n        JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n     23640814 condo-osu ondemand   jelmer  RUNNING       6:34   2:00:00      1 p0133\nIn the command above:\n\nYou specify your username with the -u option (without this, you’d see everyone’s jobs!). In this example, I used the environment variable $USER to get your user name, just so that the very same code will work for everyone (you can also simply type your username if that’s shorter or easier).\nThe option -l (lowercase L, not the number 1) will produce more verbose (“long”) output.\n\nIn the output, after a line with the date and time, and a header line, you should see information about a single compute job, as shown above: this is the Interactive App job that runs VS Code. That’s not a “batch” job, but it is a compute job, and all compute jobs are listed.\nThe following pieces of information about each job are listed:\n\nJOBID — The job ID number\nPARTITION — The type of queue\nNAME — The name of the job\nUSER — The user name of the user who submitted the job\nSTATE — The job’s state, usually PENDING (queued) or RUNNING. Finished jobs do not appear on the list.\nTIME — For how long the job has been running (here as minutes:seconds)\nTIME_LIMIT — the amount of time you reserved for the job (here as hours:minutes:seconds)\nNODES — The number of nodes reserved for the job\nNODELIST(REASON) — When running: the ID of the node on which it is running. When pending: why it is pending.\n\n\n\n\nsqueue example\nNow, let’s see a batch job in the squeue listing. Start by submitting the sleep.sh script as a batch job:\nsbatch scripts/sleep.sh\nSubmitted batch job 12431945\nIf you’re quick enough, you may be able to catch the STATE as PENDING before the job starts:\nsqueue -u $USER -l\nThu Apr 4 15:48:26 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      12520046 serial-40 sleep.sh   jelmer  PENDING       0:00   1:00:00      1 (None)\n      23640814 condo-osu ondemand   jelmer  RUNNING       7:12   2:00:00      1 p0133\nBut soon enough it should say RUNNING in the STATE column:\nsqueue -u $USER -l\nThu Apr 4 15:48:39 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      12520046 condo-osu sleep.sh   jelmer  RUNNING       0:12   1:00:00      1 p0133\n      23640814 condo-osu ondemand   jelmer  RUNNING       8:15   2:00:00      1 p0133\nThe script should finish after 30 seconds (because your command was sleep 30s), after which the job will immediately disappear from the squeue listing, because only pending and running jobs are shown:\nsqueue -u $USER -l\nMon Aug 21 15:49:26 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      23640814 condo-osu ondemand   jelmer  RUNNING       9:02   2:00:00      1 p0133\n\n\n\nChecking the output files\nWhenever you’re running a script as a batch job, even if you’ve been monitoring it with squeue, you should also make sure it ran successfully. You typically do so by checking the expected output file(s). As mentioned above, you’ll usually have two types of output from a batch job:\n\nFile(s) directly created by the command inside the script (here, sleep.sh).\nA Slurm log file with the script’s standard output and standard error (i.e. output that is normally printed to screen).\n\nAnd you saw in the exercise above that this was also the case for the output of our sleepy script:\ncat sleep.txt\nI will sleep for 30 seconds\ncat slurm-12520046.out\nI'm awake! Done with script sleep.sh\n\nLet’s keep things tidy and remove the sleepy script outputs:\n# (Replace the number in the file name with whatever you got! - check with 'ls')\nrm slurm*.out sleep.txt\n\n\n\n\n\n\n\nSee output added to the Slurm log file in real time\n\n\n\nText will be added to the Slurm log file in real time as the running script (or the program ran by the script) outputs it. However, the output that commands like cat and less print are static.\nTherefore, if you find yourself opening/printing the contents of the Slurm log file again and again to keep track of progress, then instead use tail -f, which will “follow” the file and will print new text as it’s added to the Slurm log file:\n# See the last lines of the file, with new contents added in real time\ntail -f slurm-12520046.out\nTo exit the tail -f livestream, press Ctrl+C.\n\n\n\n\n\n\n2.3 Cancelling jobs\nSometimes, you want to cancel one or more jobs, because you realize you made a mistake in the script, or because you used the wrong input files as arguments. You can do so using scancel:\n# [Example - DON'T run this: the second line would cancel your VS Code job]\nscancel 2979968        # Cancel job number 2979968\nscancel -u $USER       # Cancel all your running and queued jobs (careful with this!)\n\n\n\n\n\n\n\nAdditional job management commands and options (Click to expand)\n\n\n\n\n\n\nUse squeue’s -t option to restrict the type of jobs you want to show. For example, to only show running and not pending jobs:\nsqueue -u $USER -t RUNNING\nYou can see more details about any running or finished job, including the amount of time it ran for:\nscontrol show job &lt;jobID&gt;\nUserId=jelmer(33227) GroupId=PAS0471(3773) MCS_label=N/A\nPriority=200005206 Nice=0 Account=pas2700 QOS=pitzer-default\nJobState=RUNNING Reason=None Dependency=(null)\nRequeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\nRunTime=00:02:00 TimeLimit=01:00:00 TimeMin=N/A\nSubmitTime=2020-12-14T14:32:44 EligibleTime=2020-12-14T14:32:44\nAccrueTime=2020-12-14T14:32:44\nStartTime=2020-12-14T14:32:47 EndTime=2020-12-14T15:32:47 Deadline=N/A\nSuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-12-14T14:32:47\nPartition=serial-40core AllocNode:Sid=pitzer-login01:57954\n[...]\nUpdate directives for a job that has already been submitted (this can only be done before the job has started running):\nscontrol update job=&lt;jobID&gt; timeLimit=5:00:00\nHold and release a pending (queued) job, e.g. when needing to update input file before it starts running:\nscontrol hold &lt;jobID&gt;       # Job won't start running until released\nscontrol release &lt;jobID&gt;    # Job is free to start"
  },
  {
    "objectID": "osc/05_slurm.html#common-slurm-options",
    "href": "osc/05_slurm.html#common-slurm-options",
    "title": "Slurm batch jobs at OSC",
    "section": "3 Common Slurm options",
    "text": "3 Common Slurm options\nHere, we’ll go through the most commonly used Slurm options. As pointed out above, each of these can either be:\n\nPassed on the command line: sbatch --account=PAS2700 myscript.sh (has precedence over the next)\nAdded at the top of the script you’re submitting: #SBATCH --account=PAS2700.\n\nAlso, note that many Slurm options have a corresponding long (--account=PAS2700) and short format (-A PAS2700). For clarity, we’ll stick to long format options here.\n\n3.1 --account: The OSC project\nAs seen above. When submitting a batch job, always specify the OSC project (“account”).\n\n\n3.2 --time: Time limit (“wall time”)\nUse the --time option to specify the maximum amount of time your job will run for:\n\nYour job will be killed (stopped) as soon as it hits the specified time limit!\nCompare “Wall time” with “core hours”: if a job runs for 2 hour and used 8 cores, the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\nThe default time limit is 1 hour. Acceptable time formats include:\n\nminutes (e.g. 60 =&gt; 60 minutes)\nhours:minutes:seconds (e.g. 1:00:00 =&gt; 60 minutes)\ndays-hours (e.g. 2-12 =&gt; two-and-a-half days)\n\nFor single-node jobs, up to 168 hours (7 days) can be requested. If that’s not enough, you can request access to the longserial queue for jobs of up to 336 hours (14 days).\nOSC bills you for the time your job actually used, not what you reserved. But jobs asking for more time may be queued longer before they start.\n\nAn example, asking for 2 hours in the “minute-format”:\n#!/bin/bash\n#SBATCH --time=120\nOr for 12 hours in the “hour-format”:\n#!/bin/bash\n#SBATCH --time=12:00:00\n\n\n\n\n\n\nWhen in doubt, reserve more time\n\n\n\nIt is common to be uncertain about how much time your job will take (i.e., how long it will take for your script to finish). Whenever this happens, ask for more, perhaps much more, time than what you think/guesstimate you will need. It is really annoying to have a job run out of time after several hours, while the increase in queueing time for jobs asking for more time is often quite minimal at OSC.\n\n\n\n\n Exercise: exceed the time limit\nModify the sleep.sh script to reserve only 1 minute for the job while making the script run for longer than that.\nIf you succeed in exceeding the time limit, an error message will be printed. Where do you think this error message will be printed: to the screen, in the Slurm log file, or in sleep.txt? After waiting for the job to be killed after 60 seconds, check if you were correct and what the error message is.\n\n\nClick for the solution\n\nThis script would do the trick, where we request 1 minute of wall-time while we let the script sleep for 80 seconds:\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --time=1\n\necho \"I will sleep for 80 seconds\" &gt; sleep.txt\nsleep 80s\necho \"I'm awake! Done with script sleep.sh\"\nSubmit it as usual:\nsbatch scripts/sleep.sh\nSubmitted batch job 23641567\nThis would result in the following type of error, which will be printed in the Slurm log file:\nslurmstepd: error: *** JOB 23641567 ON p0133 CANCELLED AT 2024-04-04T14:55:24 DUE TO TIME LIMIT ***\n\n\n\n\n\n3.3 Cores (& nodes and tasks)\nThere are several options to specify the number of nodes (≈ computers), cores, or “tasks” (processes). These are separate but related options, and this is where things can get confusing! Some background:\n\nNote that Slurm mostly uses the terms “core” and “CPU” interchangeably3. More generally with bioinformatics tools, “thread” is also commonly used interchangeably with core/CPU4. Therefore, as also mentioned in the session on OSC, for our purposes, you can think of core, CPU, and thread as synonyms that refer to the sub-parts/components of a node that you can reserve and use separately.\n\n\nRunning a program with multiple threads/cores/CPUs (“multi-threading”) is very common, and this can make the running time of such programs much shorter. While the specifics depend on the program, using 8-12 cores is often a sweet spot, whereas asking for even more cores can lead to rapidly diminishing returns.\nRunning multiple processes (tasks) or needing multiple nodes in a single batch job is not common.\n\nIn practice, my recommendations are to basically always:\n\nSpecify the number of threads/cores/CPUs to Slurm with --cpus-per-task=n (the short notation is -c).\nKeep the number of tasks and nodes to their defaults of 1 (in which case the above -c option specifies the number of cores, period).\nTell the program that you’re running about the number of available cores — most bioinformatics tools have an option like --cores or --threads. You should set this to the same value n as the --cpus-per-task.\n\nAn example, where we ask for 8 CPUs/cores/threads:\n#!/bin/bash\n#SBATCH --cpus-per-task=8\n\n# And we tell a fictional program about that number of cores:\ncool_program.py --cores 8 sampleA_R1.fastq.gz\n\n\n\n\n\n\nRare cases: multiple nodes or tasks (Click to expand)\n\n\n\n\n\n\nYou can specify the number of nodes with --nodes and the number of tasks with --ntasks and/or --ntasks-per-node; all have defaults of 1 (see the table below).\nOnly ask for more than one node when a program is parallelized with e.g. “MPI”, which is rare in bioinformatics.\nFor jobs with multiple processes (tasks), you can use --ntasks=n or --ntasks-per-node=n — this is also quite rare! However, note in practice, specifying the number of tasks n with one of these options is equivalent to using --cpus-per-task=n, in the sense that both ask for n cores that can subsequently be used by a program in your script. Therefore, some people use tasks as opposed to cpus for multi-threading, and you can see this usage in the OSC documentation too. Yes, this is confusing!\n\nHere is an overview of the options related to cores, tasks, and nodes:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nNr. of cores/CPUs/threads (per task)\n-c 1\n--cpus-per-task=1\n1\n\n\nNr. of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr. of tasks per node\n-\n--ntasks-per-node=1\n1\n\n\nNr. of nodes\n-N 1\n--nodes=1\n1\n\n\n\n\n\n\n\n\n\n3.4 --mem: RAM memory\nUse the --mem option to specify the maximum amount of RAM (Random Access Memory) that your job can use:\n\nEach core on a node has 4 GB of memory “on it”, and therefore, the default amount of memory you will get is 4 GB is per reserved core. For example, if you specify --cpus-per-task=4, you will have 16 GB of memory. And the default number of cores is 1, so the default amount of memory is 4 GB.\nBecause it is common to ask for multiple cores and due to the above-mentioned adjustment of the memory based on the number of cores, you will usually end up having enough memory automatically — therefore, it is common to omit the --mem option.\nThe default --mem unit is MB (MegaBytes); append G for GB (i.e. 100 means 100 MB, 10G means 10 GB).\nLike with the time limit, your job gets killed by Slurm when it hits the memory limit.\nThe maximum amount of memory you can request on regular Pitzer compute nodes is 177 GB (and 117 GB on Owens). If you need more than that, you will need one of the specialized largemem or hugemem nodes — switching to such a node can happen automatically based on your requested amount, though with caveats: see this OSC page for details on Pitzer, and this page for details on Owens.\n\nFor example, to request 20 GB of RAM:\n#!/bin/bash\n#SBATCH --mem=20G\n\n\n\n\n\n\nIt is not always clear what happened when your job ran out of memory (Click to expand)\n\n\n\n\n\nWhereas you get a very clear Slurm error message when you hit the time limit (as seen in the exercise above), hitting the memory limit can result in a variety of errors.\nBut look for keywords such as “Killed”, “Out of Memory” / “OOM”, and “Core Dumped”, as well as actual “dumped cores” in your working dir (large files with names like core.&lt;number&gt;, these can be deleted).\n\n\n\n\n\n Exercise: Adjusting cores and memory\nThink about submitting a shell script that runs a bioinformatics tool like FastQC as a batch job, in the following two scenarios:\n\nThe program has an option --threads, and you want to set that to 8. The program also says you’ll need 25 GB of memory. What #SBATCH options related to this will you use?\n\n\n\nClick for the solution\n\nYou should only need the following, since this will give you 8 * 4 = 32 GB of memory. There is no point in “downgrading” the amount of memory.\n#SBATCH --cpus-per-task=8\n\n\nThe program has an option --cores, and you want to set that to 12. The program also says you’ll need 60 GB of memory. What #SBATCH options will you use?\n\n\n\nClick for the solution\n\nHere, it will make sense to ask for --mem separately.\n#SBATCH --cpus-per-task=12\n#SBATCH --mem=60G\nAlternatively, you could ask for 15 cores, but then instruct the program to use only 12. Or you could reason that since you’ll need 15 cores anyway due to the amount of memory you’ll need, you might as well instruct the program to use all 15, since this may well speed things up a little more.\n\n\n\n\n\n3.5 --output: Slurm log files\nAs we saw above, by default, all output from a script that would normally be printed to screen will end up in a Slurm log file when we submit the script as a batch job. This file will be created in the directory from which you submitted the script, and will be called slurm-&lt;job-number&gt;.out, e.g. slurm-12431942.out.\nBut it is possible to change the name of this file. For instance, it can be useful to include the name of the bioinformatics program that the script runs, so that it’s easier to recognize this file later. We can do this with the --output option, e.g. --output=slurm-fastqc.out if we were running FastQC.\nBut you’ll generally want to keep the batch job number in the file name too5. Since we won’t know the batch job number in advance, we need a trick here — and that is to use %j, which represents the batch job number:\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n\n\n\n\n\n\nThe output streams stdout and stderr, and separating them (Click to expand)\n\n\n\n\n\nBy default, two output streams from commands and programs called “standard output” (stdout) and “standard error” (stderr) are printed to screen. Without discussing this in detail, we have seen this several times: any regular output by a command is stdout and any error messages we’ve seen were stderr. Both of these streams by default also end up in the same Slurm log file, but it is possible to separate them into different files.\nBecause stderr, as you might have guessed, often contains error messages, it could be useful to have those in a separate file. You can make that happen with the --error option, e.g. --error=slurm-fastqc-%j.err.\nHowever, reality is more messy: some programs print their main output not to a file but to standard out, and their logging output, errors and regular messages alike, to standard error. Yet other programs use stdout or stderr for all messages.\nI therefore usually only specify --output, such that both streams end up in that file.\n\n\n\n\n\n\n3.6 --mail-type: Receive emails\nYou can use the --mail-type option to have Slurm email you for example when a job begins, completes or fails. You don’t have to specify your email address: you’ll be automatically emailed on the email address that is linked to your OSC account. I tend to use:\n\nFAIL for shorter-running jobs (roughly up to a few hours)\nFAIL will email you upon job failure, e.g. when the scripts exits with an error or times out. This is especially useful when submitting many jobs with a loop: this way you know immediately whether any of the jobs failed.\nEND and FAIL for longer-running jobs\nThis is helpful because you don’t want to have to keep checking in on jobs that run for many hours.\n\nI would avoid having Slurm send you emails upon regular completion for shorter jobs, because you may get inundated with emails and then quickly start ignoring the emails altogether.\n#!/bin/bash\n#SBATCH --mail-type=END,FAIL\n#!/bin/bash\n#SBATCH --mail-type=FAIL\n\n\n\n\n\n\nGet warned when your job is close to its time limit (Click to expand)\n\n\n\n\n\nYou may also find the values TIME_LIMIT_90, TIME_LIMIT_80, and TIME_LIMIT_50 useful for very long-running jobs, which will warn you when the job is at 90/80/50% of the time limit. For example, it is possible to email OSC to ask for an extension on individual jobs. You shouldn’t do this often, but if you have a job that ran for 6 days and it looks like it may time out, this may well be worth it.\n\n\n\n\n\n Exercise: Submit your FastQC script as a batch job\nIn this tutorial, we created a shell script to run FastQC, and ran it as follows:\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nbash scripts/fastqc.sh \"$fastq_file\" results/fastqc\n\nAdd an #SBATCH lines to the script to specify the OSC project PAS2700, and submit the modified script as a batch job with the same arguments as above.\n\n\n\nSolution (click here)\n\n\nThe top of your script should read as follows:\n#!/bin/bash\n#SBATCH --account=PAS2700\nSubmit the script as follows:\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nsbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\nSubmitted batch job 12431988\n\n\n\nMonitor your job with squeue.\nWhen it has finished, check the Slurm log file in your working dir and the main FastQC output files in results/fastqc.\nBonus — add these #SBATCH options, then resubmit:\n\nLet the Slurm log file include ‘fastqc’ in the file name as well as the job ID number.\nLet Slurm email you both when the job completes normally and when it fails. Check that you received the email.\n\n\n\n\nSolution (click here)\n\n\nThe top of your script should read as follows:\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --output=slurm-fastqc-%j.out\n#SBATCH --mail-type=END,FAIL"
  },
  {
    "objectID": "osc/05_slurm.html#in-closing-making-sure-your-jobs-ran-successfully",
    "href": "osc/05_slurm.html#in-closing-making-sure-your-jobs-ran-successfully",
    "title": "Slurm batch jobs at OSC",
    "section": "4 In closing: making sure your jobs ran successfully",
    "text": "4 In closing: making sure your jobs ran successfully\nHere are some summarizing notes on the overall strategy to monitor your batch jobs:\n\nTo see whether your job(s) have started, check the queue (with squeue) or check for Slurm log files (with ls).\nOnce the jobs are no longer listed in the queue, they will have finished: either successfully or because of an error.\nWhen you’ve submitted many jobs that run the same script for different samples/files:\n\nCarefully read the full Slurm log file, and check other output files, for at least 1 one of the jobs.\nCheck whether no jobs have failed: via email when using --mail-type=END, or by checking the tail of each log for “Done with script” messages6.\nCheck that you have the expected number of output files and that no files have size zero (run ls -lh)."
  },
  {
    "objectID": "osc/05_slurm.html#self-study-material",
    "href": "osc/05_slurm.html#self-study-material",
    "title": "Slurm batch jobs at OSC",
    "section": "5 Self-study material",
    "text": "5 Self-study material\n\nSlurm environment variables\nInside a shell script that will be submitted as a batch job, you can use a number of Slurm environment variables that will automatically be available, such as:\n\n\n\n\n\n\n\n\nVariable\nCorresponding option\nDescription\n\n\n\n\n$SLURM_JOB_ID\nN/A\nJob ID assigned by Slurm\n\n\n$SLURM_JOB_NAME\n--job-name\nJob name\n\n\n$SLURM_CPUS_PER_TASK\n-c / --cpus-per-task\nNumber of CPUs (~ cores/threads) available\n\n\n$SLURM_MEM_PER_NODE\n--mem\nAmount of memory available (per node)\n\n\n$TMPDIR\nN/A\nPath to the Compute storage available during the job\n\n\n$SLURM_SUBMIT_DIR\nN/A\nPath to dir from which job was submitted.\n\n\n\n\nAs an example of how these environment variables can be useful, the command below uses $SLURM_CPUS_PER_TASK in its call to the program STAR inside the script:\nSTAR --runThreadN \"$SLURM_CPUS_PER_TASK\" --genomeDir ...\nWith this strategy, you will automatically use the correct (requested) number of cores, and don’t risk having a mismatch. Also, if you need to change the number of cores, you’ll only have to modify it in one place: in the resource request to Slurm.\n\n\n\n5.1 Interactive shell jobs\nInteractive shell jobs will grant you interactive shell access on a compute node. We’ve been working in a shell in VS Code Server, which means that we already have interactive shell access on a compute node!\nHowever, we only have access to 1 core and 4 GB of memory in this VS Code shell, and there is no way of changing this. If you want an interactive shell job with more resources, you’ll have to start one with Slurm commands.\nA couple of different commands can be used to start an interactive shell job. I prefer the general srun command7, which we can use with --pty /bin/bash added to get an interactive Bash shell.\nsrun --account=PAS2700 --pty /bin/bash\nsrun: job 12431932 queued and waiting for resources  \nsrun: job 12431932 has been allocated resources\n\n[...regular login info, such as quota, not shown...]\n\n[jelmer@p0133 PAS2700]$\nThere we go! First some Slurm scheduling info was printed to screen: initially, the job was queued, and then it was “allocated resources”: that is, computing resources such as a compute node were reserved for the job. After that:\n\nThe job starts and because we’ve reserved an interactive shell job, a new Bash shell is initiated: for that reason, we get to see our regular login info once again.\nWe have now moved to the compute node at which our interactive job is running, so you should have a different p number in your prompt.\n\n\n\n\n5.2 Table with sbatch options\nFirst, here are the options we discussed above:\n\n\n\n\n\n\n\n\n\nResource/use\nshort\nlong\ndefault\n\n\n\n\nProject to be billed\n-A PAS2700\n--account=PAS2700\nN/A\n\n\nTime limit\n-t 4:00:00\n--time=4:00:00\n1:00:00\n\n\nNr of nodes\n-N 1\n--nodes=1\n1\n\n\nNr of cores\n-c 1\n--cpus-per-task=1\n1\n\n\nNr of “tasks” (processes)\n-n 1\n--ntasks=1\n1\n\n\nNr of tasks per node\n-\n--ntasks-per-node\n1\n\n\nMemory limit per node\n-\n--mem=4G\n(4G)\n\n\nLog output file\n-o\n--output=slurm-fastqc-%j.out\n\n\n\nError output (stderr)\n-e\n--error=slurm-fastqc-%j.err\n\n\n\nGet email when job starts, ends,fails, or all of the above\n-\n--mail-type=START  --mail-type=END  --mail-type=FAIL  --mail-type=ALL\n\n\n\n\n\nAnd a couple of additional ones:\n\n\n\n\n\n\n\nResource/use\noption\n\n\n\n\nJob name (displayed in the queue)\n--job-name=fastqc\n\n\nPartition (=queue type)\n--partition=longserial  --partition=hugemem\n\n\nLet job begin only after a specific time\n--begin=2024-04-05T12:00:00\n\n\nLet job begin only after another job is done\n--dependency=afterany:123456"
  },
  {
    "objectID": "osc/05_slurm.html#footnotes",
    "href": "osc/05_slurm.html#footnotes",
    "title": "Slurm batch jobs at OSC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n This type of output is referred standard out (non-error output) and standard error — see the box in the section on Slurm log files for more↩︎\nUnless you actively “Delete” to job on the Ondemand website.↩︎\nEven though technically, one CPU often contains multiple cores.↩︎\nEven though technically, one core often contains multiple threads.↩︎\nFor instance, we might be running the FastQC script multiple times, and otherwise those would all have the same name and be overwritten.↩︎\nThe combination of using strict Bash settings (set -euo pipefail) and printing a line that marks the end of the script (echo \"Done with script\") makes it easy to spot scripts that failed, because they won’t have that marker line at the end of the Slurm log file.↩︎\nOther options: salloc works almost identically to srun, whereas sinteractive is an OSC convenience wrapper but with more limited options.↩︎"
  },
  {
    "objectID": "r/r_data-structures.html#introduction",
    "href": "r/r_data-structures.html#introduction",
    "title": "R’s data structures and data types",
    "section": "1 Introduction",
    "text": "1 Introduction\n\nWhat we’ll cover\nIn this session, we will learn about R’s data structures and data types.\n\nData structures are the kinds of objects that R can store data in. Here, we will cover the two most common ones: vectors and data frames.\nData types are how R distinguishes between different kinds of data like numbers and character strings. Here, we’ll talk about the 4 main data types: character, integer, double, and logical. We’ll also cover factors, a construct related to the data types.\n\n\n\nSetting up\nTo make it easier to keep track of what we do, we’ll write our code in a script (and send it to the console from there) – here is how to create and save a new R script:\n\nOpen a new R script (Click the + symbol in toolbar at the top, then click R Script)1.\nSave the script straight away as data-structures.R – you can save it anywhere you like, though it is probably best to save it in a folder specifically for this workshop.\nIf you want the section headers as comments in your script, as in the script I am showing you now, then copy-and-paste the following into your script:\n\n\n\nSection headers for your script (Click to expand)\n\n\n# 2 - Vectors ------------------------------------------------------------------\n# 2.1 - Single-element vectors and quoting\n\n# 2.2 - Multi-element vectors\n\n# 2.3 - Vectorization\n\n# Challenge 1\n# A. Start by making a vector x with the whole numbers 1 through 26.\n#    Then, subtract 0.5 from each element in the vector and save the result in vector y.\n#    Check your results by printing both vectors.\n\n# B. What do you think will be the result of the following operation?\n#    1:5 * 1:5\n\n# 2.4 - Exploring vectors\n\n# 2.5 - Extracting element from vectors\n\n# 3 - Data frames --------------------------------------------------------------\n# 3.1 - Data frame intro\n\n# 4 - Data types ---------------------------------------------------------------\n# 4.1 - R's main data types\n\n# 4.2 - Factors\n\n# 4.3 - A vector can only contain one data type\n\n# Challenge 2\n# What type of vector (if any) do you think each of the following will produce?\n# Try it out and see if you were right.\n#   typeof(\"TRUE\")\n#   typeof(banana)\n#   typeof(c(2, 6, \"3\"))\n# Bonus / trick question:\n#   typeof(18, 3)\n\n# 4.4 - Automatic type coercion\n# 4.5 - Manual type conversion"
  },
  {
    "objectID": "r/r_data-structures.html#data-structure-1-vectors",
    "href": "r/r_data-structures.html#data-structure-1-vectors",
    "title": "R’s data structures and data types",
    "section": "2 Data structure 1: Vectors",
    "text": "2 Data structure 1: Vectors\nThe first data structure we will explore is the simplest: the vector. A vector in R is essentially a collection of one or more items. Moving forward, we’ll call such individual items “elements”.\n\n2.1 Single-element vectors and quoting\nVectors can consist of just a single element, so each of the two lines of code below creates a vector:\n\nvector1 &lt;- 8\nvector2 &lt;- \"panda\"\n\nTwo things are worth noting about the \"panda\" example, which is a so-called character string (or string for short):\n\n\"panda\" constitutes one element, not 5 (its number of letters).\nUnlike when dealing with numbers, we have to quote the string.2\n\nCharacter strings need to be quoted because they are otherwise interpreted as R objects – for example, because our vectors vector1 and vector2 are objects, we refer to them without quotes:\n\n# [Note that R will show auto-complete options after you type 3 characters]\nvector1\n\n[1] 8\n\nvector2\n\n[1] \"panda\"\n\n\nTherefore, the code below doesn’t work, because there is no object called panda:\n\nvector_fail &lt;- panda\n\nError: object 'panda' not found\n\n\n\n\n\n2.2 Multi-element vectors\nA common way to make vectors with multiple elements is by using the c (combine) function:\n\nc(2, 6, 3)\n\n[1] 2 6 3\n\n\n\n\n\n\n\n\nUnlike in the first couple of vector examples, we didn’t save the above vector to an object: now the vector simply printed to the console – but it is created all the same.\n\n\n\nc() can also append elements to an existing vector:\n\n# First we create a vector:\nvector_to_append &lt;- c(\"vhagar\", \"meleys\")\nvector_to_append\n\n[1] \"vhagar\" \"meleys\"\n\n# Then we append another element to it:\nc(vector_to_append, \"balerion the dread\")\n\n[1] \"vhagar\"             \"meleys\"             \"balerion the dread\"\n\n\n\nTo create vectors with series of numbers, a couple of shortcuts are available. First, you can make series of whole numbers with the : operator:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nSecond, you can use a function like seq() for fine control over the sequence:\n\nmyseq &lt;- seq(from = 6, to = 8, by = 0.2)\nmyseq\n\n [1] 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0\n\n\n\n\n\n2.3 Vectorization\nConsider the output of this command:\n\nmyseq * 2\n\n [1] 12.0 12.4 12.8 13.2 13.6 14.0 14.4 14.8 15.2 15.6 16.0\n\n\nAbove, every individual element in myseq was multiplied by 2. We call this behavior “vectorization” and this is a key feature of the R language. (Alternatively, you may have expected this code to repeat myseq twice, but this did not happen!)\n\n\n\n\n\n\nFor more about vectorization, see episode 9 from the Carpentries lesson that this material is based on.\n\n\n\n\n\n\n Challenge 1\n\nA. Start by making a vector x with the whole numbers 1 through 26. Then, subtract 0.5 from each element in the vector and save the result in vector y. Check your results by printing both vectors.\n\n\nClick for the solution\n\n\nx &lt;- 1:26\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26\n\ny &lt;- x - 0.5\ny\n\n [1]  0.5  1.5  2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5 10.5 11.5 12.5 13.5 14.5\n[16] 15.5 16.5 17.5 18.5 19.5 20.5 21.5 22.5 23.5 24.5 25.5\n\n\n\n\nB. What do you think will be the result of the following operation? Try it out and see if you were right.\n\n1:5 * 1:5\n\n\n\nClick for the solution\n\n\n1:5 * 1:5\n\n[1]  1  4  9 16 25\n\n\nBoth vectors are of length 5 which will lead to “element-wise matching”: the first element in the first vector will be multiplied with the first element in the second vector, the second element in the first vector will be multiplied with the second element in the second vector, and so on.\n\n\n\n\n2.4 Exploring vectors\nR has many built-in functions to get information about vectors and other types of objects, such as:\nGet the first and last few elements, respectively, with head() and tail():\n\n# Print the first 6 elements:\nhead(myseq)\n\n[1] 6.0 6.2 6.4 6.6 6.8 7.0\n\n# Both head and tail take an argument `n` to specify the number of elements to print:\nhead(myseq, n = 2)\n\n[1] 6.0 6.2\n\n# Print the last 6 elements:\ntail(myseq)\n\n[1] 7.0 7.2 7.4 7.6 7.8 8.0\n\n\n\nGet the number of elements with length():\n\nlength(myseq)\n\n[1] 11\n\n\n\nGet arithmetic summaries like sum() and mean() for vectors with numbers:\n\n# sum() will sum the values of all elements\nsum(myseq)\n\n[1] 77\n\n# mean() will compute the mean (average) across all elements\nmean(myseq)\n\n[1] 7\n\n\n\n\n\n2.5 Extracting elements from vectors\nExtracting element from objects like vectors is often referred to as “indexing”. In R, we can do this using bracket notation – for example:\n\nGet the second element:\n\nmyseq[2]\n\n[1] 6.2\n\n\nGet the second through the fifth elements:\n\nmyseq[2:5]\n\n[1] 6.2 6.4 6.6 6.8\n\n\nGet the first and eight elements:\n\nmyseq[c(1, 8)]\n\n[1] 6.0 7.4\n\n\n\nTo put this in a general way: we can extract elements from a vector by using another vector, whose values are the positional indices of the elements in the original vector."
  },
  {
    "objectID": "r/r_data-structures.html#data-structure-2-data-frames",
    "href": "r/r_data-structures.html#data-structure-2-data-frames",
    "title": "R’s data structures and data types",
    "section": "3 Data structure 2: Data frames",
    "text": "3 Data structure 2: Data frames\n\n3.1 R stores tabular data in “data frames”\nOne of R’s most powerful features is its built-in ability to deal with tabular data – i.e., data with rows and columns like you are familiar with from spreadsheets like those you create with Excel.\nIn R, tabular data is stored in objects that are called “data frames”, the second R data structure we’ll cover in some depth. Let’s start by making a toy data frame with information about 3 cats:\n\ncats &lt;- data.frame(\n  name = c(\"Luna\", \"Thomas\", \"Daisy\"),\n  coat = c(\"calico\", \"black\", \"tabby\"),\n  weight = c(2.1, 5.0, 3.2)\n  )\n\ncats\n\n    name   coat weight\n1   Luna calico    2.1\n2 Thomas  black    5.0\n3  Daisy  tabby    3.2\n\n\nAbove:\n\nWe created 3 vectors and pasted them side-by-side to create a data frame in which each vector constitutes a column.\nWe gave each vector a name (e.g., coat), and those names became the column names.\nThe resulting data frame has 3 rows (one for each cat) and 3 columns (each with a type of info about the cats, like coat color).\n\nData frames are typically (and best) organized like above, where:\n\nEach column contains a different “variable” (e.g. coat color, weight)\nEach row contains a different “observation” (data on e.g. one cat/person/sample)\n\nThat’s all we’ll say about data frames for now, but in today’s remaining sessions we will explore this key R data structure more!"
  },
  {
    "objectID": "r/r_data-structures.html#data-types",
    "href": "r/r_data-structures.html#data-types",
    "title": "R’s data structures and data types",
    "section": "4 Data types",
    "text": "4 Data types\n\n4.1 R’s main Data Types\nR distinguishes different kinds of data, such as character strings and numbers, in a formal way, using several pre-defined “data types”. The behavior of R in various operations will depend heavily on the data type – for example, the below fails:\n\n\"valerion\" * 5\n\nError in \"valerion\" * 5: non-numeric argument to binary operator\n\n\nWe can ask what type of data something is in R using the typeof() function:\n\ntypeof(\"valerion\")\n\n[1] \"character\"\n\n\nR sets the data type of \"valerion\" to character, which we commonly refer to as character strings or strings. In formal terms, the failed command did not work because R will not allow us to perform mathematical functions on vectors of type character.\nThe character data type most commonly contains letters, but anything that is placed between quotes (\"...\") will be interpreted as the character data type — even plain numbers:\n\ntypeof(\"5\")\n\n[1] \"character\"\n\n\n\nBesides character, the other 3 common data types are:\n\ndouble / numeric – numbers that can have decimal points:\n\ntypeof(3.14)\n\n[1] \"double\"\n\n\ninteger – whole numbers only:\n\ntypeof(1:3)\n\n[1] \"integer\"\n\n\nlogical (either TRUE or FALSE – unquoted!):\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\n\n\n\n\n\n4.2 Factors\nCategorical data, like treatments in an experiment, can be stored as “factors” in R. Factors are useful for statistical analyses and for plotting, e.g. because they allow you to specify a custom order.\n\ndiet_vec &lt;- c(\"high\", \"medium\", \"low\", \"low\", \"medium\")\ndiet_vec\n\n[1] \"high\"   \"medium\" \"low\"    \"low\"    \"medium\"\n\nfactor(diet_vec)\n\n[1] high   medium low    low    medium\nLevels: high low medium\n\n\nIn the example above, we turned a character vector into a factor. Its “levels” (low, medium, high) are sorted alphabetically by default, but we can manually specify an order that makes more sense:\n\ndiet_fct &lt;- factor(diet_vec, levels = c(\"low\", \"medium\", \"high\"))\ndiet_fct\n\n[1] high   medium low    low    medium\nLevels: low medium high\n\n\nThis ordering would be automatically respected in plots and statistical analyses.\n\n\n\n\n\n\n\nOddly, factors are technically not a data type (Click to expand)\n\n\n\n\n\nFor most intents and purposes, it makes sense to think of factors as another data type, even though technically, they are a kind of data structure build on the integer data type:\n\ntypeof(diet_fct)\n\n[1] \"integer\"\n\n\n\n\n\n\n\n\n4.3 A vector can only contain one data type\nIndividual vectors, and therefore also individual columns in data frames, can only be composed of a single data type.\nR will silently pick the “best-fitting” data type when you enter or read data into a data frame. So let’s see what the data types are in our cats data frame:\n\nstr(cats)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"Luna\" \"Thomas\" \"Daisy\"\n $ coat  : chr  \"calico\" \"black\" \"tabby\"\n $ weight: num  2.1 5 3.2\n\n\n\nThe name and coat columns are character, abbreviated chr.\nThe weight column is double/numeric, abbreviated num.\n\n\n\n\n Challenge 2\nWhat type of vector (if any) do you think each of the following will produce? Try it out and see if you were right.\n\ntypeof(\"TRUE\")\ntypeof(banana)\ntypeof(c(2, 6, \"3\"))\n\nBonus / trick question:\n\ntypeof(18, 3)\n\n\n\nClick for the solutions\n\n\ntypeof(\"TRUE\")\n\n[1] \"character\"\n\n\n\n\"TRUE\" is character (and not logical) because of the quotes around it.\n\n\ntypeof(banana)\n\nError: object 'banana' not found\n\n\n\nRecall the earlier example: this returns an error because the object banana does not exist. Any unquoted string (that is not a special keyword like TRUE and FALSE) is interpreted as a reference to an object in R.\n\n\ntypeof(c(2, 6, \"3\"))\n\n[1] \"character\"\n\n\n\nWe’ll talk about why this produces a character vector in the next section.\n\n\ntypeof(18, 3)\n\nError in typeof(18, 3): unused argument (3)\n\n\n\nThis produces an error because the typeof() only accepts a single argument, which is an R object like a vector. Because we did not wrap 18, 3 within c() (i.e. we did not use c(18, 3)), we ended up passing two arguments to the function, and this resulted in an error.\nIf you guessed that it would have TWICE returned integer (or double), you were on the right track: you couldn’t have known that the function does not accept multiple objects.\n\n\n\n\n\n4.4 Automatic Type Coercion\nThat a character vector was returned by c(2, 6, \"3\") in the challenge above is due to something called type coercion.\nWhen R encounters a mix of types (here, numbers and characters) to be combined into a single vector, it will force them all to be the same type. It “must” do this because, as pointed out above, a vector can consist of only a single data type.\nType coercion can be the source of many surprises, and is one reason we need to be aware of the basic data types and how R will interpret them.\n\n\n\n4.5 Manual Type Conversion\nLuckily, you are not simply at the mercy of whatever R decides to do automatically, but can convert vectors at will using the as. group of functions:\n\n\n\n\n\n\nTry to use RStudio’s auto-complete functionality here: type “as.” and then press the Tab key.\n\n\n\n\n\n\n\nas.integer(c(\"0\", \"2\", \"4\"))\n\n[1] 0 2 4\n\nas.character(c(0, 2, 4))\n\n[1] \"0\" \"2\" \"4\"\n\n\nAs you may have guessed, though, not all type conversions are possible — for example:\n\nas.double(\"kiwi\")\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\n\n(NA is R’s way of denoting missing data – see this bonus section for more.)"
  },
  {
    "objectID": "r/r_data-structures.html#bonus-material-for-self-study",
    "href": "r/r_data-structures.html#bonus-material-for-self-study",
    "title": "R’s data structures and data types",
    "section": "5 Bonus material for self-study",
    "text": "5 Bonus material for self-study\n\n5.1 Changing vector elements using indexing\nAbove, we saw how we can extract elements of a vector using indexing. To change elements in a vector, simply use the bracket on the other side of the arrow – for example:\n\nChange the first element to 30:\n\nmyseq[1] &lt;- 30\nmyseq\n\n [1] 30.0  6.2  6.4  6.6  6.8  7.0  7.2  7.4  7.6  7.8  8.0\n\n\nChange the last element to 0:\n\nmyseq[length(myseq)] &lt;- 0\nmyseq\n\n [1] 30.0  6.2  6.4  6.6  6.8  7.0  7.2  7.4  7.6  7.8  0.0\n\n\nChange the second element to the mean value of the vector:\n\nmyseq[2] &lt;- mean(myseq)\nmyseq\n\n [1] 30.000000  8.454545  6.400000  6.600000  6.800000  7.000000  7.200000\n [8]  7.400000  7.600000  7.800000  0.000000\n\n\n\n\n\n\n5.2 Extracting columns from a data frame\nWe can extract individual columns from a data frame using the $ operator:\n\ncats$weight\n\n[1] 2.1 5.0 3.2\n\ncats$coat\n\n[1] \"calico\" \"black\"  \"tabby\" \n\n\nThis kind of operation will return a vector – and can be indexed as well:\n\ncats$weight[2]\n\n[1] 5\n\n\n\n\n\n5.3 More on the logical data type\nLet’s add a column to our cats data frame indicating whether each cat does or does not like string:\n\ncats$likes_string &lt;- c(1, 0, 1)\ncats\n\n    name   coat weight likes_string\n1   Luna calico    2.1            1\n2 Thomas  black    5.0            0\n3  Daisy  tabby    3.2            1\n\n\nSo, likes_string is numeric, but the 1s and 0s actually represent TRUE and FALSE.\nWe could instead use the logical data type here, by converting this column with the as.logical() function, which will turn 0’s into FALSE and everything else, including 1, to TRUE:\n\nas.logical(cats$likes_string)\n\n[1]  TRUE FALSE  TRUE\n\n\nAnd to actually modify this column in the dataframe itself, we would do this:\n\ncats$likes_string &lt;- as.logical(cats$likes_string)\ncats\n\n    name   coat weight likes_string\n1   Luna calico    2.1         TRUE\n2 Thomas  black    5.0        FALSE\n3  Daisy  tabby    3.2         TRUE\n\n\n\nYou might think that 1/0 could be a handier coding than TRUE/FALSE because it may make it easier, for exmaple, to count the number of times something is true or false. But consider the following:\n\nTRUE + TRUE\n\n[1] 2\n\n\nSo, logicals can be used as if they were numbers, in which case FALSE represents 0 and TRUE represents 1.\n\n\n\n5.4 Missing values (NA)\nR has a concept of missing data, which is important in statistical computing, as not all information/measurements are always available for each sample.\nIn R, missing values are coded as NA (and like TRUE/FALSE, this is not a character string, so it is not quoted):\n\n# This vector will contain one missing value\nvector_NA &lt;- c(1, 3, NA, 7)\nvector_NA\n\n[1]  1  3 NA  7\n\n\nA key thing to be aware of with NAs is that many functions that operate on vectors will return NA if any element in the vector is NA:\n\nsum(vector_NA)\n\n[1] NA\n\n\nThe way to get around this is by setting na.rm = TRUE in such functions, for example:\n\nsum(vector_NA, na.rm = TRUE)\n\n[1] 11\n\n\n\n\n\n5.5 A few other data structures in R\nWe did not go into details about R’s other data structures, which are less common than vectors and data frames. Two that are worth mentioning briefly, though, are:\n\nMatrix, which can be convenient when you have tabular data that is exclusively numeric (excluding names/labels).\nList, which is more flexible (and complicated) than vectors: it can contain multiple data types, and can also be hierarchically structured.\n\n\n\n\n Bonus Challenge\nAn important part of every data analysis is cleaning input data. Here, you will clean a cat data set that has an added observation with a problematic data entry.\nStart by creating the new data frame:\n\ncats_v2 &lt;- data.frame(\n  name = c(\"Luna\", \"Thomas\", \"Daisy\", \"Oliver\"),\n  coat = c(\"calico\", \"black\", \"tabby\", \"tabby\"),\n  weight = c(2.1, 5.0, 3.2, \"2.3 or 2.4\")\n)\n\nThen move on to the tasks below, filling in the blanks (_____) and running the code:\n\n# 1. Explore the data frame,\n#    including with an overview that shows the columns' data types:\ncats_v2\n_____(cats_v2)\n\n# 2. The \"weight\" column has the incorrect data type _____.\n#    The correct data type is: _____.\n\n# 3. Correct the 4th weight with the mean of the two given values,\n#    then print the data frame to see the effect:\ncats_v2$weight[4] &lt;- 2.35\ncats_v2\n\n# 4. Convert the weight column to the right data type:\ncats_v2$weight &lt;- _____(cats_v2$weight)\n\n# 5. Calculate the mean weight of the cats:\n_____\n\n\n\nClick for the solution\n\n\n# 1. Explore the data frame,\n#    including with an overview that shows the columns' data types:\ncats_v2\n\n    name   coat     weight\n1   Luna calico        2.1\n2 Thomas  black          5\n3  Daisy  tabby        3.2\n4 Oliver  tabby 2.3 or 2.4\n\nstr(cats_v2)\n\n'data.frame':   4 obs. of  3 variables:\n $ name  : chr  \"Luna\" \"Thomas\" \"Daisy\" \"Oliver\"\n $ coat  : chr  \"calico\" \"black\" \"tabby\" \"tabby\"\n $ weight: chr  \"2.1\" \"5\" \"3.2\" \"2.3 or 2.4\"\n\n# 2. The \"weight\" column has the incorrect data type CHARACTER.\n#    The correct data type is: DOUBLE.\n\n# 3. Correct the 4th weight data point with the mean of the two given values,\n#    then print the data frame to see the effect:\ncats_v2$weight[4] &lt;- 2.35\ncats_v2\n\n    name   coat weight\n1   Luna calico    2.1\n2 Thomas  black      5\n3  Daisy  tabby    3.2\n4 Oliver  tabby   2.35\n\n# 4. Convert the weight column to the right data type:\ncats_v2$weight &lt;- as.double(cats_v2$weight)\n\n# 5. Calculate the mean weight of the cats:\nmean(cats_v2$weight)\n\n[1] 3.1625\n\n\n\n\n\n\n5.6 Learn more\nTo learn more about data types and data structures, see this episode from a separate Carpentries lesson."
  },
  {
    "objectID": "r/r_data-structures.html#footnotes",
    "href": "r/r_data-structures.html#footnotes",
    "title": "R’s data structures and data types",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Or Click File =&gt; New file =&gt; R Script.↩︎\n Either double quotes (\"...\") or single quotes ('...') work, but the former are most commonly used by convention.↩︎"
  },
  {
    "objectID": "rnaseq/01_data.html#introduction",
    "href": "rnaseq/01_data.html#introduction",
    "title": "RNA-seq data files",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n1.1 Our dataset\nWe will work with RNA-seq data from the paper “Two avian Plasmodium species trigger different transcriptional responses on their vector Culex pipiens”, published in 2023 in Molecular Ecology (link):\n\n\n\n\n\nThis paper uses RNA-seq data to study gene expression in Culex pipiens mosquitos infected with malaria-causing Plasmodium protozoans — specifically, it compares mosquitos according to:\n\nInfection status: Plasmodium cathemerium vs. P. relictum vs. control\nTime after infection: 24 h vs. 10 days vs. 21 days\n\n\n\n\n1.2 What we will do\nToday we’ll focus on the general aspects of reference genomes and HTS data — specifically, we will:\n\nSee how you can find a reference genome and the associated files\nExplore the reference genome files\nExplore our HTS reads\nPerform quality-control on some of our reads with a command-line tool, FastQC\n\nNext week, after covering RNA-seq methodology in the lecture, we will run a differential expression analysis.\n\n\n\n1.3 Getting your own copy of the files\nThe main data files in a reference-based HTS project are reference genome files and sequence reads. We’ll discuss those files in more detail below — first, let’s get everyone their own copy of the data.\n\nGo to your personal dir in /fs/scratch/PAS2250/ENT6703:\n\n# Replace `jelmer` with your personal dir's name!\ncd /fs/scratch/PAS2250/ENT6703/jelmer\n\nThen, use the Unix copy command cp as follows (yes, there’s a space + period at the end!):\n\ncp -rv ../share/data .\n‘/fs/scratch/PAS2250/ENT6703/share/data’ -&gt; ‘./data’\n‘/fs/scratch/PAS2250/ENT6703/share/data/meta’ -&gt; ‘./data/meta’\n‘/fs/scratch/PAS2250/ENT6703/share/data/meta/metadata.tsv’ -&gt; ‘./data/meta/metadata.tsv’\n‘/fs/scratch/PAS2250/ENT6703/share/data/ref’ -&gt; ‘./data/ref’\n‘/fs/scratch/PAS2250/ENT6703/share/data/ref/GCF_016801865.2.gtf’ -&gt; ‘./data/ref/GCF_016801865.2.gtf’\n‘/fs/scratch/PAS2250/ENT6703/share/data/ref/GCF_016801865.2.fna’ -&gt; ‘./data/ref/GCF_016801865.2.fna’\n‘/fs/scratch/PAS2250/ENT6703/share/data/fastq’ -&gt; ‘./data/fastq’\n‘/fs/scratch/PAS2250/ENT6703/share/data/fastq/ERR10802868_R2.fastq.gz’ -&gt; ‘./data/fastq/ERR10802868_R2.fastq.gz’\n‘/fs/scratch/PAS2250/ENT6703/share/data/fastq/ERR10802863_R1.fastq.gz’ -&gt; ‘./data/fastq/ERR10802863_R1.fastq.gz’\n‘/fs/scratch/PAS2250/ENT6703/share/data/fastq/ERR10802880_R2.fastq.gz’ -&gt; ‘./data/fastq/ERR10802880_R2.fastq.gz’\n‘/fs/scratch/PAS2250/ENT6703/share/data/fastq/ERR10802880_R1.fastq.gz’ -&gt; ‘./data/fastq/ERR10802880_R1.fastq.gz’\n# [...truncated...]\n\n\n\n\n\n\nIn the command above:\n\n\n\n\nOption -r will enable “recursive” (=dirs, not just files) copying\nOption -v will turn on “verbose” output: it will report what it’s copying\nThe first argument (../share/data) is the source directory, with .. meaning one dir “up”\nThe second argument is the target directory: . means the current working dir\n\n\n\n\nNow, use the tree command (-C to show colors) to recursively list files your newly copied files in a way that gives a nice overview:\n\ntree -C\n# (Unfortunately colors aren't shown in the output on the website)\n.\n└── data\n    ├── fastq\n    │   ├── ERR10802863_R1.fastq.gz\n    │   ├── ERR10802863_R2.fastq.gz\n    │   ├── ERR10802864_R1.fastq.gz\n    │   ├── ERR10802864_R2.fastq.gz\n    │   ├── [...truncated - more FASTQ files...]\n    ├── meta\n    │   └── metadata.tsv\n    └── ref\n        ├── GCF_016801865.2.fna\n        └── GCF_016801865.2.gtf\n4 directories, 47 files\nAs we saw earlier, we have a whole bunch of FASTQ files (.fastq.gz extension, the HTS reads), a metadata file, and two reference genomes files (.fna and .gtf). You’ll take a closer look at each of those below.\n\n\n\n1.4 Viewing the metadata\nYou’ll first look at the “metadata” associated with the samples analyzed in this paper, such as the treatment information for each sample, and a sample ID that can be used to match them to the files with reads.\nThe metadata file metadata.tsv (tsv for “tab-separated values”) is in the folder data/meta:\nls -lh data/meta\n-rw-r--r-- 1 jelmer PAS0471 644 Jan 21 09:15 metadata.tsv\nYou can find this file in the VS Code side bar and click on it to open it in the editor. Alternatively, you could use the cat command to show the file contents in the shell:\ncat data/meta/metadata.tsv\nsample_id    time     treatment\nERR10802882  10_days  cathemerium\nERR10802875  10_days  cathemerium\nERR10802879  10_days  cathemerium\nERR10802883  10_days  cathemerium\nERR10802878  10_days  control\nERR10802884  10_days  control\nERR10802877  10_days  control\nERR10802881  10_days  control\nERR10802876  10_days  relictum\nERR10802880  10_days  relictum\nERR10802885  10_days  relictum\nERR10802886  10_days  relictum\nERR10802864  24_h     cathemerium\nERR10802867  24_h     cathemerium\nERR10802870  24_h     cathemerium\nERR10802866  24_h     control\nERR10802869  24_h     control\nERR10802863  24_h     control\nERR10802871  24_h     relictum\nERR10802874  24_h     relictum\nERR10802865  24_h     relictum\nERR10802868  24_h     relictum\n\n\n Your Turn: Based on this metadata, try to understand the experimental design (Click to see pointers)\n\n\nThe time column contains the amount of time after infection, with “h” short for hours.\nThe treatment column contains the treatment: which Plasmodium species, or “control” (not infected).\nWe have 3 treatments across each of two timepoints, with a number of replicates per treatment-timepoint combination.\n\n\n\n\n Your Turn: How many biological replicates are there? Is a treatment missing relative to what was described above and in the paper? (Click to see the answers)\n\n\nThere are 4 replicates per time x treatment combination, except in two cases (the paper says those two samples were removed from the final analysis).\nThe 21-days timepoint is missing: I removed it to simplify the dataset a bit.\n\n\n\n\n\n\n\n\nSide note: How did I retrieve this paper’s data? (Click to expand)\n\n\n\n\n\nAt the end of the paper, there is a section called “Open Research” with a “Data Availability Statement”, which reads:\n\nRaw sequences generated in this study have been submitted to the European Nucleotide Archive ENA database (https://www.ebi.ac.uk/ena/browser/home) under project accession number PRJEB41609, Study ERP125411. Sample metadata are available at https://doi.org/10.20350/digitalCSIC/15708.\n\nI used the second link above to download the metadata, which I slightly edited to simplify. I used the project accession number PRJEB41609 to directly download the raw sequences (i.e., the FASTQ files we’ll explore below) to OSC using a command-line tool called fastq-dl."
  },
  {
    "objectID": "rnaseq/01_data.html#reference-genome-files",
    "href": "rnaseq/01_data.html#reference-genome-files",
    "title": "RNA-seq data files",
    "section": "2 Reference genome files",
    "text": "2 Reference genome files\nWe’ll cover the two main types of reference genome files you need in a HTS project like reference-based RNA-seq:\n\nFASTA files: Files with just sequences and their IDs. Your reference genome assembly is in this format.\nGTF (& GFF) files: These contain annotations in a tabular format, e.g. the start & stop position of each gene.\n\n\n2.1 Finding your reference genome\nImagine that you are one of the researchers involved in the Culex study — or alternatively, that you are still you, but you just want to redo their analysis.\nYou’ll want to see if there’s a Cx. pipiens reference genome available, and if so, download the relevant files. The authors state the following in the paper (section 2.3, “Data analysis”):\n\nBecause the reference genome and annotations of Cx. pipiens are not published yet, we used the reference genome and annotations of phylogenetically closest species that were available in Ensembl, Cx. quinquefasciatus.\n\nBut perhaps that genome of Cx. pipiens has been published in the meantime?\nGenerally, the first place to look reference genome data is at NCBI, https://ncbi.nlm.nih.gov, where you can start by simply typing the name of your organism in the search box at the home page — by means of example, let’s first search for the hoverfly Episyrphus balteatus:\n\n\n\n\n\nFor this species, we get the following “card” at the top of the results:\n\n\n\n\n\nIf you next click on “Genomes” in the result page above, you should get the following:\n\n\n\n\n\nSo, NCBI has 3 genomes assemblies for Episyrphus balteatus. The top one has a green check mark next to it (which means that this genome has been designated the primary reference genome for the focal organism), and it is also the only genome with an entry in the Annotation column and with a “Chromosome” (vs. “Scaffold”) assembly Level. Therefore, that top assembly, idEpiBalt1.1, would be the one to go with.\nYou can click on each assembly to get more information, including statistics like the number of scaffolds.\n\n\n Your Turn: Now let’s switch to Culex. How many Culex assemblies are on NCBI (do a genus-wide search)? Are there any for Culex pipiens, and if so, which would you pick? (Click for the answer)\n\nGo through the same process as shown above for Episyrphus balteatus, instead entering “Culex” in the search box.\nYou should find that there are 5 Culex assemblies, 2 of which are Culex pipiens. The first one, TS_CPP_V2, has the reference checkmark next to it and has an entry in the Annotation column, which the second one (TS_CPM_V1) doesn’t:\n\n\n\n\n\n(These two are also from different subspecies, but as far as I could see, the authors of our study don’t specify the focal subspecies – though you could probably figure that out based on geographic range.)\n\nAs shown in the solutions above, there is currently a reference genome for Cx. pipiens available1, and we’ll be “using” (looking at) that one. I have downloaded its files for you, which were among the files you just copied.\n\n\n Your Turn: Take a closer look at our focal genome on the NCBI website. What is the size of the genome assembly? How many chromosomes and scaffolds does it contain? And how many protein-coding genes? (Click for the solutions)\n\nOn the genome’s page at NCBI, some of the information includes the following stats on the assembly and the annotation:\n\n\n\n\n\n\n\n\n\n\nSo:\n\nIt is 566.3 Mb (Megabases)\nIt contains 3 chromosomes and 289 unplaced scaffolds\nIt has 16,297 protein-coding genes\n\n\n\n\n\n\n\n\nSide note: Downloading the reference genome files (Click to expand)\n\n\n\n\n\nIf you wanted to download the reference genome files from the NCBI website, you could either select an assembly in the overview table and click the Download button, or click the Download button at the top of the page for a specific assembly. That should get you the following pop-up window:\n\n\n\n\n\nYou’ll want to select at least the “Genome sequences (FASTA)” and one or both of the “Annotation features” files (GTF is often preferred with RNA-seq).\nThis allows you to download the data to your computer, and you could then upload it OSC. (Though a faster and more reproducible solution would be to use a command in your OSC shell to directly download these — the datasets and curl buttons next to the Download one a genome’s page help with that.)\nFinally, if a “RefSeq” assembly is available, like it is for this genome, you’ll want to select that, as it has been curated and standardized by NCBI (whereas “GenBank” entries are exactly as submitted by researchers). This mostly makes a difference for the annotation rather than the assembly itself.\n\n\n\n\n\n\n\n\n\nSide note: Reference genome complications (Click to expand)\n\n\n\n\n\nOther useful database for reference genomes are Ensembl and the specialized databases that exist for certain organisms, like FlyBase for Drosophila, VectorBase mostly for mosquitos, and JGI Phytozome for plants.\nIn many cases, these databases don’t contain the exact same reference genome files than NCBI. Often, at least the annotation is different (and actually the product of an independent annotation effort), but even the assembly may have small differences including in chromosome/scaffold names, which can make these files completely incompatible. And to make matters even more complicated, it is not always clear which database is the best source for your genome.\n\nInterestingly, the paper with the Cx. pipiens reference genome that we just found was already published in 2021, well before our focal Molecular Ecology paper.\nAnd when we take a closer look at the quote from the paper, they say no genome for Cx. pipiens is “available in Ensembl” — which is in fact still the case. Could it be that the authors preferred the Ensembl genome from Cx. quinquefasciatus over the NCBI genome of Cx. pipiens? Or perhaps they just did their analyses already several years ago?\n\n\n\n\n\n\n2.2 Reference genome files I: FASTA\n\nThe FASTA format\nFASTA files contain one or more DNA or amino acid sequences, with no limits on the number of sequences or the sequence lengths. FASTA is the standard format for, e.g.:\n\nGenome assembly sequences\nTranscriptomes and proteomes (all of an organism’s transcripts & amino acid sequences, respectively)\nSequence downloads from NCBI such as a single gene/protein or other GenBank entry\n\nThe following example FASTA file contains two entries:\n&gt;unique_sequence_ID Optional description\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n&gt;unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\nEach entry consists of a header line and the sequence itself. Header lines start with a &gt; (greater-than sign) and are otherwise “free form”, though the idea is that they provide an identifier for the sequence that follows.2\n\n\n\n\n\n\nFASTA file name extensions are variable: .fa, .fasta, .fna, .faa (Click to expand)\n\n\n\n\n\n\n“Generic” extensions are .fasta and .fa (e.g: culex_assembly.fasta)\nAlso used are extensions that explicitly indicate whether sequences are nucleotides (.fna) or amino acids (.faa)\n\n\n\n\n\n\nYour Culex genome assembly FASTA\nYour reference genome files are in data/ref:\nls -lh data/ref\n-rw------- 1 jelmer PAS0471 547M Jan 22 12:34 GCF_016801865.2.fna\n-rw------- 1 jelmer PAS0471 123M Jan 22 12:34 GCF_016801865.2.gtf\nWhile we can easily open small to medium-size files in the editor pane, “visual editors” like that do not work well for larger files like these.\nA handy command to view text files of any size is less, which opens them up in a “pager” within your shell – you’ll see what that means if you try it with one of the assembly FASTA file:\nless data/ref/GCF_016801865.2.fna\n&gt;NC_068937.1 Culex pipiens pallens isolate TS chromosome 1, TS_CPP_V2, whole genome shotgun sequence\naagcccttttatggtcaaaaatatcgtttaacttgaatatttttccttaaaaaataaataaatttaagcaaacagctgag\ntagatgtcatctactcaaatctacccataagcacacccctgttcaatttttttttcagccataagggcgcctccagtcaa\nattttcatattgagaatttcaatacaattttttaagtcgtaggggcgcctccagtcaaattttcatattgagaatttcaa\ntacatttttttatgtcgtaggggcgcctccagtcaaattttcatattgagaatttcaatacattttttttaagtcgtagg\nggcgcctccagtcaaattttcatattgagaatttcaatacatttttttaagtcttaggggcgcctccagtcaaattttca\ntattgagaatttcaatacatttttttaagtcgtaggggcgcctccagtcaaattttcatattgagaattttaatacaatt\nttttaaatcctaggggcgccttcagacaaacttaatttaaaaaatatcgctcctcgacttggcgactttgcgactgactg\ncgacagcactaccttggaacactgaaatgtttggttgactttccagaaagagtgcatatgacttgaaaaaaaaagagcgc\nttcaaaattgagtcaagaaattggtgaaacttggtgcaagcccttttatggttaaaaatatcgtttaacttgaatatttt\ntccttaaaaaataaataaatttaagcaaacagctgagtagatgtcatctactcaaatctacccataagcacacccctgga\nCCTAATTCATGGAGGTGAATAGAGCATACGTAAATACAAAACTCATGACATTAGCCTGTAAGGATTGTGTaattaatgca\naaaatattgaTAGAATGAAAGATGCAAGTCccaaaaattttaagtaaatgaATAGTAATCATAAAGATAActgatgatga\n\n\n Your Turn: Explore the file with less (Click to see the instructions)\n\nAfter running the command above, the file should have “opened” inside the less pager.\nYou can move around in the file in several ways: by scrolling with your mouse, with up and down arrows, or, if you have them, PgUp and PgDn keys (also, u will move up half a page and d will move down half a page).\nIf you find yourself scrolling down and down to try and reach the end of the file, you can instead press G to go to the very end right away (and g to go back to the top).\nNotice that you are “inside” this pager and won’t have your shell prompt back until you press q to quit less.\n\n\n\n\n\n\n\nSide note: Lowercase vs. uppercase nucleotide letters? (Click to expand)\n\n\n\n\n\nAs you have probably noticed, nucleotide bases are typically typed in uppercase (A, C, G, T). What does the mixture of lowercase and uppercase bases in the Cx. pipiens assembly FASTA mean, then?\nLowercase bases are what is called “soft-masked”: they are repetitive sequences, and bioinformatics programs will treat them differently than non-repetitive sequences, which are in uppercase.\n\n\n\n\n\n\n\n2.3 Reference genome files II: GFF/GTF\n\nThe GFF/GTF format\nThe GTF and GFF formats are very similar tab-delimited tabular files that contain genome annotations, with:\n\nOne row for each annotated “genomic feature” (gene, exon, etc.)\nOne column for each piece of information about a feature, like its genomic coordinates\n\nSee the sample below, with an added header line (not normally present) with column names:\nseqname     source  feature start   end     score  strand  frame    attributes\nNC_000001   RefSeq  gene    11874   14409   .       +       .       gene_id \"DDX11L1\"; transcript_id \"\"; db_xref \"GeneID:100287102\"; db_xref \"HGNC:HGNC:37102\"; description \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; gbkey \"Gene\"; gene \"DDX11L1\"; gene_biotype \"transcribed_pseudogene\"; pseudo \"true\"; \nNC_000001   RefSeq  exon    11874   12227   .       +       .       gene_id \"DDX11L1\"; transcript_id \"NR_046018.2\"; db_xref \"GeneID:100287102\"; gene \"DDX11L1\"; product \"DEAD/H-box helicase 11 like 1 (pseudogene)\"; pseudo \"true\"; \nSome details on the more important/interesting columns:\n\nseqname — Name of the chromosome, scaffold, or contig\nfeature — Name of the feature type, e.g. “gene”, “exon”, “intron”, “CDS”\nstart & end — Start & end position of the feature\nstrand — Whether the feature is on the + (forward) or - (reverse) strand\nattribute — A semicolon-separated list of tag-value pairs with additional information\n\n\n\nYour Culex GTF file\nFor our Cx. pipiens reference genome, we only have a GTF file.3\nTake a look at it, again with less, but now with the -S option:\nless -S data/ref/GCF_016801865.2.gtf\n#gtf-version 2.2\n#!genome-build TS_CPP_V2\n#!genome-build-accession NCBI_Assembly:GCF_016801865.2\n#!annotation-source NCBI RefSeq GCF_016801865.2-RS_2022_12\nNC_068937.1     Gnomon  gene    2046    110808  .       +       .       gene_id \"LOC120427725\"; transcript_id \"\"; db_xref \"GeneID:120427725\"; description \"homeotic protein deformed\"; gbkey \"Gene\"; gene \"LOC120427725\"; gene_biotype \"protein_coding\"; \nNC_068937.1     Gnomon  transcript      2046    110808  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; \nNC_068937.1     Gnomon  exon    2046    2531    .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1     Gnomon  exon    52113   52136   .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1     Gnomon  exon    70113   70962   .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1     Gnomon  exon    105987  106087  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1     Gnomon  exon    106551  106734  .       +       .       gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \n\n\n\n\n\n\nAvoid line-wrapping with less -S\n\n\n\nLines in a file may contain too many characters to fit on your screen, as will be the case for this GTF file. less will by default “wrap” such lines onto the next line on your screen, but this is often confusing for files like FASTQ and tabular files like GTF. Therefore, we turned off line-wrapping above by using the -S option to less.\n\n\n\n\n Your Turn: The GTF file is sorted: all entries from the first line of the table, until you again see “gene” in the third column, belong to the first gene. Can you make sense of all these entries for this gene, given what you know of gene structures? How many transcripts does this gene have? (Click to see some pointers)\n\n\nThe first gene (“LOC120427725”) has 3 transcripts.\nEach transcript has 6-7 exons, 5 CDSs, and a start and stop codon.\n\nBelow, I’ve printed all lines belonging to the first gene:\nNC_068937.1 Gnomon  gene    2046    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"\"; db_xref \"GeneID:120427725\"; description \"homeotic protein deformed\"; gbkey \"Gene\"; gene \"LOC120427725\"; gene_biotype \"protein_coding\"; \nNC_068937.1 Gnomon  transcript  2046    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    2046    2531    .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    52113   52136   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  exon    109726  110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 25 Proteins\"; product \"homeotic protein deformed, transcript variant X3\"; transcript_biotype \"mRNA\"; exon_number \"7\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_052707445.1\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_052563405.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  transcript  5979    110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    5979    6083    .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    52113   52136   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  exon    109726  110808  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X2\"; transcript_biotype \"mRNA\"; exon_number \"7\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592629.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448563.1\"; exon_number \"7\"; \nNC_068937.1 Gnomon  transcript  60854   110807  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"mRNA\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; \nNC_068937.1 Gnomon  exon    60854   61525   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"1\"; \nNC_068937.1 Gnomon  exon    70113   70962   .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"2\"; \nNC_068937.1 Gnomon  exon    105987  106087  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"3\"; \nNC_068937.1 Gnomon  exon    106551  106734  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"4\"; \nNC_068937.1 Gnomon  exon    109296  109660  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"5\"; \nNC_068937.1 Gnomon  exon    109726  110807  .   +   .   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gene \"LOC120427725\"; model_evidence \"Supporting evidence includes similarity to: 24 Proteins\"; product \"homeotic protein deformed, transcript variant X1\"; transcript_biotype \"mRNA\"; exon_number \"6\"; \nNC_068937.1 Gnomon  CDS 70143   70962   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"2\"; \nNC_068937.1 Gnomon  CDS 105987  106087  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"3\"; \nNC_068937.1 Gnomon  CDS 106551  106734  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"4\"; \nNC_068937.1 Gnomon  CDS 109296  109660  .   +   2   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"5\"; \nNC_068937.1 Gnomon  CDS 109726  110025  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"6\"; \nNC_068937.1 Gnomon  start_codon 70143   70145   .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"2\"; \nNC_068937.1 Gnomon  stop_codon  110026  110028  .   +   0   gene_id \"LOC120427725\"; transcript_id \"XM_039592628.2\"; db_xref \"GeneID:120427725\"; gbkey \"CDS\"; gene \"LOC120427725\"; product \"homeotic protein deformed\"; protein_id \"XP_039448562.1\"; exon_number \"6\";"
  },
  {
    "objectID": "rnaseq/01_data.html#fastq-files",
    "href": "rnaseq/01_data.html#fastq-files",
    "title": "RNA-seq data files",
    "section": "3 FASTQ files",
    "text": "3 FASTQ files\n\n3.1 The FASTQ format\nFASTQ is the standard HTS read data file format. Like the other genomic data files we’ve seen so far, these are plain text files. Each read forms one FASTQ entry and is represented by four lines:\n\nA header that starts with @ and e.g. uniquely identifies the read\nThe sequence itself\nA + (plus sign — yes, that’s all!)\nOne-character quality scores for each base in the sequence\n\n\n\n\n\nOne entry (read) in a FASTQ file covers 4 lines. The header line is annotated, with some of the more useful components highlighted in red. For viewing purposes, this read (at only 56 bp) is shorter than what is typical.\n\n\n\n\n\n\n\n\n\nSide note: FASTQ quality scores (Click to expand)\n\n\n\n\n\nThe quality scores we saw in the read above represent an estimate of the error probability of the base call.\nSpecifically, they correspond to a numeric “Phred” quality score (Q), which is a function of the estimated probability that a base call is erroneous (P):\n\nQ = -10 * log10(P)\n\nFor some specific probabilities and their rough qualitative interpretation for Illumina data:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent\n\n\n\nThis numeric quality score is represented in FASTQ files not by the number itself, but by a corresponding “ASCII character”. This allows for a single-character representation of each possible score — as a consequence, each quality score character can conveniently correspond to (& line up with) a base character in the read.\n\n\n\nPhred quality score\nError probability\nASCII character\n\n\n\n\n10\n1 in 10\n+\n\n\n20\n1 in 100\n5\n\n\n30\n1 in 1,000\n?\n\n\n40\n1 in 10,000\nI\n\n\n\nIn practice, you almost never have to manually check the quality scores of bases in FASTQ files, but if you do, a rule of thumb is that letter characters are good (Phred of 32 and up). For your reference, here is a complete lookup table (look at the top table (BASE=33)).\n\n\n\n\n\n\n3.2 Listing your FASTQ files\nFirst, let’s take another look at your list of FASTQ files:\nls -lh data/fastq\n-rw-r--r-- 1 jelmer PAS0471 21M Jan 21 13:36 ERR10802863_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 22M Jan 21 13:36 ERR10802863_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 21M Jan 21 13:36 ERR10802864_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 22M Jan 21 13:36 ERR10802864_R2.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 22M Jan 21 13:36 ERR10802865_R1.fastq.gz\n-rw-r--r-- 1 jelmer PAS0471 22M Jan 21 13:36 ERR10802865_R2.fastq.gz\n[...truncated...]\nIn the file listing above:\n\nFirst, take note of the file sizes. They are “only” about 22 Mb in size, and all have a very similar size. This is because I “subsampled” the FASTQ files to only have 500,000 reads per file. The original files were on average over 1 Gb in size with about 30 million reads.\nSecond, if you look closely at the file names, it looks like we have two FASTQ files per sample: one with _R1 at the end of the file name, and one with _R2.\n\n\n\nYour Turn: What might each of the two files per sample represent/contain? (Click for the solution)\n\nThese contain the forward reads (_R1.fastq.gz) vs. the reverse reads (_R2.fastq.gz).\n\n\n\nYour Turn: Do you have any idea why the file extension ends in .gz? (Click for the solution)\n\nThis means it is gzip-compressed. This saves a lot of space: compressed files can be up to 10 times smaller than uncompressed files. Most bioinformatics tools, including FastQC which we’ll run in a bit, can work with gzipped files directly, so there is no need to unzip them.\n\n\n\n\n3.3 Viewing your FASTQ files\nDespite the gzip-compression, we can simply use the less command as before to view the FASTQ files (!):\nless data/fastq/ERR10802863_R1.fastq.gz\n@ERR10802863.8435456 8435456 length=74\nCAACGAATACATCATGTTTGCGAAACTACTCCTCCTCGCCTTGGTGGGGATCAGTACTGCGTACCAGTATGAGT\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n@ERR10802863.27637245 27637245 length=74\nGCCACACTTTTGAAGAACAGCGTCATTGTTCTTAATTTTGTCGGCAACGCCTGCACGAGCCTTCCACGTAAGTT\n+\nAAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE&lt;EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n\n\n\n\n\n\nDifferent header lines\n\n\n\nThe header lines (starting with @) are quite different from the earlier example, because these files were downloaded from SRA. When you get files directly from a (Illumina) sequencer, they will have headers much like the earlier example.\n\n\nIn practice, we don’t often have to closely look at the contents of our FASTQ files ourselves. There are simply too many reads to make sense of!\nInstead, we’ll have specialized tools like FastQC summarize them for us: e.g. how many sequences there are, what the quality scores look like, and if there are adapter sequences. We’ll run FastQC below.\n\n\nYour Turn: All of that said, in this case, you should be able to spot some very different-looking reads soon when looking at the file with less. What are they? (Click for the solution)\n\nThe 5th read looks as follows — and if you scroll down you should quickly see several more reads like this:\n@ERR10802863.11918285 11918285 length=35\nNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+\n###################################\nThese only consists of N bases (and they are also shorter than the other reads), and are therefore completely useless!\nFYI: It not common to run into these kinds of failed reads as easily!"
  },
  {
    "objectID": "rnaseq/01_data.html#quality-control-with-fastqc",
    "href": "rnaseq/01_data.html#quality-control-with-fastqc",
    "title": "RNA-seq data files",
    "section": "4 Quality control with FastQC",
    "text": "4 Quality control with FastQC\nA good example of a tool with a command-line interface is FastQC, for quality control of FASTQ files. FastQC is ubiquitous: nearly all HTS data comes in FASTQ files, and the first step is always to check the read quality.\n\n4.1 Running FastQC\nTo run FastQC, use the command fastqc.\nIf you want to analyze one of your FASTQ files with default FastQC settings, a complete FastQC command to do so would simply be fastqc followed by the name of the file4:\n# (Don't run this)\nfastqc data/fastq/ERR10802863_R1.fastq.gz\nHowever, an annoying default behavior by FastQC is that it writes its output files in the dir where the input files are — in general, it’s not great practice to directly mix your primary data and results like that!\nTo figure out how we can change that behavior, first consider that many commands and bioinformatics tools alike have an option -h and/or --help to print usage information to the screen.\n\n\nYour Turn: Try to print FastQC’s help info, and figure out which option you can use to specify an output directory of your choice. (Click for the solution)\n\nfastqc -h and fastqc --help will both work to show the help info.\nYou’ll get quite a bit of output printed to screen, including the snippet about output directories that is reproduced below:\nfastqc -h\n  -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\nSo, you can use -o or equivalently, --outdir to specify an output dir.\n\nNow, let’s try to run FastQC and tell it to use the output dir results/fastqc:\nfastqc --outdir results/fastqc data/fastq/ERR10802863_R1.fastq.gz\nbash: fastqc: command not found...\nHowever, there is a wrinkle, as you can see above. It turns out that FastQC is installed at OSC5, but we have to “load it” before we can use it. Without having time to go into further details about software usage at OSC, please accept that we can load FastQC as follows:\nmodule load fastqc\nNow, let’s try again:\nfastqc --outdir results/fastqc data/fastq/ERR10802863_R1.fastq.gz\nSpecified output directory 'results/fastqc' does not exist\n\n\n\n Your Turn: Now what is going on this time? (Or had you perhaps seen this coming given the help text we saw earlier?) Can you try to fix the problem? (Click here for hints)\n\nYou’ll need to create a new directory, which you can do either by using the buttons in the VS Code side bar, or with the mkdir command — here, try it as mkdir -p followed by the name (path) of the directory you want to create.\n\n\n\nExercise solution (Click to expand)\n\n\nThe problem, as the error fairly clearly indicates, is that the output directory that we specified with --outdir does not currently exist. We might have expected FastQC to be smart/flexible enough to create this dir for us (many bioinformatics tools are), but alas. On the other hand, if we had read the help text clearly, it did warn us about this.\nWith the mkdir command, to create “two levels” of dirs at once, like we need to here (both results and then fastqc within there), we need its -p option:\n\nmkdir -p results/fastqc\n\nAnd for our final try before we give up and throw our laptop out the window (make sure to run the code in the exercise solution before you retry!):\nfastqc --outdir results/fastqc data/fastq/ERR10802863_R1.fastq.gz\nStarted analysis of ERR10802863_R1.fastq.gz\nApprox 5% complete for ERR10802863_R1.fastq.gz\nApprox 10% complete for ERR10802863_R1.fastq.gz\nApprox 15% complete for ERR10802863_R1.fastq.gz\n[...truncated...]\nAnalysis complete for ERR10802863_R1.fastq.gz\nSuccess!! (That should have taken just a few seconds with our subsampled FASTQ files.)\n\n\n\n4.2 FastQC output files\nLet’s take a look at the files in the output dir we specified:\nls -lh results/fastqc\n-rw-r--r-- 1 jelmer PAS0471 241K Jan 25 15:50 ERR10802863_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 256K Jan 25 15:50 ERR10802863_R1_fastqc.zip\n\nThere is a .zip file, which contains tables with FastQC’s data summaries\nThere is an .html (HTML) file, which contains plots — this is what we’ll look at next\n\n\n\n Your Turn: Run FastQC for the corresponding R2 file. Would you use the same output dir? (Click for the solution)\n\nYes, it makes sense to use the same output dir, since as you could see above, the output file names have the input file identifiers in them. As such, we don’t need to worry about overwriting files, and it will be easier to have all the results in a single dir.\nTo run FastQC for the R2 (=reverse-read) file:\nfastqc --outdir results/fastqc ERR10802863_R2.fastq.gz\nStarted analysis of ERR10802863_R2.fastq.gz\nApprox 5% complete for ERR10802863_R2.fastq.gz\nApprox 10% complete for ERR10802863_R2.fastq.gz\nApprox 15% complete for ERR10802863_R2.fastq.gz\n[...truncated...]\nAnalysis complete for ERR10802863_2.fastq.gz\nls -lh results/fastqc\n-rw-r--r-- 1 jelmer PAS0471 241K Jan 21 21:50 ERR10802863_R1_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 256K Jan 21 21:50 ERR10802863_R1_fastqc.zip\n-rw-r--r-- 1 jelmer PAS0471 234K Jan 21 21:53 ERR10802863_R2_fastqc.html\n-rw-r--r-- 1 jelmer PAS0471 244K Jan 21 21:53 ERR10802863_R2_fastqc.zip\nNow, we have four files: two for each of our preceding successful FastQC runs.\n\n\n\n\n4.3 Interpreting FastQC’s output\nFirst, we’ll unfortunately have to download FastQC’s output HTML files6:\n\nFind the FastQC HTML files in the file explorer in the VS Code side bar.\nRight-click on one of them, click Download... and follow the prompt to download the file somewhere to your computer (doesn’t matter where, just make sure you see where it goes).\nRepeat this for the second file\nThen, open your computer’s file browser, find the downloaded files, and double-click on one. It should be opened in your default web browser.\n\nWe’ll now go through a couple of the FastQC plots/modules, with first some example plots with good/bad results for reference.\n\n\nOverview of module results\nFastQC has “pass” (checkmark in green), “warning” (exclamation mark in orange), and “fail” (cross in red) assessments for each module, as you can see below.\nThese are handy and typically at least somewhat meaningful, but it is important to realize that a “warning” or a “fail” is not necessarily the bad news that it may appear to be, because, e.g.:\n\nSome of these modules could perhaps be called overly strict.\nSome warnings and fails are easily remedied or simply not a very big deal.\nFastQC assumes that your data is derived from whole-genome shotgun sequencing — some other types of data like RNA-seq data will always trigger a couple of warnings and files based on expected differences.\n\n\n\n\n\n\n\nBasic statistics\nThis shows, for example, the number of sequences (reads) and the read length range for your file:\n\n\n\n\n\n\nPer base quality sequence quality\nThis figure visualize the mean per-base quality score (y-axis) along the length of the reads (x-axis). Note that:\n\nA decrease in sequence quality along the reads is normal.\nR2 (reverse) reads are usually worse than R1 (forward) reads.\n\n\n\nGood / acceptable:\n\n\n\n\nBad:\n\n\n\n\n\nTo interpret the quality scores along the y-axis, note the color scaling in the graphs (green is good, etc.), and see this table for details:\n\n\n\nPhred quality score\nError probability\nRough interpretation\n\n\n\n\n10\n1 in 10\nterrible\n\n\n20\n1 in 100\nbad\n\n\n30\n1 in 1,000\ngood\n\n\n40\n1 in 10,000\nexcellent\n\n\n\n\n\n\nPer sequence quality scores\nThis shows the same quality scores we saw above, but now simply as a density plot of per-read averages, with the quality score now along the x-axis, and the number of reads with that quality score along the y-axis:\n\n\nGood:\n\n\n\n\nBad:\n\n\n\n\n\n\n\n\nSequence length distribution\nWill throw a warning as soon as not all sequences are of the same length (like below), but this is quite normal.\n\n\n\n\n\n\nAdapter content\nChecks for known adapter sequences. When some of the insert sizes are shorter than the read length, adapters can end up in the sequence – these should be removed!\n\n\nGood:\n\n\n\n\nBad:\n\n\n\n\n\n\n\n\n\n4.4 Interpreting our FastQC output\n\n\nYour Turn: Open the HTML file for the R1 FASTQ file and go through the modules we discussed above. Can you make sense of it? Does the data look good to you, overall?\n\n\n\n\nYour Turn: Now open the HTML file for the R2 FASTQ file and take a look just at the quality scores. Does it look any worse than the R1?"
  },
  {
    "objectID": "rnaseq/01_data.html#in-closing",
    "href": "rnaseq/01_data.html#in-closing",
    "title": "RNA-seq data files",
    "section": "5 In closing",
    "text": "5 In closing\nIn today’s lab, you were introduced to:\n\nWorking at the Ohio Supercomputer Center\nUsing the VS Code text editor\nUsing the Unix shell\nReference genome FASTA & GTF files & where to find these\nHTS read FASTQ files and how to quality-control these\nHow to run a command-line bioinformatics tool\n\nTaking a step back, I’ve shown you the main pieces of the computational infrastructure for what we may call “command-line genomics”: genomics analysis using command-line tools. And we’ve seen a basic example of loading and running a command-line tool at OSC.\n\n\n\n\n\n\nSide note: scaling the analysis & next steps for real projects (Click to expand)\n\n\n\n\n\nThe missing pieces for a typical, fuller example of how such tools are run in the context of an actual genomics project are (if we stay with FastQC):\n\nPutting the command to run FastQC in a “shell script”.\nSubmitting the script to the SLURM scheduler queue as a “batch job”.\nTo make speed things, using the OSC’s capabilities, we can submit multiple jobs in parallel using a loop.\n\nIf it seems that speed and computing power may not be an issue, given how fast FastQC ran, keep in mind that:\n\nWe here worked with subsampled (much smaller than usual) FASTQ files\nWe only ran FastQC for one of our 23 samples, and your experiment may have 50+ samples\nWe need to run a bunch more tools, and some of those take much longer to run or need lots of RAM memory.\n\nAll that said, those missing pieces mentioned above are outside the scope of this short introduction — but if you managed today, it should not be hard to learn those skills either."
  },
  {
    "objectID": "rnaseq/01_data.html#appendix",
    "href": "rnaseq/01_data.html#appendix",
    "title": "RNA-seq data files",
    "section": "6 Appendix",
    "text": "6 Appendix\n\nBonus exercises\n\n\n Your Turn (Bonus): Explore the assembly FASTA file with grep (Click to see the instructions)\n\ngrep is an incredibly useful Unix command with which you can search files for specific text. By default, it will print lines that match your search in their entirety.\nFor example, we could search the genome for the short sequence ACCGATACGACG:\ngrep \"ACCGATACGACG\" data/ref/GCF_016801865.2.fna\naaaatcgaaaaacgcgTTTACCTTACATTGACAAAGTTGACCGATACGACGGCTCGATGTGCCAAACCGGTCACAAAGTC\nAATATTGACATTTCTTTTGCATTCTTCAGGTTCAGTGACCACAAACGGGACCGATACGACGGCTACCATCGGAATGCACC\nTCAAAATGTGTCAATTAACGTAACTAGATTTTTACGATCATAATAAGTAGATACCGATACGACGGGGCGGCATTTATGCT\nTAAGTAGATACCGATACGACGGGGCGGCATTCATGCTGCTACAGGGCTCAGCGGACCGACAAGCGACTGTGAAACGCAGC\n(Matches should be highlighted in red, but that doesn’t show here, unfortunately)\nOr count the number of times the much shorter sequence GGACC occurs:\ngrep -c \"GGACC\" data/ref/GCF_016801865.2.fna\n120492\nTry to adapt the above example to:\n\nPrint all entry headers in the assembly FASTA file\nCount the number of entry header (= the number of entries) in the assembly FASTA file\n\nDoes the number of entries make sense given how many chromosomes and scaffolds the assembly consists of according to NCBI?\n\n\n\n Bonus exercise solutions (Click to expand)\n\nTo print all FASTA entry headers, simply search for &gt; with grep (since &gt; should not occur in the sequences themselves, which can only be bases or N). Make sure to uses quotes (\"&gt;\")!\ngrep \"&gt;\" data/ref/GCF_016801865.2.fna\n&gt;NC_068937.1 Culex pipiens pallens isolate TS chromosome 1, TS_CPP_V2, whole genome shotgun sequence\n&gt;NC_068938.1 Culex pipiens pallens isolate TS chromosome 2, TS_CPP_V2, whole genome shotgun sequence\n&gt;NC_068939.1 Culex pipiens pallens isolate TS chromosome 3, TS_CPP_V2, whole genome shotgun sequence\n&gt;NW_026292818.1 Culex pipiens pallens isolate TS unplaced genomic scaffold, TS_CPP_V2 Cpp_Un0001, whole genome shotgun sequence\n&gt;NW_026292819.1 Culex pipiens pallens isolate TS unplaced genomic scaffold, TS_CPP_V2 Cpp_Un0002, whole genome shotgun sequence\n&gt;NW_026292820.1 Culex pipiens pallens isolate TS unplaced genomic scaffold, TS_CPP_V2 Cpp_Un0003, whole genome shotgun sequence\n&gt;NW_026292821.1 Culex pipiens pallens isolate TS unplaced genomic scaffold, TS_CPP_V2 Cpp_Un0004, whole genome shotgun sequence\n[...truncated...]\nWe can count the number of header lines by modifying our above command only with the addition of the -c option:\ngrep -c \"&gt;\" data/ref/GCF_016801865.2.fna\n290\n\n\n\nAttribution\n\nSome of the FastQC example plots were taken from here."
  },
  {
    "objectID": "rnaseq/01_data.html#footnotes",
    "href": "rnaseq/01_data.html#footnotes",
    "title": "RNA-seq data files",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut see the box below for more info/context↩︎\nNote that because individual sequence entries are commonly spread across multiple lines, FASTA entries do not necessarily cover 2 lines (cf. FASTQ).↩︎\nA GFF file would contain the same information but in a slightly different format. For programs used in RNA-seq analysis, GTF files tend to be the preferred format.↩︎\nNote that this is very similar to running, say, the ls command!↩︎\nFor a full list of installed software at OSC: https://www.osc.edu/resources/available_software/software_list↩︎\nThe installed version of VS Code does not allow us to view HTML files↩︎"
  },
  {
    "objectID": "rnaseq/05_trimgalore.html#overview-setting-up",
    "href": "rnaseq/05_trimgalore.html#overview-setting-up",
    "title": "Trimming with TrimGalore",
    "section": "Overview & setting up",
    "text": "Overview & setting up\nIn this tutorial, we will run TrimGalore to filter our FASTQ files, removing:\n\nAny adapter sequences that are present in the reads\nPoor-quality bases at the start and end of the reads\nReads that have become very short after the prior two steps\n\nFor reference-based RNAseq, this step is sometimes considered optional, since current tools that align reads to the genome should generally be able to deal with poor-quality bases and adapter sequences.\nSeveral largely equivalent tools exist for this kind of FASTQ preprocessing — Trimmomatic and fastp are two other commonly used ones. TrimGalore itself is in fact “just” a wrapper around another tool called CutAdapt, but it is much simpler to use. Two advantages of of TrimGalore are that it will auto-detect the adapters that are present in your reads (e.g., different library prep protocols use different adapters), and that it can automatically run FastQC on the processed sequences."
  },
  {
    "objectID": "rnaseq/05_trimgalore.html#using-trimgalore-at-osc",
    "href": "rnaseq/05_trimgalore.html#using-trimgalore-at-osc",
    "title": "Trimming with TrimGalore",
    "section": "1 Using TrimGalore at OSC",
    "text": "1 Using TrimGalore at OSC\nTrimGalore isn’t installed at OSC, so we’ll use my Conda environment for TrimGalore like we did with MultiQC:\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\nLet’s see if we can now run it — note that the command is trim_galore with an underscore:\ntrim_galore --version\n\n            Quality-/Adapter-/RRBS-/Speciality-Trimming\n                    [powered by Cutadapt]\n                        version 0.6.10\n\n                    Last update: 02 02 2023\n\n\n\n\n\n\nHow you can create your own TrimGalore Conda environment (click to expand)\n\n\n\n\n\n# Make sure the load the latest miniconda version when doing installations\nmodule load miniconda3/23.3.1-py310\n\n# Create a new environment called 'trimgalore' and install the program into it\n# (Yes, the Conda package is named 'trim-galore' with a dash!)\nconda create -y -n trimgalore -c bioconda trim-galore"
  },
  {
    "objectID": "rnaseq/05_trimgalore.html#trimgalore-syntax",
    "href": "rnaseq/05_trimgalore.html#trimgalore-syntax",
    "title": "Trimming with TrimGalore",
    "section": "2 TrimGalore syntax",
    "text": "2 TrimGalore syntax\nLet’s run TrimGalore with the --help option to get some information about how we can run it:\ntrim_galore --help\n\n# Note: Below I am only showing (truncated) output for the key options!\n USAGE:\ntrim_galore [options] &lt;filename(s)&gt;\n\n--paired                This option performs length trimming of quality/adapter/RRBS trimmed reads for\n                        paired-end files.\n\n-o/--output_dir &lt;DIR&gt;   If specified all output will be written to this directory instead of the current\n                        directory. If the directory doesn't exist it will be created for you.\n\n-j/--cores INT          Number of cores to be used for trimming [default: 1].\n\n--fastqc                Run FastQC in the default mode on the FastQ file once trimming is complete.\n\n--fastqc_args \"&lt;ARGS&gt;\"  Passes extra arguments to FastQC.\n\n-a/--adapter &lt;STRING&gt;   Adapter sequence to be trimmed. If not specified explicitly, Trim Galore will\n                        try to auto-detect whether the Illumina universal, Nextera transposase or Illumina\n                        small RNA adapter sequence was used.\n\n-q/--quality &lt;INT&gt;      Trim low-quality ends from reads in addition to adapter removal.\n\n--length &lt;INT&gt;          Discard reads that became shorter than length INT because of either\n                        quality or adapter trimming. A value of '0' effectively disables\n                        this behaviour. Default: 20 bp.\nThe line below “USAGE:” tells us that the FASTQ file names should be specified as positional arguments at the end of the command.\nWe will run TrimGalore for one sample at a time, but this means having to specify two FASTQ file names: one with the forward reads, and one with the reverse reads. When we have paired-end reads, much of the trimming happens separately for the R1 (forward) and R2 (reverse) files, but at the end of the run, TrimGalore will make sure that every R1 read still has its R2 counterpart, and vice versa. Any “orphaned” reads will by default be removed, because R1 and R2 files for the same samples always need to contain all the same reads. (TrimGalore does have an option to retain these orphaned reads into separate files.)\nNone of the options are required, as the square brackets around [options] indicate, but any option we may want to use should be placed after the trim_galore command and before the FASTQ file name(s). With paired-end reads, we’ll have to use the --paired option — otherwise, TrimGalore will only process the R1 and R2 separately and omit the final step where it removes orphaned reads."
  },
  {
    "objectID": "rnaseq/05_trimgalore.html#building-our-trimgalore-command",
    "href": "rnaseq/05_trimgalore.html#building-our-trimgalore-command",
    "title": "Trimming with TrimGalore",
    "section": "3 Building our TrimGalore command",
    "text": "3 Building our TrimGalore command\nGiven what we discussed above, a minimal functional example of a TrimGalore command would be\n(with fictional FASTQ files A_R1.fastq.gz and A_R2.fastq.gz):\n# (Don't run this, fictional example command)\ntrim_galore --paired A_R1.fastq.gz A_R2.fastq.gz\nAs for other TrimGalore options:\n\nWe will use TrimGalore with default settings for the following options (i.e., we will not use these options in our command):\n\nThe trimming of adapters (--adapter option, default: auto-detect the adapters used)\nThe minimum base quality threshold (--quality option, default: Phred score of 20)\nThe minimum read length threshold (--length option, default: 20 bp)\n…but it’s good to know we could easily change those if we wanted to.\n\nWe do want to specify the output directory, since it’s pretty inconvenient to have the output files placed in the current working dir, as is the TrimGalore default. We could for instance use --output_dir results/trimgalore.\nWe’ll typically also want to have TrimGalore run FastQC on the filtered FASTQ files, since it will be good to check if adapter sequences were successfully removed, and so on.\nWhen we do so, we’ll also have to tell FastQC about the output dir of its files: we can do so via --fastqc_args (and using that option will already trigger the running of FastQC, i.e., there is then no need to also use the --fastqc option).\nSo, we could use --fastqc_args \"--outdir results/trimgalore/fastqc\".\nFinally, we’ll usually want to specify the number of cores/threads/CPUs, and it should correspond to what we have available for our compute job. Since we have 1 core available in the VS Code session, we’ll use --cores 1 in the test-run, but something else in our final script.\n\nA final test command to run TrimGalore on our actual (but small, subsetted) FASTQ files in data/fastq could therefore look as follows:\n# For clarity, I am specifying each option on a separate line\ntrim_galore \\\n    --paired \\\n    --output_dir results/trimgalore \\\n    --cores 1 \\\n    --fastqc_args \"--outdir results/trimgalore/fastqc\" \\\n    data/fastq/ASPC1_A178V_R1.fastq.gz data/fastq/ASPC1_A178V_R2.fastq.gz\nLet’s try that out."
  },
  {
    "objectID": "rnaseq/05_trimgalore.html#trimgalore-output",
    "href": "rnaseq/05_trimgalore.html#trimgalore-output",
    "title": "Trimming with TrimGalore",
    "section": "4 TrimGalore output",
    "text": "4 TrimGalore output\nAfter you ran the line above, quite a lot of logging output should have been printed to screen. If we take a look at that output:\n\nThe first section of interest is “AUTO-DETECTING ADAPTER TYPE”, where we can see that TrimGalore detected the “Illumina adapter” and will use that for trimming.\nAfter that, the section “SUMMARISING RUN PARAMETERS” tells us what the final parameters (settings) are for running Cutadapt, which will do the actual trimming. We can for example see which base quality and read length parameters are being used (the defaults, in this case).\nThen, …\n\nThe log for each FASTQ file is also saved to the output dir in files named XXX. Curiously, though, logging output that is not R1/R2-specific, like that reporting on the final removal of orphaned reads, is not. As such, when we will run TrimGalore by submitting batch jobs in a minute, and the logging output that was now printed to screen will go to a Slurm log file, it is a good idea to keep these Slurm files.\nLet’s take a look at what files have been added to our output dir:\nls -lh results/trimgalore"
  },
  {
    "objectID": "rnaseq/05_trimgalore.html#a-script-to-run-trimgalore",
    "href": "rnaseq/05_trimgalore.html#a-script-to-run-trimgalore",
    "title": "Trimming with TrimGalore",
    "section": "5 A script to run TrimGalore",
    "text": "5 A script to run TrimGalore\nTo run TrimGalore efficiently at OSC, we will submit a separate compute job for each sample, and submit these all at the same time using a for loop in which we pass arguments to the script.\nThis is quite similar to how we ran FastQC. However, one added complication is that we now have to work with two FASTQ files (an R1 and R2 file for the same sample) at the same time.\n\n5.1 Dealing with two FASTQ files per run\nThere are several different ways of dealing with having to run TrimGalore not for one but for two files at the same time, but we’ll do it as follows: we’ll only pass an R1 FASTQ filename to the script, and the script will then infer the name of the R2 file (by replacing _R1 with _R2). This is generally the easiest method, because it allows us to simply loop over the R1 FASTQ files in our “runner script”.\nThat is, in our run.sh script, the code to submit the TrimGalore scripts would look as follows:\n# We're only looping over the R1 files: we include the R2 files _inside_ the script\nfor R1 in data/fastq/*_R1.fastq.gz; do\n    sbatch scripts/trimgalore.sh $R1 results/trimgalore\ndone\nHere is how we can include the R2 files inside the script:\n# Arguments passed to the script include the R1 filename,\nR1=$1                # \"$1\" will for example be data/fastq/ASPC1_A178V_R1.fastq.gz\n# ....\n\n# Infer the name of the corresponding R2 file:\nR2=${R1/_R1/_R2}     # \"$2\" will then be data/fastq/ASPC1_A178V_R2.fastq.gz\nThe funny-looking ${R1/_R1/_R2} bit does a search-and-replace in the value of the $R1 variable (this is called “parameter expansion”), and assigns the output to a new variable $R2:\n\nTake the $R1 variable, using the long notation ${R1}\nAfter the first forward slash, we enter the search pattern: _R1\nAfter the second forward slash, we enter the replacement: _R2.\n\nLet’s practice with parameter expansion:\nfilename1=myfile.txt\nfilename2=${filename1/.txt/_copy.txt}\n\necho $filename2\nmyfile_copy.txt\n\n\n5.2 Using multiple threads\nWhen we ran FastQC, we already practiced with using multiple threads/cores/cpus (remember, these are all all equivalent for our purposes here), which always involves two steps:\n\nRequesting the desired number of threads for the Slurm job, e.g. a line #SBATCH --cpus-per-node=8 at the top of your script.\nTelling the bioinformatics tool that it can use that same number of threads, e.g. with --cores 8 in the case of TrimGalore.\n\nSo, you’re always specifying your desired number of cores in (at least) two different places. As such, there is the possibility that you’re accidentally specifying a different number in the two places, e.g. when you want to change the number but only do so in one of these two places.\nLet’s introduce a trick you can use to automatically make sure that the number of threads is the same in both places, by using the Slurm “environment variable” $SLURM_CPUS_ON_NODE. When you run a Slurm job and you’ve requested a number of threads with --cpus-per-node, then this variable will always be set automatically. (In fact, there are many Slurm variables, see this section in the Slurm documentation for more information.)\nHere’s how we can use this environment variable in practice:\n#SBATCH --cpus-per-node=8\n\n# [...]\ntrim_galore \\\n    --cores \"$SLURM_CPUS_ON_NODE\" \\\n\n# [...]\n\n\n5.3 Our final TrimGalore command\nLike we’ve done in our FastQC and MultiQC scripts, we’ll also include an argument with the output directory name, as it is good practice not to hardcode this in your script.\nWith that, our final TrimGalore will be:\ntrim_galore \\\n    --paired \\\n    --output_dir \"$outdir\" \\\n    --cores \"$SLURM_CPUS_ON_NODE\" \\\n    --fastqc_args \"--outdir \"$outdir\"/fastqc\" \\\n    \"$R1\" \"$R2\"    \n\n\n5.4 Renaming the output files\nOne annoying aspect of TrimGalore is that it will give the output FASTQ files suffixes — here are the output files of our earlier test run:\nASPC1_A178V_R1_val_1.fq.gz\nASPC1_A178V_R2_val_2.fq.gz\nFirst, the extension is .fq instead of .fastq, and second, there are _val_1 / _val_2 additions to the filename. With filenames like that, we would need to modify our globbing patterns (e.g. *_R1.fastq.gz) in the next step in our workflow. This is of course possible and not even hard, per se, but it is inconvenient — and in practice, these kind of inconsistencies easily end up leading to confusion and errors.\nTherefore, I usually choose to rename TrimGalore’s output files inside the script, and give the output files the exact same name as the input files. We can do so as follows:\nfile_id=$(basename \"$R1\" _R1.fastq.gz)\n\nR1_out=\"$outdir\"/\"$file_id\"_R1_val_1.fq.gz\nR2_out=\"$outdir\"/\"$file_id\"_R2_val_2.fq.gz\n\nmv -v \"$R1_out\" \"$outdir\"/\"$file_id\"_R1.fastq.gz\nmv -v \"$R2_out\" \"$outdir\"/\"$file_id\"_R2.fastq.gz\n\n\n\n\n\n\nCommand substitution\n\n\n\nThe first line in the code block above used a construct called “command substitution”, which allows you to save the output of a command in a variable. To understand this better, please refer to this at-home reading section in module A07.\n\n\nOne thing that we do need to be wary of, however, when we give the output FASTQ files the same name as the input files, is that we don’t overwrite the output files. In general with these kind of scripts, you don’t want the output dir to be the same as the input dir, and as long as that is the case, the input files won’t be overwritten. But here, we should really make absolutely sure that the input dir isn’t the same as the output dir inside the script, and we can do so as follows:\n# Check that the output dir isn't the same as the input dir\n# This is because we will let the output files have the same name as the input files\nif [[ $(dirname \"$R1\") == \"$outdir\" ]]; then\n    echo \"# ERROR: Input dir is the same as the output dir ($outdir)\" \n    exit 1\nfi\nThere are several new things here:\n\nIf statement\ndirname command\nexit 1"
  },
  {
    "objectID": "rnaseq/05_trimgalore.html#the-final-script-to-run-trimgalore",
    "href": "rnaseq/05_trimgalore.html#the-final-script-to-run-trimgalore",
    "title": "Trimming with TrimGalore",
    "section": "6 The final script to run TrimGalore",
    "text": "6 The final script to run TrimGalore\n#!/bin/bash\n#SBATCH --account=PAS0471\n#SBATCH --time=1:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --job-name=trimgalore\n#SBATCH --output=slurm-trimgalore-%j.out\n\n# Copy the placeholder variables\nR1=$1\noutdir=$2\n\n# Load the Conda environment\nmodule load miniconda3\nsource activate /fs/ess/PAS0471/jelmer/conda/trimgalore\n\n# Use strict Bash settings\nset -euo pipefail\n\n# Infer derived variables\nR2=${R1/_R1/_R2}\n\n# Report\necho \"# Starting script trimgalore.sh\"\ndate\necho \"# Input R1 FASTQ file:      $R1\"\necho \"# Output dir:               $outdir\"\necho\n\n# Check that the output dir isn't the same as the input dir\n# This is because we will let the output files have the same name as the input files\nif [[ $(dirname \"$R1\") == \"$outdir\" ]]; then\n    echo \"# ERROR: Input dir is the same as the output dir ($outdir)\" \n    exit 1\nfi\n\n# Create the output dir\nmkdir -p \"$outdir\" \"$outdir\"/fastqc\n\n# Run TrimGalore\ntrim_galore \\\n    --paired \\\n    --output_dir \"$outdir\" \\\n    --cores \"$SLURM_CPUS_ON_NODE\" \\\n    --fastqc_args \"--outdir $outdir/fastqc\" \\\n    \"$R1\" \"$R2\"\n\n# Rename output files\necho -e \"\\n# Renaming the output files:\"\nfile_id=$(basename \"$R1\" _R1.fastq.gz)\nR1_out=\"$outdir\"/\"$file_id\"_R1_val_1.fq.gz\nR2_out=\"$outdir\"/\"$file_id\"_R2_val_2.fq.gz\nmv -v \"$R1_out\" \"$outdir\"/\"$file_id\"_R1.fastq.gz\nmv -v \"$R2_out\" \"$outdir\"/\"$file_id\"_R2.fastq.gz\n\n# Report\necho -e \"\\n# Done with script trimgalore.sh\"\ndate\necho -e \"\\n# Listing files in the output dir:\"\nls -lh \"$outdir\""
  },
  {
    "objectID": "bac/02_assembly.html#introduction",
    "href": "bac/02_assembly.html#introduction",
    "title": "Genome assembly and assembly QC",
    "section": "Introduction",
    "text": "Introduction\nWe will now create a genome assembly from the preprocessed reads with the program SPAdes.\nAfter the assembly, we will run several assembly QC and filtering steps to:\n\nGet some basic assembly summary stats with BBtools\nRun Busco to check for genome completeness\nIdentify and remove contaminant contigs with Kraken2\n\n\n Setting up\nYou should have an active VS Code session with an open terminal. In that terminal, you should be be in your dir /fs/scratch/PAS2250/cabana/$USER/bact/bact."
  },
  {
    "objectID": "bac/02_assembly.html#the-fasta-format",
    "href": "bac/02_assembly.html#the-fasta-format",
    "title": "Genome assembly and assembly QC",
    "section": "1 The FASTA format",
    "text": "1 The FASTA format\nThe genome assembly that we will create will be in the FASTA formet. FASTA files contain one or more DNA or amino acid sequences, with no limits on the number of sequences or the sequence lengths.\nThe following example FASTA file contains two entries:\n&gt;unique_sequence_ID Optional description\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n&gt;unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\nEach entry consists of a header line and the sequence itself. Header lines start with a &gt; (greater-than sign) and are otherwise “free form”, though the idea is that they provide an identifier for the sequence that follows.1\n\n\n\n\n\n\nFASTA file name extensions are variable: .fa, .fasta, .fna, .faa (Click to expand)\n\n\n\n\n\n\n“Generic” extensions are .fasta and .fa (e.g: my_assembly.fasta)\nAlso used are extensions that explicitly indicate whether sequences are nucleotides (.fna) or amino acids (.faa)"
  },
  {
    "objectID": "bac/02_assembly.html#batch-jobs-with-sbatch",
    "href": "bac/02_assembly.html#batch-jobs-with-sbatch",
    "title": "Genome assembly and assembly QC",
    "section": "2 “Batch jobs” with sbatch",
    "text": "2 “Batch jobs” with sbatch\nSo far, we having been running programs by directly typing/pasting the commands in the terminal. Because SPAdes needs more computing resources, we will run it differently.\nWe will submit it as a so-called “batch” (non-interactive) job using the sbatch command. This job will then be run on a compute node that we ourselves never move to (!).\nTo first see a simple example, say that we just wanted to run echo Hello there $USER as a batch job, where we’ll use these options:\n\n-A for the OSC Project we want to use\n-t for the time in minutes that we need\n-c for the number of cores that we need\n-o for the name of the log file (where “Hello there ” will be printed)\nThe command that we want to run in the batch job is wrapped in wrap=\"&lt;command&gt;\"\n\nsbatch -A PAS2250 -t 1 -c 1 -o slurm-hello.out \\\n  --wrap=\"echo Hello there $USER\"\nSubmitted batch job 25928455\n# [You will get a different number, each job has a unique ID]\nNow, the output of our command (ls -lh) is not printed to the screen, but will end up in a file in our working directory, whose name starts with slurm and contains the job ID number:\nls\ndata  README.md  results  slurm-hello.out\n\n\n\n\n\n\nThe slurm file will only show up once the job has started running, which can take up to a minute or so.\n\n\n\n\n\n\nLet’s take a look at that “Slurm log file”:\ncat slurm-hello.out\nHello there jelmer"
  },
  {
    "objectID": "bac/02_assembly.html#assembly-with-spades",
    "href": "bac/02_assembly.html#assembly-with-spades",
    "title": "Genome assembly and assembly QC",
    "section": "3 Assembly with SPAdes",
    "text": "3 Assembly with SPAdes\nSPAdes is a well-performing and very flexible assembler that can be used to do many kinds of assemblies, including metagenomes and transcriptomes. It has a special “mode” for bacterial isolate assembly, which can be activated with the --isolate flag.\n\n\n3.1 Our SPAdes command\nWe will run SPAdes with the following options:\n\n-1 and -2 for the R1 and R2 FASTQ files\n-o for the output dir, which should be sample-specific, and should not yet exist\n--isolate to activate the bacterial isolate mode\n-k 21,33,55,77,99,127 to assemble with a variety of kmer sizes\n--threads 20 and --memory 80 to use 20 threads and 80 GB of memory\n\n# (Don't run this yet)\nspades.py \\\n  -1 results/trimgalore/SM04_R1_val_1.fq.gz \\\n  -2 results/trimgalore/SM04_R2_val_2.fq.gz \\\n  -o results/spades/SM04 \\\n  -k 21,33,55,77,99,127 \\\n  --isolate \\\n  --threads 20 \\\n  --memory 80\n\n\n\n3.2 Running SPAdes\nFirst, we’ll have to switch back to the cabana Conda environment, since we activated a different Conda environment for TrimGalore earlier.\nsource activate /fs/ess/PAS0471/jelmer/conda/cabana\n\n\nSubmitting the SPAdes sbatch job\nNow we’re ready to submit our SPAdes job, with 30 minutes (-t 30) and 20 cores (-c 20):\nsbatch -A PAS2250 -t 30 -c 20 -o slurm-spades.out --wrap=\"\n    spades.py \\\n      -1 results/trimgalore/SM04_R1_val_1.fq.gz \\\n      -2 results/trimgalore/SM04_R2_val_2.fq.gz \\\n      -o results/spades/SM04 \\\n      -k 21,33,55,77,99,127 \\\n      --isolate \\\n      --threads 20 \\\n      --memory 80\n\"\n\n\n Exercise: Monitor the SPAdes run\n\nUse less to check the slurm-spades.out file, which will have the SPAdes “log”.\nIn less, press G (capital G!) to look at the end of the file.\n\nWhen you first check the slurm-spades.out file, SPAdes should still be running. You know it will be done when you see the following line at the end:\nThank you for using SPAdes!\n\nTo monitor the progress of the SPAdes run, you can use tail -f slurm-spades.out, which will “follow” the file and add any new text in real-time! To exit this, press Ctrl+c.\n\n\n\n\n\n\n\nSPAdes may take 5-10 minutes to complete\n\n\n\n\n\n\n\n\n\n\n3.3 SPAdes output files\nLet’s check the files in the output dir:\nls -lh results/spades/SM04\n\n\nClick to show the output\n\ntotal 47M\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 assembly_graph_after_simplification.gfa\n-rw-r--r-- 1 jelmer PAS0471  12M Feb  4 21:49 assembly_graph.fastg\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 assembly_graph_with_scaffolds.gfa\n-rw-r--r-- 1 jelmer PAS0471 5.9M Feb  4 21:49 before_rr.fasta\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 contigs.fasta\n-rw-r--r-- 1 jelmer PAS0471 9.8K Feb  4 21:49 contigs.paths\n-rw-r--r-- 1 jelmer PAS0471   79 Feb  4 21:42 dataset.info\n-rw-r--r-- 1 jelmer PAS0471  278 Feb  4 21:42 input_dataset.yaml\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:49 K127\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:43 K21\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:44 K33\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:45 K55\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:47 K77\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:48 K99\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 logs\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 misc\n-rw-r--r-- 1 jelmer PAS0471 1.5K Feb  4 21:42 params.txt\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 pipeline_state\n-rw-r--r-- 1 jelmer PAS0471 3.4K Feb  4 21:42 run_spades.sh\n-rw-r--r-- 1 jelmer PAS0471 4.9K Feb  4 21:42 run_spades.yaml\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 scaffolds.fasta\n-rw-r--r-- 1 jelmer PAS0471 8.9K Feb  4 21:49 scaffolds.paths\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 SM04\n-rw-r--r-- 1 jelmer PAS0471 194K Feb  4 21:49 spades.log\n\nThere are quite some files, as well as subdirs for different k-mer sizes, but we’re really only interested in the assembly FASTA file, which is contigs.fasta. Let’s take a look at that file:\nless results/spades/SM04/contigs.fasta\n&gt;NODE_1_length_1267796_cov_33.239498\nACCTTGAGTTCCCTAAAGGGCCGTCGAAGACTACGACGTTGATAGGTTGGGTGTGTAAGC\nGCTGTGAGGCGTTGAGCTAACCAATACTAATTGCCCGTGAGGCTTGACCATATAACACCC\nAAGCAATTTGCGTTGAATGAGCAGATTGCGGTGACTGTGAAGATGACACGAACCGAAAGT\nTTGCGTCACGAACGACACCTGAACCAGCTTGCTATCACATACCCGATTTGCTGAAGCGCG\nCCGCAAGGCACGATTCGGTACCCGAATTTCTTGACGACCATAGAGCATTGGAACCACCTG\nATCCCATCCCGAACTCAGTAGTGAAACGATGTATCGCCGATGGTAGTGTGGGGTTTCCCC\nATGTGAGAGTAGGTCATCGTCAAGATTAAATTCCAGAAACCCTCATCGCTTACGCGTTGA\nGGGTTTTTGTTTGTCTGGGGTTCCAGAAACCTCTGCATTCTCTATCTGGCTCATCTCATT\nGCAATGCAGCCGCATTGGCGCCAGAGACCCCCAAGGTTTAGTGAAACGCCCCCATCCCTG\nIn this file, each contig is one FASTA entry. The contig headers have some metadata, such as its length in base pairs, and its depth of coverage (cov_; i.e. how many reads, on average, cover each base).\nWe can see a few more headers by using the grep command, which will print lines matching a search pattern (in our case, the &gt; from the header), as follows:\n# (Do NOT omit the quotes around the \"&gt;\"!)\ngrep \"&gt;\" results/spades/SM04/contigs.fasta | head\n&gt;NODE_1_length_1267796_cov_33.239498\n&gt;NODE_2_length_902255_cov_32.000245\n&gt;NODE_3_length_697265_cov_34.901625\n&gt;NODE_4_length_534491_cov_32.088021\n&gt;NODE_5_length_350317_cov_33.463137\n&gt;NODE_6_length_339735_cov_31.812540\n&gt;NODE_7_length_291220_cov_35.951730\n&gt;NODE_8_length_274792_cov_32.455031\n&gt;NODE_9_length_167931_cov_33.795917\n&gt;NODE_10_length_164349_cov_34.581646\n\n\n\n\n\n\nIf you don’t pipe (|) the grep output to head, you will see all headers\n\n\n\n\n\n\nIf we use grep’s -c option, it will count the number of matching lines, which will give us a count of the number of contigs:\ngrep -c \"&gt;\" results/spades/SM04/contigs.fasta\n86"
  },
  {
    "objectID": "bac/02_assembly.html#basic-assembly-stats",
    "href": "bac/02_assembly.html#basic-assembly-stats",
    "title": "Genome assembly and assembly QC",
    "section": "4 Basic assembly stats",
    "text": "4 Basic assembly stats\nWe can use the tools stats.sh from the BBTools suite of genomics tools to get some (more) basic statistics for our genome assembly:\nstats.sh results/spades/SM04/contigs.fasta \nA       C       G       T       N       IUPAC   Other   GC      GC_stdev\n0.2048  0.2969  0.2945  0.2038  0.0000  0.0000  0.0000  0.5914  0.0875\n\nMain genome scaffold total:             86\nMain genome contig total:               86\nMain genome scaffold sequence total:    5.968 MB\nMain genome contig sequence total:      5.968 MB        0.000% gap\nMain genome scaffold N/L50:             4/534.491 KB\nMain genome contig N/L50:               4/534.491 KB\nMain genome scaffold N/L90:             14/97.943 KB\nMain genome contig N/L90:               14/97.943 KB\nMax scaffold length:                    1.268 MB\nMax contig length:                      1.268 MB\nNumber of scaffolds &gt; 50 KB:            18\n% main genome in scaffolds &gt; 50 KB:     96.64%"
  },
  {
    "objectID": "bac/02_assembly.html#assembly-completeness-check-with-busco",
    "href": "bac/02_assembly.html#assembly-completeness-check-with-busco",
    "title": "Genome assembly and assembly QC",
    "section": "5 Assembly completeness check with Busco",
    "text": "5 Assembly completeness check with Busco\nWe will use the program Busco (Manni et al. 2021, documentation) to check how complete our genome assembly is. Busco checks for the presence of genes that are expected to be universally present in a single copy in a specific taxonomic lineage.\n\n\n\n\n\n\nOther tool options\n\n\n\nAnother commonly used program to check assembly completeness and also contamination is CheckM (Parks et al. 2015, documentation), but in the interest of time, we will only run Busco.\n\n\nBecause Busco will always output a number of files in your working directory, we will move into our desired output dir in advance:\ncd results/busco\nWe will also have to load a different Conda environment, again:\nsource activate /fs/ess/PAS0471/jelmer/conda/busco\nUsing the --lineage_dataset option, we have to tell Busco which lineage’s reference database it should use: there is a list of lineages on Busco’s website.\n\n Exercise: Busco database\nTake a look at the website linked to above. Which lineage dataset should we pick?\n\n\nClick for the solution\n\nThere is a database for the order that Pseudomonas is in: pseudomonadales.\nAlternatively, we could use the database for all bacteria: bacteria.\n\n\nOtherwise, we will use the following options:\n\n--in — input file\n--out — output ID (not the full filename)\n--mode genome — our input file is a genome assembly, not a transcriptome assembly or proteome\n\n# Run Busco\nbusco \\\n    --in ../spades/SM04/contigs.fasta \\\n    --out SM04 \\\n    --lineage_dataset pseudomonadales \\\n    --mode genome\n# (Only showing the bit of outkey results output)\n        ---------------------------------------------------\n        |Results from dataset pseudomonadales_odb10        |\n        ---------------------------------------------------\n        |C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782       |\n        |779    Complete BUSCOs (C)                        |\n        |779    Complete and single-copy BUSCOs (S)        |\n        |0      Complete and duplicated BUSCOs (D)         |\n        |1      Fragmented BUSCOs (F)                      |\n        |2      Missing BUSCOs (M)                         |\n        |782    Total BUSCO groups searched                |\n        ---------------------------------------------------\n\n\nClick to see the full expected Busco output\n\n2024-02-05 17:31:00 INFO:       ***** Start a BUSCO v5.5.0 analysis, current time: 02/05/2024 17:31:00 *****\n2024-02-05 17:31:00 INFO:       Configuring BUSCO with local environment\n2024-02-05 17:31:00 INFO:       Mode is genome\n2024-02-05 17:31:00 INFO:       Downloading information on latest versions of BUSCO data...\n2024-02-05 17:31:03 INFO:       Input file is /fs/scratch/PAS2250/cabana/jelmer_prep/results/spades/SM04/contigs.fasta\n2024-02-05 17:31:03 INFO:       Downloading file 'https://busco-data.ezlab.org/v5/data/lineages/pseudomonadales_odb10.2024-01-08.tar.gz'\n2024-02-05 17:31:06 INFO:       Decompressing file '/fs/scratch/PAS2250/cabana/jelmer_prep/busco_downloads/lineages/pseudomonadales_odb10.tar.gz'\n2024-02-05 17:31:24 INFO:       Running BUSCO using lineage dataset pseudomonadales_odb10 (prokaryota, 2024-01-08)\n2024-02-05 17:31:24 INFO:       Running 1 job(s) on bbtools, starting at 02/05/2024 17:31:24\n2024-02-05 17:31:26 INFO:       [bbtools]       1 of 1 task(s) completed\n2024-02-05 17:31:26 INFO:       ***** Run Prodigal on input to predict and extract genes *****\n2024-02-05 17:31:26 INFO:       Running Prodigal with genetic code 11 in single mode\n2024-02-05 17:31:26 INFO:       Running 1 job(s) on prodigal, starting at 02/05/2024 17:31:26\n2024-02-05 17:31:44 INFO:       [prodigal]      1 of 1 task(s) completed\n2024-02-05 17:31:45 INFO:       Genetic code 11 selected as optimal\n2024-02-05 17:31:45 INFO:       ***** Run HMMER on gene sequences *****\n2024-02-05 17:31:45 INFO:       Running 782 job(s) on hmmsearch, starting at 02/05/2024 17:31:45\n2024-02-05 17:31:55 INFO:       [hmmsearch]     79 of 782 task(s) completed\n2024-02-05 17:32:04 INFO:       [hmmsearch]     157 of 782 task(s) completed\n2024-02-05 17:32:13 INFO:       [hmmsearch]     235 of 782 task(s) completed\n2024-02-05 17:32:21 INFO:       [hmmsearch]     313 of 782 task(s) completed\n2024-02-05 17:32:28 INFO:       [hmmsearch]     392 of 782 task(s) completed\n2024-02-05 17:32:34 INFO:       [hmmsearch]     470 of 782 task(s) completed\n2024-02-05 17:32:42 INFO:       [hmmsearch]     548 of 782 task(s) completed\n2024-02-05 17:32:47 INFO:       [hmmsearch]     626 of 782 task(s) completed\n2024-02-05 17:32:53 INFO:       [hmmsearch]     704 of 782 task(s) completed\n2024-02-05 17:33:01 INFO:       [hmmsearch]     782 of 782 task(s) completed\n2024-02-05 17:33:19 INFO:       Results:        C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782        \n\n2024-02-05 17:33:21 INFO:\n\n        ---------------------------------------------------\n        |Results from dataset pseudomonadales_odb10        |\n        ---------------------------------------------------\n        |C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782       |\n        |779    Complete BUSCOs (C)                        |\n        |779    Complete and single-copy BUSCOs (S)        |\n        |0      Complete and duplicated BUSCOs (D)         |\n        |1      Fragmented BUSCOs (F)                      |\n        |2      Missing BUSCOs (M)                         |\n        |782    Total BUSCO groups searched                |\n        ---------------------------------------------------\n2024-02-05 17:33:21 INFO:       BUSCO analysis done. Total running time: 138 seconds\n2024-02-05 17:33:21 INFO:       Results written in /fs/scratch/PAS2250/cabana/jelmer_prep/SM04\n2024-02-05 17:33:21 INFO:       For assistance with interpreting the results, please consult the userguide: https://busco.ezlab.org/busco_userguide.html\n\n2024-02-05 17:33:21 INFO:       Visit this page https://gitlab.com/ezlab/busco#how-to-cite-busco to see how to cite BUSCO\n\nThat is looking pretty good, 99.6% (n=779) of expected genes are indeed present completely and as a single copy. Only 0.1% (n=1) of genes are fragmented and 0.3% (n=2) are missing.\nFinally, we should move back to your main project dir, and load the main Conda environment\ncd ../..\n\nsource activate /fs/ess/PAS0471/jelmer/conda/cabana"
  },
  {
    "objectID": "bac/02_assembly.html#bonus-assembly-filtering-with-kraken2",
    "href": "bac/02_assembly.html#bonus-assembly-filtering-with-kraken2",
    "title": "Genome assembly and assembly QC",
    "section": "6 Bonus: Assembly filtering with Kraken2",
    "text": "6 Bonus: Assembly filtering with Kraken2\nYou’ll probably want to filter your assembly, based on:\n\nMinimum contig size\nMinimum contig depth of coverage\nInferred contamination from other organisms\n\nHere, we will perform the third and most complex of these.\n\n\n\n\n\n\nFiltering on contig size (Click to expand)\n\n\n\n\n\n200 bp is the minimum contig size when you upload a genome assembly to NCBI. But you may want to be more stringent, e.g. using a 300 or 500 bp threshold.\nTODO - Add seqkit command\n\n\n\n\n\n6.1 Kraken2\nTo identify contaminant contigs, we will run the program Kraken2 (Wood et al. 2019, manual). This is a general purpose lowest-common ancestor (LCA) taxonomic classifier of sequences (can be reads, contigs, etc).\n(It is also possible to run Kraken2 or a similar program on the reads rather than on the assembly, but this can be more error-prone due to errors and their shorter lengths compared to (most) contigs.)\nKraken requires a reference database. Ready-made databases can be downloaded from this site — in this case, I already downloaded one for you, which we can use.\nCheck that you are back in your bact dir, and have the cabana Conda environment active.\npwd\n# Should be:\n/fs/scratch/PAS2250/cabana/$USER/bact/bact\nWe will start by creating an output dir for Kraken:\n# (If we don't create the output dir, Kraken will not produce output files!)\nmkdir results/kraken\n\n\n\n6.2 Running Kraken\nWe’ll run Kraken2 with the following options:\n\n--db /fs/scratch/PAS2250/cabana/databases/kraken_std — the database for Kraken2\n--minimum-hit-groups 3 (Following recommendations from Lu et al. 2022)\n--confidence 0.15 (Following recommendations from Wright et al. 2023)\n--report and --output to indicate where the output files should go\n--threads 20 to use 20 threads\nAnd finally, the input file (our assembly) is passed as an argument at the end of the command\n\n# Run Kraken2 -- like with Spades, we will run it as a batch job\nsbatch -A PAS2250 -t 5 -c 20 -o slurm-kraken.out --wrap=\"\n  kraken2 \\\n    --db /fs/scratch/PAS2250/cabana/databases/kraken_std \\\n    --minimum-hit-groups 3 \\\n    --confidence 0.15 \\\n    --threads 20 \\\n    --report results/kraken/SM04_report.txt \\\n    --output results/kraken/SM04_main.txt \\\n    results/spades/SM04/contigs.fasta\n\"\nOnce its done (this should only take 1-2 minutes), the Slurm log file should contain the following:\ncat slurm-kraken.out\nLoading database information... done.\n86 sequences (5.97 Mbp) processed in 0.619s (8.3 Kseq/m, 578.49 Mbp/m).\n  84 sequences classified (97.67%)\n  2 sequences unclassified (2.33%)\n\n\n\n6.3 Interpreting the Kraken output\nLet’s take a look at the output files:\nls -lh results/kraken\n-rw-r--r-- 1 jelmer PAS0471 3.0M Feb  5 15:41 SM04_main.txt\n-rw-r--r-- 1 jelmer PAS0471 3.5K Feb  5 15:41 SM04_report.txt\nThe report file (report.txt) file has a summary of taxonomic assignments, whereas the main output file (main.txt) has one line for each contig with its taxonomic assignment.\nWe’ll first take a look at the report file, which has the following columns:\n\nPercentage of fragments covered by the clade rooted at this taxon\nNumber of fragments covered by the clade rooted at this taxon\nNumber of fragments assigned directly to this taxon\nA rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies.\nNCBI taxonomic ID number\nIndented scientific name\n\nless -S results/kraken/SM04_report.txt\n\n\nClick to show the contents of the file\n\n  2.33  2   2   U   0   unclassified\n 97.67  84  0   R   1   root\n 97.67  84  0   R1  131567    cellular organisms\n 73.26  63  0   D   2       Bacteria\n 72.09  62  3   P   1224          Pseudomonadota\n 68.60  59  2   C   1236            Gammaproteobacteria\n 58.14  50  0   O   72274             Pseudomonadales\n 58.14  50  1   F   135621              Pseudomonadaceae\n 56.98  49  12  G   286               Pseudomonas\n 43.02  37  1   G1  136849                  Pseudomonas syringae group\n 41.86  36  0   G2  251695                    Pseudomonas syringae group genomosp. 1\n 41.86  36  36  S   317                     Pseudomonas syringae\n  8.14  7   0   O   91347             Enterobacterales\n  8.14  7   2   F   543             Enterobacteriaceae\n  5.81  5   5   G   590               Salmonella\n  1.16  1   0   D1  1783272       Terrabacteria group\n  1.16  1   0   P   201174          Actinomycetota\n  1.16  1   0   C   1760              Actinomycetes\n  1.16  1   0   O   85009               Propionibacteriales\n  1.16  1   0   F   31957                 Propionibacteriaceae\n  1.16  1   0   G   1912216                 Cutibacterium\n  1.16  1   1   S   1747                      Cutibacterium acnes\n 24.42  21  0   D   2759        Eukaryota\n 24.42  21  0   D1  33154         Opisthokonta\n 24.42  21  0   K   33208           Metazoa\n 24.42  21  0   K1  6072              Eumetazoa\n 24.42  21  0   K2  33213               Bilateria\n 24.42  21  0   K3  33511                 Deuterostomia\n 24.42  21  0   P   7711                    Chordata\n 24.42  21  0   P1  89593                     Craniata\n 24.42  21  0   P2  7742                        Vertebrata\n 24.42  21  0   P3  7776                          Gnathostomata\n 24.42  21  0   P4  117570                          Teleostomi\n 24.42  21  0   P5  117571                            Euteleostomi\n 24.42  21  0   P6  8287                                Sarcopterygii\n 24.42  21  0   P7  1338369                               Dipnotetrapodomorpha\n 24.42  21  0   P8  32523                                   Tetrapoda\n 24.42  21  0   P9  32524                                     Amniota\n 24.42  21  0   C   40674                                       Mammalia\n 24.42  21  0   C1  32525                                         Theria\n 24.42  21  0   C2  9347                                            Eutheria\n 24.42  21  0   C3  1437010                                           Boreoeutheria\n 24.42  21  0   C4  314146                                              Euarchontoglires\n 24.42  21  0   O   9443                                                  Primates\n 24.42  21  0   O1  376913                                                  Haplorrhini\n 24.42  21  0   O2  314293                                                    Simiiformes\n 24.42  21  0   O3  9526                                                        Catarrhini\n 24.42  21  0   O4  314295                                                        Hominoidea\n 24.42  21  0   F   9604                                                            Hominidae\n 24.42  21  0   F1  207598                                                            Homininae\n 24.42  21  0   G   9605                                                                Homo\n 24.42  21  21  S   9606                                                                  Homo sapiens\n\n\n Exercise: Contamination?\nTry to interpret the Kraken report — are there any contaminant contigs?\n\n\nClick for the solution\n\nOuch! While the majority of our contigs have been classified as Pseudomonas syringae, we also have a few other bacteria (including the human skin bacterium Cutibacterium acnes), and no fewer than 21 human contigs!\n\n\n\nThe 5th column of the Kraken report has the NCBI taxonomic IDs, and that of human (on the last line) is 9606.\nThe main.txt output file reports the taxonomic ID in the 3rd column, so we can use the following command to print just the lines where the 3rd column is 9606:\nawk '$3 == 9606' results/kraken/SM04_main.txt\nC       NODE_28_length_766_cov_1.003130 9606    766     9606:4 131567:5 9606:1 131567:36 9606:11 131567:1 9606:20 131567:47 9606:16 131567:2 9606:5 131567:1 9606:1 131567:1 9606:6 131567:32 9606:68 131567:18 9606:95 131567:146 9606:12 131567:4 9606:11 131567:96 9606:28 131567:5 9606:11 131567:2 9606:29 131567:2 9606:1 131567:5 9606:10\nC       NODE_33_length_621_cov_0.706478 9606    621     9606:180 0:1 9606:3 0:17 9606:1 0:2 9606:12 0:32 9606:199 131567:5 9606:21 131567:2 1:1 9606:111\nC       NODE_36_length_567_cov_0.784091 9606    567     0:6 9606:527\nC       NODE_37_length_563_cov_0.779817 9606    563     9606:271 0:29 9606:229\nC       NODE_39_length_553_cov_0.809859 9606    553     0:1 9606:7 0:3 9606:261 0:3 9606:1 0:15 9606:3 0:5 9606:220\nC       NODE_40_length_552_cov_0.809412 9606    552     9606:41 131567:1 9606:1 131567:17 9606:10 131567:5 9606:56 131567:5 9606:89 131567:6 9606:63 131567:3 9606:84 131567:1 9606:1 131567:17 9606:10 131567:5 9606:103\nC       NODE_42_length_521_cov_0.746193 9606    521     9606:349 0:9 9606:1 0:21 9606:107\nC       NODE_44_length_510_cov_0.906005 9606    510     9606:476\nC       NODE_45_length_497_cov_0.772973 9606    497     9606:218 0:47 9606:1 0:28 9606:1 0:15 9606:1 0:4 9606:140 0:8\nC       NODE_47_length_480_cov_0.866856 9606    480     9606:446\nC       NODE_48_length_479_cov_0.849432 9606    479     9606:445\nC       NODE_50_length_470_cov_1.011662 9606    470     9606:117 0:1 9606:5 0:1 9606:3 0:24 9606:285\nC       NODE_51_length_470_cov_0.915452 9606    470     9606:436\nC       NODE_52_length_466_cov_1.017699 9606    466     9606:20 0:17 9606:6 0:9 9606:135 0:30 9606:215\nC       NODE_54_length_458_cov_1.000000 9606    458     9606:213 0:4 9606:5 0:15 9606:1 0:5 9606:125 0:20 9606:2 0:5 9606:3 0:5 9606:21\nC       NODE_55_length_455_cov_1.179878 9606    455     9606:2 131567:3 9606:218 131567:3 9606:195\nC       NODE_60_length_444_cov_0.933754 9606    444     9606:410\nC       NODE_61_length_442_cov_0.907937 9606    442     9606:21 131567:1 9606:386\nC       NODE_65_length_438_cov_0.405145 9606    438     0:24 9606:2 0:8 9606:80 131567:2 9606:31 131567:19 9606:10 131567:3 9606:13 131567:7 9606:205\nC       NODE_66_length_433_cov_1.133987 9606    433     9606:399\nC       NODE_67_length_432_cov_0.718033 9606    432     9606:398\n\n Exercise: Which contigs are contaminants?\nIn the output above, the contig IDs are in the second column. Do you notice anything about these? Does that provide some independent support for the idea that they are contaminants?\n\n\nClick for the solution\n\nAll of these contigs are small (&lt;600 bp) and have very low coverage (&lt;1.2x, versus the &gt;30x we saw for the contigs IDs that we printed earlier).\nNote that Kraken doesn’t use this kind of information at all, so this provides independent evidence that this is contamination.\n\n\n\n\n\n6.4 Removing contaminant contigs\nWe will use the Kraken’s companion program KrakenTools (paper, documentation) to remove the contaminant contigs, with options:\n\n-k — main Kraken output file\n-s — input sequence file to be filtered\no — output sequence file\n-t — NCBI taxonomic ID (9606 = human)\n--exclude — exclude (rather than extract) contigs with the specified taxonomic ID\n\nmkdir results/decontam\nextract_kraken_reads.py \\\n    -k results/kraken/SM04_main.txt \\\n    -s results/spades/SM04/contigs.fasta \\\n    -o results/decontam/SM04.fasta \\\n    -t 9606 \\\n    --exclude\nPROGRAM START TIME: 02-05-2024 22:16:21\n        1 taxonomy IDs to parse\n&gt;&gt; STEP 1: PARSING KRAKEN FILE FOR READIDS results/kraken/SM04.main.txt\n        0.00 million reads processed\n        65 read IDs saved\n&gt;&gt; STEP 2: READING SEQUENCE FILES AND WRITING READS\n        65 read IDs found (0.00 mill reads processed)\n        65 reads printed to file\n        Generated file: results/decontam/SM04.fasta\nPROGRAM END TIME: 02-05-2024 22:16:21\nLet’s check that we indeed have 65 contigs left:\ngrep -c \"&gt;\" results/decontam/SM04.fasta\n65"
  },
  {
    "objectID": "bac/02_assembly.html#footnotes",
    "href": "bac/02_assembly.html#footnotes",
    "title": "Genome assembly and assembly QC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that because individual sequence entries are commonly spread across multiple lines, FASTA entries do not necessarily cover 2 lines (cf. FASTQ).↩︎"
  }
]