{
  "hash": "083e907cc81a53a1517844934c7bec75",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Genome assembly and assembly QC\"\npagetitle: \"Genome assembly\"\nauthor: Jelmer Poelstra\ndate: 2024-02-08\nexecute: \n  eval: false\nknitr:\n  opts_chunk:\n    out.width: \"85%\"\n    class-output: bash-out\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n<br>\n\n## Introduction {-}\n\nWe will now create a genome assembly from the preprocessed reads with the program **SPAdes**.\n\n_After the assembly_, we will run several assembly QC and filtering steps to:\n\n- Get some basic assembly summary stats with BBtools\n- Run Busco to check for genome completeness\n- Identify and remove contaminant contigs with Kraken2\n\n::: exercise\n#### {{< fa user-edit >}} **Setting up**\n\nYou should have an active VS Code session with an open terminal.\nIn that terminal, you should be be in your dir\n`/fs/scratch/PAS2250/cabana/$USER/bact/bact`.\n:::\n\n<br>\n\n## The FASTA format\n\nThe genome assembly that we will create will be in the FASTA formet.\nFASTA files contain one or more DNA or amino acid sequences,\nwith no limits on the number of sequences or the sequence lengths.\n\nThe following example FASTA file contains two entries:\n\n``` bash-in-nocolor\n>unique_sequence_ID Optional description\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAAAA\n>unique_sequence_ID2\nATTCATTAAAGCAGTTTATTGGCTTAATGTACATCAGTGAAATCATAAATGCTAAATG\n```\n\nEach entry consists of a **header** line and the **sequence** itself. Header lines **start with a `>`** (greater-than sign) and are otherwise \"free form\", though the idea is that they provide an identifier for the sequence that follows.[^1]\n\n[^1]: Note that because individual sequence entries are commonly spread across multiple lines, FASTA entries do not necessarily cover 2 lines (cf. FASTQ).\n\n::: {.callout-note collapse=\"true\"}\n#### FASTA file name extensions are variable: `.fa`, `.fasta`, `.fna`, `.faa` (*Click to expand*)\n\n-   \"Generic\" extensions are `.fasta` and `.fa` (e.g: `my_assembly.fasta`)\n-   Also used are extensions that explicitly indicate whether sequences are *nucleotides* (`.fna`) or *amino acids* (`.faa`)\n:::\n\n<br>\n\n## \"Batch jobs\" with `sbatch`\n\nSo far, we having been running programs by directly typing/pasting the commands\nin the terminal.\nBecause SPAdes needs more computing resources, we will run it differently.\n\nWe will submit it as a so-called \"batch\" (non-interactive) job using the `sbatch` command.\nThis job will then be run on a compute node that we ourselves never move to (!).\n\nTo first see a simple example, say that we just wanted to run `echo Hello there $USER`\nas a batch job, where we'll use these options:\n\n- `-A` for the OSC Project we want to use\n- `-t` for the time in minutes that we need\n- `-c` for the number of cores that we need\n- `-o` for the name of the log file (where \"Hello there <user>\" will be printed)\n- The command that we want to run in the batch job is wrapped in `wrap=\"<command>\"`\n\n```bash\nsbatch -A PAS2250 -t 1 -c 1 -o slurm-hello.out \\\n  --wrap=\"echo Hello there $USER\"\n```\n```bash-out\nSubmitted batch job 25928455\n# [You will get a different number, each job has a unique ID]\n```\n\nNow, the output of our command (`ls -lh`) is not printed to the screen,\nbut will end up in a file in our working directory, whose name starts with `slurm`\nand contains the job ID number:\n\n```bash\nls\n```\n```bash-out\ndata  README.md  results  slurm-hello.out\n```\n\n::: callout-tip\n### The `slurm` file will only show up once the job has started running, which can take up to a minute or so.\n:::\n\nLet's take a look at that \"Slurm log file\":\n\n```bash\ncat slurm-hello.out\n```\n```bash-out\nHello there jelmer\n```\n\n<br>\n\n## Assembly with SPAdes\n\nSPAdes is a well-performing and very flexible assembler that can be used to do many\nkinds of assemblies, including metagenomes and transcriptomes.\nIt has a special \"mode\" for bacterial isolate assembly,\nwhich can be activated with the `--isolate` flag.\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Our SPAdes command\n\nWe will run SPAdes with the following options:\n\n- `-1` and `-2` for the R1 and R2 FASTQ files\n- `-o` for the output dir, which should be sample-specific, and should not yet exist\n- `--isolate` to activate the bacterial isolate mode\n- `-k 21,33,55,77,99,127` to assemble with a variety of kmer sizes\n- `--threads 20` and `--memory 80` to use 20 threads and 80 GB of memory\n\n```bash\n# (Don't run this yet)\nspades.py \\\n  -1 results/trimgalore/SM04_R1_val_1.fq.gz \\\n  -2 results/trimgalore/SM04_R2_val_2.fq.gz \\\n  -o results/spades/SM04 \\\n  -k 21,33,55,77,99,127 \\\n  --isolate \\\n  --threads 20 \\\n  --memory 80\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Running SPAdes\n\nFirst, we'll have to switch back to the `cabana` Conda environment,\nsince we activated a different Conda environment for TrimGalore earlier.\n\n```bash\nsource activate /fs/ess/PAS0471/jelmer/conda/cabana\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n#### Submitting the SPAdes `sbatch` job\n\nNow we're ready to submit our SPAdes job,\nwith 30 minutes (`-t 30`) and 20 cores\n(`-c 20`):\n\n```bash\nsbatch -A PAS2250 -t 30 -c 20 -o slurm-spades.out --wrap=\"\n    spades.py \\\n      -1 results/trimgalore/SM04_R1_val_1.fq.gz \\\n      -2 results/trimgalore/SM04_R2_val_2.fq.gz \\\n      -o results/spades/SM04 \\\n      -k 21,33,55,77,99,127 \\\n      --isolate \\\n      --threads 20 \\\n      --memory 80\n\"\n```\n\n::: exercise\n\n#### {{< fa user-edit >}} **Exercise**: Monitor the SPAdes run\n\n- Use `less` to check the `slurm-spades.out` file, which will have the SPAdes \"log\".\n\n- In `less`, press <kbd>G</kbd> (_capital_ G!) to look at the end of the file.\n\nWhen you first check the `slurm-spades.out` file, SPAdes should still be running.\nYou know it will be  done when you see the following line at the end:\n\n```bash-out\nThank you for using SPAdes!\n```\n\n- To monitor the progress of the SPAdes run, you can use `tail -f slurm-spades.out`,\n  which will \"follow\" the file and add any new text in real-time!\n  To exit this, press <kbd>Ctrl</kbd>+<kbd>c</kbd>.\n  \n::: callout-note\n#### SPAdes may take 5-10 minutes to complete\n:::\n\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### SPAdes output files\n\nLet's check the files in the output dir:\n\n```bash\nls -lh results/spades/SM04\n```\n\n<details><summary>Click to show the output</summary>\n\n```bash-out\ntotal 47M\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 assembly_graph_after_simplification.gfa\n-rw-r--r-- 1 jelmer PAS0471  12M Feb  4 21:49 assembly_graph.fastg\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 assembly_graph_with_scaffolds.gfa\n-rw-r--r-- 1 jelmer PAS0471 5.9M Feb  4 21:49 before_rr.fasta\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 contigs.fasta\n-rw-r--r-- 1 jelmer PAS0471 9.8K Feb  4 21:49 contigs.paths\n-rw-r--r-- 1 jelmer PAS0471   79 Feb  4 21:42 dataset.info\n-rw-r--r-- 1 jelmer PAS0471  278 Feb  4 21:42 input_dataset.yaml\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:49 K127\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:43 K21\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:44 K33\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:45 K55\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:47 K77\ndrwxr-xr-x 4 jelmer PAS0471 4.0K Feb  4 21:48 K99\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 logs\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 misc\n-rw-r--r-- 1 jelmer PAS0471 1.5K Feb  4 21:42 params.txt\ndrwxr-xr-x 2 jelmer PAS0471 4.0K Feb  4 21:49 pipeline_state\n-rw-r--r-- 1 jelmer PAS0471 3.4K Feb  4 21:42 run_spades.sh\n-rw-r--r-- 1 jelmer PAS0471 4.9K Feb  4 21:42 run_spades.yaml\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 scaffolds.fasta\n-rw-r--r-- 1 jelmer PAS0471 8.9K Feb  4 21:49 scaffolds.paths\n-rw-r--r-- 1 jelmer PAS0471 5.8M Feb  4 21:49 SM04\n-rw-r--r-- 1 jelmer PAS0471 194K Feb  4 21:49 spades.log\n```\n</details>\n\nThere are quite some files, as well as subdirs for different k-mer sizes,\nbut we're really only interested in the assembly FASTA file, which is `contigs.fasta`.\nLet's take a look at that file:\n\n```bash\nless results/spades/SM04/contigs.fasta\n```\n```bash-out\n>NODE_1_length_1267796_cov_33.239498\nACCTTGAGTTCCCTAAAGGGCCGTCGAAGACTACGACGTTGATAGGTTGGGTGTGTAAGC\nGCTGTGAGGCGTTGAGCTAACCAATACTAATTGCCCGTGAGGCTTGACCATATAACACCC\nAAGCAATTTGCGTTGAATGAGCAGATTGCGGTGACTGTGAAGATGACACGAACCGAAAGT\nTTGCGTCACGAACGACACCTGAACCAGCTTGCTATCACATACCCGATTTGCTGAAGCGCG\nCCGCAAGGCACGATTCGGTACCCGAATTTCTTGACGACCATAGAGCATTGGAACCACCTG\nATCCCATCCCGAACTCAGTAGTGAAACGATGTATCGCCGATGGTAGTGTGGGGTTTCCCC\nATGTGAGAGTAGGTCATCGTCAAGATTAAATTCCAGAAACCCTCATCGCTTACGCGTTGA\nGGGTTTTTGTTTGTCTGGGGTTCCAGAAACCTCTGCATTCTCTATCTGGCTCATCTCATT\nGCAATGCAGCCGCATTGGCGCCAGAGACCCCCAAGGTTTAGTGAAACGCCCCCATCCCTG\n```\n\nIn this file, **each contig is one FASTA entry**.\nThe contig headers have some metadata, such as its length in base pairs,\nand its depth of coverage (`cov_`; i.e. how many reads, on average, cover each base).\n\nWe can see a few more headers by using the **`grep`** command,\nwhich will print lines matching a search pattern (in our case, the **`>`** from the header),\nas follows:\n\n```bash\n# (Do NOT omit the quotes around the \">\"!)\ngrep \">\" results/spades/SM04/contigs.fasta | head\n```\n```bash-out\n>NODE_1_length_1267796_cov_33.239498\n>NODE_2_length_902255_cov_32.000245\n>NODE_3_length_697265_cov_34.901625\n>NODE_4_length_534491_cov_32.088021\n>NODE_5_length_350317_cov_33.463137\n>NODE_6_length_339735_cov_31.812540\n>NODE_7_length_291220_cov_35.951730\n>NODE_8_length_274792_cov_32.455031\n>NODE_9_length_167931_cov_33.795917\n>NODE_10_length_164349_cov_34.581646\n```\n\n::: callout-tip\n#### If you don't pipe (**`|`**) the `grep` output to `head`, you will see all headers\n:::\n\nIf we use `grep`'s `-c` option, it will count the number of matching lines,\nwhich will give us a count of the number of contigs:\n\n```bash\ngrep -c \">\" results/spades/SM04/contigs.fasta\n```\n```bash-out\n86\n```\n\n<br>\n\n## Basic assembly stats\n\nWe can use the tools `stats.sh` from the [BBTools](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/)\nsuite of genomics tools to get some (more) basic statistics for our genome assembly:\n\n```bash\nstats.sh results/spades/SM04/contigs.fasta \n```\n```bash-out\nA       C       G       T       N       IUPAC   Other   GC      GC_stdev\n0.2048  0.2969  0.2945  0.2038  0.0000  0.0000  0.0000  0.5914  0.0875\n\nMain genome scaffold total:             86\nMain genome contig total:               86\nMain genome scaffold sequence total:    5.968 MB\nMain genome contig sequence total:      5.968 MB        0.000% gap\nMain genome scaffold N/L50:             4/534.491 KB\nMain genome contig N/L50:               4/534.491 KB\nMain genome scaffold N/L90:             14/97.943 KB\nMain genome contig N/L90:               14/97.943 KB\nMax scaffold length:                    1.268 MB\nMax contig length:                      1.268 MB\nNumber of scaffolds > 50 KB:            18\n% main genome in scaffolds > 50 KB:     96.64%\n```\n\n<br>\n\n## Assembly completeness check with Busco\n\nWe will use the program Busco\n([Manni et al. 2021](https://academic.oup.com/mbe/article/38/10/4647/6329644),\n[documentation](https://busco.ezlab.org/))\nto check how complete our genome assembly is.\nBusco checks for the presence of genes that are expected to be\n_universally present in a single copy_ in a specific taxonomic lineage.\n\n::: callout-note\n#### Other tool options\nAnother commonly used program to check assembly completeness and also contamination is CheckM\n([Parks et al. 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4484387/),\n[documentation](https://ecogenomics.github.io/CheckM/)),\nbut in the interest of time, we will only run Busco.\n:::\n\nBecause Busco will always output a number of files in your working directory,\nwe will move into our desired output dir in advance:\n\n```bash\ncd results/busco\n```\n\nWe will also have to load a different Conda environment, again:\n\n```bash\nsource activate /fs/ess/PAS0471/jelmer/conda/busco\n```\n\nUsing the **`--lineage_dataset`** option,\nwe have to tell Busco which lineage's reference database it should use:\nthere is a [list of lineages on Busco's website](https://busco.ezlab.org/list_of_lineages.html).\n\n::: exercise\n{{< fa user-edit >}} **Exercise:** Busco database\n\nTake a look at the website linked to above. Which lineage dataset should we pick?\n\n<details><summary>Click for the solution</summary>\nThere is a database for the order that Pseudomonas is in: `pseudomonadales`.\n\nAlternatively, we could use the database for all bacteria: `bacteria`.\n\n</details>\n:::\n\nOtherwise, we will use the following options:\n\n- `--in` --- input file\n- `--out` --- output ID (not the full filename)\n- `--mode genome` --- our input file is a genome assembly, not a transcriptome assembly or proteome\n\n```bash\n# Run Busco\nbusco \\\n    --in ../spades/SM04/contigs.fasta \\\n    --out SM04 \\\n    --lineage_dataset pseudomonadales \\\n    --mode genome\n```\n```bash-out\n# (Only showing the bit of outkey results output)\n        ---------------------------------------------------\n        |Results from dataset pseudomonadales_odb10        |\n        ---------------------------------------------------\n        |C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782       |\n        |779    Complete BUSCOs (C)                        |\n        |779    Complete and single-copy BUSCOs (S)        |\n        |0      Complete and duplicated BUSCOs (D)         |\n        |1      Fragmented BUSCOs (F)                      |\n        |2      Missing BUSCOs (M)                         |\n        |782    Total BUSCO groups searched                |\n        ---------------------------------------------------\n```\n\n<details><summary>Click to see the full expected Busco output</summary>\n\n``` bash-out\n2024-02-05 17:31:00 INFO:       ***** Start a BUSCO v5.5.0 analysis, current time: 02/05/2024 17:31:00 *****\n2024-02-05 17:31:00 INFO:       Configuring BUSCO with local environment\n2024-02-05 17:31:00 INFO:       Mode is genome\n2024-02-05 17:31:00 INFO:       Downloading information on latest versions of BUSCO data...\n2024-02-05 17:31:03 INFO:       Input file is /fs/scratch/PAS2250/cabana/jelmer_prep/results/spades/SM04/contigs.fasta\n2024-02-05 17:31:03 INFO:       Downloading file 'https://busco-data.ezlab.org/v5/data/lineages/pseudomonadales_odb10.2024-01-08.tar.gz'\n2024-02-05 17:31:06 INFO:       Decompressing file '/fs/scratch/PAS2250/cabana/jelmer_prep/busco_downloads/lineages/pseudomonadales_odb10.tar.gz'\n2024-02-05 17:31:24 INFO:       Running BUSCO using lineage dataset pseudomonadales_odb10 (prokaryota, 2024-01-08)\n2024-02-05 17:31:24 INFO:       Running 1 job(s) on bbtools, starting at 02/05/2024 17:31:24\n2024-02-05 17:31:26 INFO:       [bbtools]       1 of 1 task(s) completed\n2024-02-05 17:31:26 INFO:       ***** Run Prodigal on input to predict and extract genes *****\n2024-02-05 17:31:26 INFO:       Running Prodigal with genetic code 11 in single mode\n2024-02-05 17:31:26 INFO:       Running 1 job(s) on prodigal, starting at 02/05/2024 17:31:26\n2024-02-05 17:31:44 INFO:       [prodigal]      1 of 1 task(s) completed\n2024-02-05 17:31:45 INFO:       Genetic code 11 selected as optimal\n2024-02-05 17:31:45 INFO:       ***** Run HMMER on gene sequences *****\n2024-02-05 17:31:45 INFO:       Running 782 job(s) on hmmsearch, starting at 02/05/2024 17:31:45\n2024-02-05 17:31:55 INFO:       [hmmsearch]     79 of 782 task(s) completed\n2024-02-05 17:32:04 INFO:       [hmmsearch]     157 of 782 task(s) completed\n2024-02-05 17:32:13 INFO:       [hmmsearch]     235 of 782 task(s) completed\n2024-02-05 17:32:21 INFO:       [hmmsearch]     313 of 782 task(s) completed\n2024-02-05 17:32:28 INFO:       [hmmsearch]     392 of 782 task(s) completed\n2024-02-05 17:32:34 INFO:       [hmmsearch]     470 of 782 task(s) completed\n2024-02-05 17:32:42 INFO:       [hmmsearch]     548 of 782 task(s) completed\n2024-02-05 17:32:47 INFO:       [hmmsearch]     626 of 782 task(s) completed\n2024-02-05 17:32:53 INFO:       [hmmsearch]     704 of 782 task(s) completed\n2024-02-05 17:33:01 INFO:       [hmmsearch]     782 of 782 task(s) completed\n2024-02-05 17:33:19 INFO:       Results:        C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782        \n\n2024-02-05 17:33:21 INFO:\n\n        ---------------------------------------------------\n        |Results from dataset pseudomonadales_odb10        |\n        ---------------------------------------------------\n        |C:99.6%[S:99.6%,D:0.0%],F:0.1%,M:0.3%,n:782       |\n        |779    Complete BUSCOs (C)                        |\n        |779    Complete and single-copy BUSCOs (S)        |\n        |0      Complete and duplicated BUSCOs (D)         |\n        |1      Fragmented BUSCOs (F)                      |\n        |2      Missing BUSCOs (M)                         |\n        |782    Total BUSCO groups searched                |\n        ---------------------------------------------------\n2024-02-05 17:33:21 INFO:       BUSCO analysis done. Total running time: 138 seconds\n2024-02-05 17:33:21 INFO:       Results written in /fs/scratch/PAS2250/cabana/jelmer_prep/SM04\n2024-02-05 17:33:21 INFO:       For assistance with interpreting the results, please consult the userguide: https://busco.ezlab.org/busco_userguide.html\n\n2024-02-05 17:33:21 INFO:       Visit this page https://gitlab.com/ezlab/busco#how-to-cite-busco to see how to cite BUSCO\n```\n</details>\n\nThat is looking pretty good,\n99.6% (n=779) of expected genes are indeed present completely and as a single copy.\nOnly 0.1% (n=1) of genes are fragmented and 0.3% (n=2) are missing.\n\nFinally, we should move back to your main project dir,\nand load the main Conda environment\n\n```bash\ncd ../..\n\nsource activate /fs/ess/PAS0471/jelmer/conda/cabana\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n## Bonus: Assembly filtering with Kraken2\n\nYou'll probably want to filter your assembly, based on:\n\n- Minimum contig size\n- Minimum contig depth of coverage\n- Inferred contamination from other organisms\n\nHere, we will perform the third and most complex of these.\n\n::: {.callout-note collapse=\"true\"}\n#### Filtering on contig size _(Click to expand)_\n200 bp is the minimum contig size when you upload a genome assembly to NCBI.\nBut you may want to be more stringent, e.g. using a 300 or 500 bp threshold.\n\nTODO - Add `seqkit` command\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Kraken2\n\nTo identify contaminant contigs, we will run the program Kraken2\n([Wood et al. 2019](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0),\n[manual](https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown)).\nThis is a general purpose lowest-common ancestor (LCA)\n**taxonomic classifier of sequences** (can be reads, contigs, etc).\n\n(It is also possible to run Kraken2 or a similar program on the _reads_ rather\nthan on the assembly, but this can be more error-prone due to errors and their shorter\nlengths compared to (most) contigs.)\n\nKraken requires a reference database.\nReady-made databases can be downloaded from\n[this site](https://benlangmead.github.io/aws-indexes/) ---\nin this case, I already downloaded one for you, which we can use.\n\n**Check that you are back in your `bact` dir, and have the cabana Conda environment active.**\n\n```bash\npwd\n```\n```bash-out\n# Should be:\n/fs/scratch/PAS2250/cabana/$USER/bact/bact\n```\n\nWe will start by creating an output dir for Kraken:\n\n```bash\n# (If we don't create the output dir, Kraken will not produce output files!)\nmkdir results/kraken\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Running Kraken\n\nWe'll run Kraken2 with the following options:\n\n- `--db /fs/scratch/PAS2250/cabana/databases/kraken_std` --- the database for Kraken2\n- `--minimum-hit-groups 3` (Following recommendations from [Lu et al. 2022](https://www.nature.com/articles/s41596-022-00738-y))\n- `--confidence 0.15` (Following recommendations from [Wright et al. 2023](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000949))\n- `--report` and `--output` to indicate where the output files should go\n- `--threads 20` to use 20 threads\n- And finally, the input file (our assembly) is passed as an argument at the end of the command\n\n```bash\n# Run Kraken2 -- like with Spades, we will run it as a batch job\nsbatch -A PAS2250 -t 5 -c 20 -o slurm-kraken.out --wrap=\"\n  kraken2 \\\n    --db /fs/scratch/PAS2250/cabana/databases/kraken_std \\\n    --minimum-hit-groups 3 \\\n    --confidence 0.15 \\\n    --threads 20 \\\n    --report results/kraken/SM04_report.txt \\\n    --output results/kraken/SM04_main.txt \\\n    results/spades/SM04/contigs.fasta\n\"\n```\n\nOnce its done (this should only take 1-2 minutes),\nthe Slurm log file should contain the following:\n\n```bash\ncat slurm-kraken.out\n```\n```bash-out\nLoading database information... done.\n86 sequences (5.97 Mbp) processed in 0.619s (8.3 Kseq/m, 578.49 Mbp/m).\n  84 sequences classified (97.67%)\n  2 sequences unclassified (2.33%)\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Interpreting the Kraken output\n\nLet's take a look at the output files:\n\n```bash\nls -lh results/kraken\n```\n```bash-out\n-rw-r--r-- 1 jelmer PAS0471 3.0M Feb  5 15:41 SM04_main.txt\n-rw-r--r-- 1 jelmer PAS0471 3.5K Feb  5 15:41 SM04_report.txt\n```\n\nThe report file (`report.txt`) file has a summary of taxonomic assignments,\nwhereas the main output file (`main.txt`) has one line for each contig with its taxonomic assignment.\n\nWe'll first take a look at the report file, which has the following columns:\n\n1. Percentage of fragments covered by the clade rooted at this taxon\n2. Number of fragments covered by the clade rooted at this taxon\n3. Number of fragments assigned directly to this taxon\n4. A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum,\n   (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. \n5. NCBI taxonomic ID number\n6. Indented scientific name\n\n```bash\nless -S results/kraken/SM04_report.txt\n```\n\n<details><summary>Click to show the contents of the file</summary>\n\n```bash-out\n  2.33\t2\t2\tU\t0\tunclassified\n 97.67\t84\t0\tR\t1\troot\n 97.67\t84\t0\tR1\t131567\t  cellular organisms\n 73.26\t63\t0\tD\t2\t    Bacteria\n 72.09\t62\t3\tP\t1224\t      Pseudomonadota\n 68.60\t59\t2\tC\t1236\t        Gammaproteobacteria\n 58.14\t50\t0\tO\t72274\t          Pseudomonadales\n 58.14\t50\t1\tF\t135621\t            Pseudomonadaceae\n 56.98\t49\t12\tG\t286\t              Pseudomonas\n 43.02\t37\t1\tG1\t136849\t                Pseudomonas syringae group\n 41.86\t36\t0\tG2\t251695\t                  Pseudomonas syringae group genomosp. 1\n 41.86\t36\t36\tS\t317\t                    Pseudomonas syringae\n  8.14\t7\t0\tO\t91347\t          Enterobacterales\n  8.14\t7\t2\tF\t543\t            Enterobacteriaceae\n  5.81\t5\t5\tG\t590\t              Salmonella\n  1.16\t1\t0\tD1\t1783272\t      Terrabacteria group\n  1.16\t1\t0\tP\t201174\t        Actinomycetota\n  1.16\t1\t0\tC\t1760\t          Actinomycetes\n  1.16\t1\t0\tO\t85009\t            Propionibacteriales\n  1.16\t1\t0\tF\t31957\t              Propionibacteriaceae\n  1.16\t1\t0\tG\t1912216\t                Cutibacterium\n  1.16\t1\t1\tS\t1747\t                  Cutibacterium acnes\n 24.42\t21\t0\tD\t2759\t    Eukaryota\n 24.42\t21\t0\tD1\t33154\t      Opisthokonta\n 24.42\t21\t0\tK\t33208\t        Metazoa\n 24.42\t21\t0\tK1\t6072\t          Eumetazoa\n 24.42\t21\t0\tK2\t33213\t            Bilateria\n 24.42\t21\t0\tK3\t33511\t              Deuterostomia\n 24.42\t21\t0\tP\t7711\t                Chordata\n 24.42\t21\t0\tP1\t89593\t                  Craniata\n 24.42\t21\t0\tP2\t7742\t                    Vertebrata\n 24.42\t21\t0\tP3\t7776\t                      Gnathostomata\n 24.42\t21\t0\tP4\t117570\t                        Teleostomi\n 24.42\t21\t0\tP5\t117571\t                          Euteleostomi\n 24.42\t21\t0\tP6\t8287\t                            Sarcopterygii\n 24.42\t21\t0\tP7\t1338369\t                              Dipnotetrapodomorpha\n 24.42\t21\t0\tP8\t32523\t                                Tetrapoda\n 24.42\t21\t0\tP9\t32524\t                                  Amniota\n 24.42\t21\t0\tC\t40674\t                                    Mammalia\n 24.42\t21\t0\tC1\t32525\t                                      Theria\n 24.42\t21\t0\tC2\t9347\t                                        Eutheria\n 24.42\t21\t0\tC3\t1437010\t                                          Boreoeutheria\n 24.42\t21\t0\tC4\t314146\t                                            Euarchontoglires\n 24.42\t21\t0\tO\t9443\t                                              Primates\n 24.42\t21\t0\tO1\t376913\t                                                Haplorrhini\n 24.42\t21\t0\tO2\t314293\t                                                  Simiiformes\n 24.42\t21\t0\tO3\t9526\t                                                    Catarrhini\n 24.42\t21\t0\tO4\t314295\t                                                      Hominoidea\n 24.42\t21\t0\tF\t9604\t                                                        Hominidae\n 24.42\t21\t0\tF1\t207598\t                                                          Homininae\n 24.42\t21\t0\tG\t9605\t                                                            Homo\n 24.42\t21\t21\tS\t9606\t                                                              Homo sapiens\n```\n</details>\n\n::: exercise\n#### {{< fa user-edit >}} **Exercise**: Contamination?\n\nTry to interpret the Kraken report --- are there any contaminant contigs?\n\n<details><summary>Click for the solution</summary>\nOuch! While the majority of our contigs have been classified as _Pseudomonas syringae_,\nwe also have a few other bacteria (including the human skin bacterium _Cutibacterium acnes_),\nand no fewer than 21 human contigs!\n</details>\n\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\nThe 5th column of the Kraken report has the NCBI taxonomic IDs,\nand that of human (on the last line) is `9606`.\n\nThe `main.txt` output file reports the taxonomic ID in the 3rd column,\nso we can use the following command to print just the lines where the 3rd column is `9606`:\n\n```bash\nawk '$3 == 9606' results/kraken/SM04_main.txt\n```\n\n``` bash-out\nC       NODE_28_length_766_cov_1.003130 9606    766     9606:4 131567:5 9606:1 131567:36 9606:11 131567:1 9606:20 131567:47 9606:16 131567:2 9606:5 131567:1 9606:1 131567:1 9606:6 131567:32 9606:68 131567:18 9606:95 131567:146 9606:12 131567:4 9606:11 131567:96 9606:28 131567:5 9606:11 131567:2 9606:29 131567:2 9606:1 131567:5 9606:10\nC       NODE_33_length_621_cov_0.706478 9606    621     9606:180 0:1 9606:3 0:17 9606:1 0:2 9606:12 0:32 9606:199 131567:5 9606:21 131567:2 1:1 9606:111\nC       NODE_36_length_567_cov_0.784091 9606    567     0:6 9606:527\nC       NODE_37_length_563_cov_0.779817 9606    563     9606:271 0:29 9606:229\nC       NODE_39_length_553_cov_0.809859 9606    553     0:1 9606:7 0:3 9606:261 0:3 9606:1 0:15 9606:3 0:5 9606:220\nC       NODE_40_length_552_cov_0.809412 9606    552     9606:41 131567:1 9606:1 131567:17 9606:10 131567:5 9606:56 131567:5 9606:89 131567:6 9606:63 131567:3 9606:84 131567:1 9606:1 131567:17 9606:10 131567:5 9606:103\nC       NODE_42_length_521_cov_0.746193 9606    521     9606:349 0:9 9606:1 0:21 9606:107\nC       NODE_44_length_510_cov_0.906005 9606    510     9606:476\nC       NODE_45_length_497_cov_0.772973 9606    497     9606:218 0:47 9606:1 0:28 9606:1 0:15 9606:1 0:4 9606:140 0:8\nC       NODE_47_length_480_cov_0.866856 9606    480     9606:446\nC       NODE_48_length_479_cov_0.849432 9606    479     9606:445\nC       NODE_50_length_470_cov_1.011662 9606    470     9606:117 0:1 9606:5 0:1 9606:3 0:24 9606:285\nC       NODE_51_length_470_cov_0.915452 9606    470     9606:436\nC       NODE_52_length_466_cov_1.017699 9606    466     9606:20 0:17 9606:6 0:9 9606:135 0:30 9606:215\nC       NODE_54_length_458_cov_1.000000 9606    458     9606:213 0:4 9606:5 0:15 9606:1 0:5 9606:125 0:20 9606:2 0:5 9606:3 0:5 9606:21\nC       NODE_55_length_455_cov_1.179878 9606    455     9606:2 131567:3 9606:218 131567:3 9606:195\nC       NODE_60_length_444_cov_0.933754 9606    444     9606:410\nC       NODE_61_length_442_cov_0.907937 9606    442     9606:21 131567:1 9606:386\nC       NODE_65_length_438_cov_0.405145 9606    438     0:24 9606:2 0:8 9606:80 131567:2 9606:31 131567:19 9606:10 131567:3 9606:13 131567:7 9606:205\nC       NODE_66_length_433_cov_1.133987 9606    433     9606:399\nC       NODE_67_length_432_cov_0.718033 9606    432     9606:398\n```\n\n::: exercise\n#### {{< fa user-edit >}} **Exercise**: Which contigs are contaminants?\n\nIn the output above, the contig IDs are in the second column.\nDo you notice anything about these?\nDoes that provide some independent support for the idea that they are contaminants?\n\n<details><summary>Click for the solution</summary>\nAll of these contigs are small (<600 bp) and have very low coverage (<1.2x, versus\nthe >30x we saw for the contigs IDs that we printed earlier).\n\nNote that Kraken doesn't use this kind of information at all,\nso this provides independent evidence that this is contamination.\n</details>\n\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Removing contaminant contigs\n\nWe will use the Kraken's companion program KrakenTools\n([paper](https://www.nature.com/articles/s41596-022-00738-y),\n[documentation](https://github.com/jenniferlu717/KrakenTools))\nto remove the contaminant contigs, with options:\n\n- `-k` --- main Kraken output file\n- `-s` --- input sequence file to be filtered\n- `o` --- output sequence file\n- `-t` --- NCBI taxonomic ID (`9606` = human)\n- `--exclude` --- exclude (rather than extract) contigs with the specified taxonomic ID\n\n```bash\nmkdir results/decontam\nextract_kraken_reads.py \\\n    -k results/kraken/SM04_main.txt \\\n    -s results/spades/SM04/contigs.fasta \\\n    -o results/decontam/SM04.fasta \\\n    -t 9606 \\\n    --exclude\n```\n```bash-out\nPROGRAM START TIME: 02-05-2024 22:16:21\n        1 taxonomy IDs to parse\n>> STEP 1: PARSING KRAKEN FILE FOR READIDS results/kraken/SM04.main.txt\n        0.00 million reads processed\n        65 read IDs saved\n>> STEP 2: READING SEQUENCE FILES AND WRITING READS\n        65 read IDs found (0.00 mill reads processed)\n        65 reads printed to file\n        Generated file: results/decontam/SM04.fasta\nPROGRAM END TIME: 02-05-2024 22:16:21\n```\n\nLet's check that we indeed have 65 contigs left:\n\n```bash\ngrep -c \">\" results/decontam/SM04.fasta\n```\n```bash-out\n65\n```\n\n<br>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}