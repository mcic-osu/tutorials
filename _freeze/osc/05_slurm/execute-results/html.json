{
  "hash": "0d864548f89f4f796bf9c13cba61c977",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Slurm batch jobs at OSC\"\nauthor: Jelmer Poelstra\ndate: 2024-08-04\nengine: knitr\n---\n\n\n\n\n----------------------------------------------------------------------------------------------------\n\n<br>\n\n### Overview {-}\n\nAutomated scheduling software allows hundreds of people with different\nrequirements to access supercomputer compute nodes effectively and fairly.\nOSC uses **Slurm** (Simple Linux Utility for Resource Management) for this.\n\nAs you've learned,\na reservation of resources on compute nodes is called a **compute job**.\nHere are the main ways to start a compute job at OSC:\n\n1. \"**Interactive Apps**\" --- Run programs with GUIs (e.g. VS Code or RStudio)\n   directly on the OnDemand website.\n2. **Interactive shell jobs** --- Start an interactive shell on a compute node.\n3. **Batch (non-interactive) jobs** --- Run a script on a compute node\n   without ever going to that node yourself.\n\nWe've already worked a lot with the VS Code Interactive App,\nand the self-study material at the bottom of this page will cover interactive\nshell jobs.\nWhat we'll focus on in this session are **batch jobs**.\n\n#### Setting up\n\nLet's get set up by:\n\n- Moving the `garrigos_data` dir one level up, out of `week04`:\n  \n  ```bash\n  # You should be in /fs/ess/PAS2700/users/$USER\n  mv week04/garrigos_data .\n  \n  ls\n  ```\n  ```bash-out\n  CSB  garrigos_data  week02  week03  week04\n  ```\n\n- Creating a dir for this tutorial:\n\n  ```bash\n  mkdir -p week05/class_slurm/scripts\n  cd week05/class_slurm\n  ```\n\n- Copying two scripts which you'll use again:\n  \n  ```bash\n  cp /fs/ess/PAS2700/users/$USER/week04/scripts/printname.sh scripts/\n  cp /fs/ess/PAS2700/users/$USER/week04/scripts/fastqc.sh scripts/\n  ```\n\n<br>\n\n## Basics of Slurm batch jobs\n\nWhen you request a batch job,\nyou ask the Slurm scheduler to **run a script \"out of sight\" on a compute node**.\nWhile that script will run on a compute node,\nyou stay in your current shell at your current node regardless of whether that is\non a login or compute node.\nAfter submitting a batch job,\nit will continue running even if you log off from OSC and shut down your computer.\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### The `sbatch` command\n\nYou can use Slurm's `sbatch` command to submit a batch job.\nBut first, recall that you can directly run a Bash script as follows:\n\n```bash\nbash scripts/printname.sh Jane Doe\n```\n```{.bash-out}\nThis script will print a first and a last name\nFirst name: Jane\nLast name: Doe\n```\n\nThe above command ran the script on our current node.\nTo instead submit the script to the Slurm queue,\nsimply **replace `bash` by `sbatch`**:\n\n```bash\nsbatch scripts/printname.sh Jane Doe\n```\n``` {.bash-out}\nsrun: error: ERROR: Job invalid: Must specify account for job  \nsrun: error: Unable to allocate resources: Unspecified error\n```\n\nHowever, as the above error message \"_Must specify account for job_\" tells us,\nyou need to indicate which OSC Project (or as Slurm puts it, \"account\")\nyou want to use for this compute job.\nUse the `--account=` option to `sbatch` to do this:\n\n```bash\nsbatch --account=PAS2700 scripts/printname.sh Jane Doe\n```\n```{.bash-out}\nSubmitted batch job 12431935\n```\n\nThis output line means your job was successfully submitted \n(no further output will be printed to your screen --- more about that below).\nThe job has a **unique identifier** among all compute jobs by all users\nat OSC, and we can use this number to monitor and manage it.\nEach of us will therefore see a different job number pop up.\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n:::{.callout-note}\n#### `sbatch` options _and_ script arguments\nAs you perhaps noticed in the command above,\nwe can use `sbatch` options _and_ script arguments in one command like so:\n\n```bash\nsbatch [sbatch-options] myscript.sh [script-arguments]\n```\n\nBut, depending on the details of the script itself, all combinations of using\n`sbatch` options and script arguments are possible:\n\n```bash\nsbatch scripts/printname.sh                             # No options/arguments for either\nsbatch scripts/printname.sh Jane Doe                    # Script arguments but no sbatch option\nsbatch --account=PAS2700 scripts/printname.sh           # sbatch option but no script arguments\nsbatch --account=PAS2700 scripts/printname.sh Jane Doe  # Both sbatch option and script arguments\n```\n\n(Omitting the `--account` option is possible when we specify this option\n_inside the script_, as we'll see below.)\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Adding `sbatch` options in scripts\n\nThe `--account=` option is just one of many options you can use when reserving a\ncompute job, but is the only _required_ one. Defaults exist for all other options,\nsuch as the amount of time (1 hour) and the number of cores (1 core).\n\nInstead of specifying `sbatch` options on the command-line when submitting the script,\nyou can also **add these options inside the script**.\nThis is a useful alternative because:\n\n- You'll often want to specify several options, which can lead to very long\n  `sbatch` commands.\n- It allows you to store a script's typical Slurm options as part of the script,\n  so you don't have to remember them.\n\nThese options are added in the script using another type of special comment line\nakin to the shebang (`#!/bin/bash`) line, marked by `#SBATCH`.\nJust like the shebang line, the `#SBATCH` line(s) should be at the top of the script.\nLet's add one such line to the `printname.sh` script, such that the first few lines read:\n\n```bash\n#!/bin/bash\n#SBATCH --account=PAS2700\n\nset -euo pipefail\n```\n\nSo, the equivalent of adding `--account=PAS2700` after `sbatch` on the command line\nis a line in your script that reads `#SBATCH --account=PAS2700`.\n\nAfter adding this to the script,\nyou are now able to run the `sbatch` command without options (which failed earlier):\n\n```bash\nsbatch scripts/printname.sh Jane Doe\n```\n```bash-out\nSubmitted batch job 12431942\n```\n\nAfter submitting a batch job, you **immediately get your prompt back**.\nThe job will run outside of your immediate view,\nand you can continue doing other things in the shell while it does (or log off).\nThis behavior allows you to submit many jobs at the same time,\nbecause you don't have to wait for other jobs to finish, or even to start.\n\n:::{.callout-note}\n## `sbatch` option precedence!\nAny `sbatch` option provided on the command line will override the equivalent\noption provided inside the script.\nThis is sensible because it allows you to provide \"defaults\" inside the script,\nand change one or more of those when needed \"on the go\" on the command line.\n:::\n\n:::{.callout-tip collapse=\"true\"}\n## Running a script with `#SBATCH` lines in non-Slurm contexts _(Click to expand)_\nBecause `#SBATCH` lines are special _comment_ lines,\nthey will simply be ignored (and not throw any errors) when you run a script with\nsuch lines in other contexts:\nfor example, when not running it as a batch job at OSC,\nor even when running it on a computer without Slurm installed.\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Where does the script's output go?\n\nAbove, we saw that when you ran `printname.sh` directly with `bash`,\nits output was printed to the screen, whereas when you submitted it as a batch job,\nonly `Submitted batch job <job-number>` was printed to screen.\nWhere did your output go?\n\nThe output ended up in a file called `slurm-<job-number>.out` (e.g., `slurm-12431942.out`;\nsince each job number is unique to a given job, each file has a different number).\nWe will call this type of file a **Slurm log file**.\n\n<details><summary>Any idea why we may not want batch job output printed to screen, even if it was possible? _(Click for the answer)_</summary>\nThe power of submitting batch jobs is that you can submit many at once ---\ne.g. one per sample, running the same script.\nIf the output from all those scripts ends up on your screen,\nthings become a big mess, and you have no lasting record of what happened.\n</details>\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\nYou should already have two of these Slurm log files if you ran all the above code:\n\n```bash\nls\n```\n```bash-out\nscripts slurm-12431935.out slurm-12431942.out\n```\n\nLet's take a look at the contents of one of these:\n\n```bash\n# (Replace the number in the file name with whatever you got! - check with 'ls')\ncat slurm-12431935.out\n```\n``` {.bash-out}\nThis script will print a first and a last name\nFirst name: Jane  \nLast name: Doe\n```\n\nThis file contains the script's output that was printed to screen \nwhen we ran it with `bash` --- nothing more or less.\n\n#### Two types of output files\n\nIt's important to realize the distinction between two broad types of output\na script may have:\n\n- Output that is **printed to screen** when you directly run a script\n  (`bash myscript.sh`), and that ends up in the **Slurm log file** when you\n  submit the script as a batch job.\n  This includes output produced by `echo` statements,\n  by any errors that may occur,\n  and logging output by any program that we run in the script^[\n  This type of output is referred standard out (non-error output) and standard error\n  --- see the box in the [section on Slurm log files](#output-slurm-log-files)\n  for more].\n\n- Output of commands inside the script that is redirected to a file\n  or that a program writes to an output file. This type of output\n  **will end up in the exact same files** regardless of whether we run the script\n  directly (with `bash`) or as a batch job (with `sbatch`). \n\nOur script above only had the first type of output, but typical scripts have both,\nand we'll see examples of this below.\n\n#### Cleaning up the Slurm logs\n\nWhen using batch jobs, your working dir can easily become a confusing mess of\nanonymous-looking Slurm log files.\nTwo strategies help to prevent this:\n\n- Changing the default Slurm log file name to include a one- or two-word description\n  of the job/script (see below).\n- Cleaning up your Slurm log files, by:\n  - Removing them when no longer needed --- as is e.g. appropriate for our current Slurm log file.\n  - Moving them into a Results dir, which is often appropriate after you've run\n    a bioinformatics tool, since the Slurm log file may contain some info you'd like to keep.\n    For example, we may move any Slurm log files for jobs that ran FastQC to a dir\n    `results/fastqc/logs`.\n\n```bash\n# In this case, we'll simply remove the Slurm log files\nrm slurm*out\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n:::{.callout-tip}\n## The working directory stays the same\nBatch jobs start in the directory that they were submitted from:\nthat is, your working directory remains the same.\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n## Monitoring batch jobs\n\nWhen submitting batch jobs for your research, you'll often have jobs that run for\na while, and/or you'll submit many jobs at once.\nIn addition, longer-running jobs and that ask for many cores sometimes remain queued\nfor a while before they start.\nIt's therefore important to know how you can monitor your batch jobs.\n\n### A sleepy script for practice\n\nWe'll use another short shell script to practice monitoring and managing batch jobs.\nFirst create a new file:\n\n```bash\ntouch scripts/sleep.sh\n```\n\nOpen the file in the VS Code editor and copy the following into it:\n\n```bash\n#!/bin/bash\n#SBATCH --account=PAS2700\n\necho \"I will sleep for 30 seconds\" > sleep.txt\nsleep 30s\necho \"I'm awake! Done with script sleep.sh\"\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n:::exercise\n#### {{< fa user-edit >}} Exercise: Batch job output recap\n\nPredict what would happen if you submit the `sleep.sh` script as a batch job \nusing `sbatch scripts/sleep.sh`:\n\n1. How many output files will this batch job produce?\n2. What will be in each of those files?\n3. In which directory will the file(s) appear?\n4. In terms of output,\n   what would have been different if we had run the script directly,\n   using the command `bash scripts/sleep.sh`?\n\nThen, test your predictions by running the script.\n\n<details><summary>Click for the solutions</summary>\n\n1. The job will produce 2 files:\n    - `slurm-<job-number>.out`: The Slurm log file, containing output normally printed to screen.\n    - `sleep.txt`: Containing output that was redirected to this file in the script.\n\n2. The those files will contain the following:\n    - `slurm-<job-number>.out`: _I'm awake! Done with script sleep.sh_\n    - `sleep.txt`: _\"I will sleep for 30 seconds\"_\n\n3. Both files will end up in your current working directory.\n   Slurm log files always go to the directory from which you submitted the job.\n   Slurm jobs also _run_ from the directory from which you submitted your job,\n   and since we redirected the output simply to `sleep.txt`,\n   that file was created in our working directory. \n\n4.  If we had run the script directly, `sleep.txt` would have also been created\n    with the same content, but \"_All done!_\" would have been printed to screen.\n    \nRun the script and check the outputs:\n\n```bash\nsbatch scripts/sleep.sh\n```\n```bash-out\nSubmitted batch job 27935840\n```\n```bash\ncat sleep.txt\n```\n```bash-out\nI will sleep for 30 seconds\n```\n```bash\ncat slurm-27935840.out\n```\n```bash-out\nI'm awake! Done with script sleep.sh\n```\n\n</details>\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Checking the job's status\n\nAfter you submit a job, it may be initially be waiting to be allocated resources:\ni.e., it may be queued (\"pending\"). Then, the job will start running ---\nyou've seen all of this with the VS Code Interactive App job as well.\n\nWhereas Interactive App jobs will keep running until they've reached the end\nof the allocated time^[Unless you actively \"Delete\" to job on the Ondemand website.],\nbatch jobs will stop as soon as the script has finished.\nAnd if the script is still running when the job runs out of its allocated time,\nit will be killed (stopped) right away.\n\n#### The `squeue` command\n\nYou can check the status of your batch job using the `squeue` Slurm command:\n\n```bash\nsqueue -u $USER -l\n```\n```{.bash-out}\nThu Apr 4 15:47:51 2023\n        JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n     23640814 condo-osu ondemand   jelmer  RUNNING       6:34   2:00:00      1 p0133\n```\n\nIn the command above:\n\n- You specify your username with the **`-u`** option (without this, you'd see _everyone's_ jobs!).\n  In this example, I used the environment variable `$USER` to get your user name,\n  just so that the very same code will work for everyone\n  (you can also simply type your username if that's shorter or easier).\n- The option **`-l`** (lowercase L, not the number 1) will produce more verbose (\"long\") output.\n\nIn the output, after a line with the date and time, and a header line,\nyou should see information about a single compute job, as shown above:\nthis is the Interactive App job that runs VS Code.\nThat's not a \"batch\" job, but it _is_ a compute job, and all compute jobs are listed.\n\nThe following pieces of information about each job are listed:\n\n- `JOBID` --- The job ID number\n- `PARTITION` --- The type of queue\n- `NAME` --- The name of the job\n- `USER` --- The user name of the user who submitted the job\n- `STATE` --- The job's state, usually `PENDING` (queued) or `RUNNING`.\n  Finished jobs do not appear on the list.\n- `TIME` --- For how long the job has been running (here as minutes:seconds)\n- `TIME_LIMIT` --- the amount of time you reserved for the job (here as hours:minutes:seconds)\n- `NODES` --- The number of nodes reserved for the job\n- `NODELIST(REASON)` --- When running: the ID of the node on which it is running.\n   When pending: why it is pending.\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n#### `squeue` example\n\nNow, let's see a batch job in the `squeue` listing.\nStart by submitting the `sleep.sh` script as a batch job:\n\n```bash\nsbatch scripts/sleep.sh\n```\n``` {.bash-out}\nSubmitted batch job 12431945\n```\n\nIf you're quick enough, you may be able to catch the `STATE` as `PENDING` before\nthe job starts:\n\n```bash\nsqueue -u $USER -l\n```\n``` {.bash-out}\nThu Apr 4 15:48:26 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      12520046 serial-40 sleep.sh   jelmer  PENDING       0:00   1:00:00      1 (None)\n      23640814 condo-osu ondemand   jelmer  RUNNING       7:12   2:00:00      1 p0133\n```\n\nBut soon enough it should say `RUNNING` in the `STATE` column:\n\n```sh\nsqueue -u $USER -l\n```\n``` {.bash-out}\nThu Apr 4 15:48:39 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      12520046 condo-osu sleep.sh   jelmer  RUNNING       0:12   1:00:00      1 p0133\n      23640814 condo-osu ondemand   jelmer  RUNNING       8:15   2:00:00      1 p0133\n```\n\nThe script should finish after 30 seconds (because your command was `sleep 30s`),\nafter which the job will immediately disappear from the `squeue` listing,\nbecause only pending and running jobs are shown: \n\n```bash\nsqueue -u $USER -l\n```\n``` {.bash-out}\nMon Aug 21 15:49:26 2023\n         JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n      23640814 condo-osu ondemand   jelmer  RUNNING       9:02   2:00:00      1 p0133\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n#### Checking the output files\n\nWhenever you're running a script as a batch job, even if you've been monitoring it\nwith `squeue`, you should also make sure it ran _successfully_.\nYou typically do so by checking the expected output file(s).\nAs mentioned above, you'll usually have two types of output from a batch job:\n\n- File(s) directly created by the command inside the script (here, `sleep.sh`).\n- A Slurm log file with the script's standard output and standard error\n  (i.e. output that is normally printed to screen).\n\nAnd you saw in the exercise above that this was also the case for the output\nof our sleepy script:\n\n```bash\ncat sleep.txt\n```\n``` {.bash-out}\nI will sleep for 30 seconds\n```\n\n```bash\ncat slurm-12520046.out\n```\n``` {.bash-out}\nI'm awake! Done with script sleep.sh\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\nLet's keep things tidy and remove the sleepy script outputs:\n\n```bash\n# (Replace the number in the file name with whatever you got! - check with 'ls')\nrm slurm*.out sleep.txt\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n::: callout-tip\n#### See output added to the Slurm log file in real time\nText will be added to the Slurm log file in real time as the running script\n(or the program ran by the script) outputs it.\nHowever, the output that commands like `cat` and `less` print are static.\n\nTherefore, if you find yourself opening/printing the contents of the Slurm log\nfile again and again to keep track of progress,\nthen instead **use `tail -f`**, which will \"follow\" the file and _will_ print new\ntext as it's added to the Slurm log file:\n\n```bash\n# See the last lines of the file, with new contents added in real time\ntail -f slurm-12520046.out\n```\n\nTo exit the `tail -f` livestream, press <kbd>Ctrl</kbd>+<kbd>C</kbd>.\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Cancelling jobs\n\nSometimes, you want to cancel one or more jobs,\nbecause you realize you made a mistake in the script,\nor because you used the wrong input files as arguments.\nYou can do so using `scancel`:\n\n```bash\n# [Example - DON'T run this: the second line would cancel your VS Code job]\nscancel 2979968        # Cancel job number 2979968\nscancel -u $USER       # Cancel all your running and queued jobs (careful with this!)\n```\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n:::{.callout-note collapse=\"true\"}\n## Additional job management commands and options _(Click to expand)_\n\n- Use `squeue`'s `-t` option to restrict the type of jobs you want to show.\n  For example, to only show running and not pending jobs:\n  \n  ```bash\n  squeue -u $USER -t RUNNING\n  ```\n\n- You can see more details about any running or finished job,\n  including the amount of time it ran for:\n  \n  ```bash\n  scontrol show job <jobID>\n  ```\n  ```bash-out\n  UserId=jelmer(33227) GroupId=PAS0471(3773) MCS_label=N/A\n  Priority=200005206 Nice=0 Account=pas2700 QOS=pitzer-default\n  JobState=RUNNING Reason=None Dependency=(null)\n  Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n  RunTime=00:02:00 TimeLimit=01:00:00 TimeMin=N/A\n  SubmitTime=2020-12-14T14:32:44 EligibleTime=2020-12-14T14:32:44\n  AccrueTime=2020-12-14T14:32:44\n  StartTime=2020-12-14T14:32:47 EndTime=2020-12-14T15:32:47 Deadline=N/A\n  SuspendTime=None SecsPreSuspend=0 LastSchedEval=2020-12-14T14:32:47\n  Partition=serial-40core AllocNode:Sid=pitzer-login01:57954\n  [...]\n  ```\n  \n- Update directives for a job that has already been submitted\n  (this can only be done _before_ the job has started running):\n\n  ```bash\n  scontrol update job=<jobID> timeLimit=5:00:00\n  ```\n  \n- Hold and release a pending (queued) job,\n  e.g. when needing to update input file before it starts running:\n\n  ```bash\n  scontrol hold <jobID>       # Job won't start running until released\n  scontrol release <jobID>    # Job is free to start\n  ```\n:::\n\n<br>\n\n## Common Slurm options\n\nHere, we'll go through the most commonly used Slurm options.\nAs pointed out above, each of these can either be:\n\n- Passed on the command line: `sbatch --account=PAS2700 myscript.sh` _(has precedence over the next)_\n- Added at the top of the script you're submitting: `#SBATCH --account=PAS2700`.\n\nAlso, note that many Slurm options have a corresponding long\n(`--account=PAS2700`) and short format (`-A PAS2700`).\nFor clarity, we'll stick to long format options here.\n\n### `--account`: The OSC project\n\nAs seen above. When submitting a batch job, always specify the OSC project (\"account\").\n\n### `--time`: Time limit (\"wall time\")\n\nUse the `--time` option to specify the maximum amount of time your job will run for:\n\n- Your job will be killed (stopped) as soon as it hits the specified time limit!\n\n- Compare \"Wall time\" with \"core hours\": if a job runs for 2 hour and used 8 cores,\n  the wall time was 2 hours and the number of core hours was 2 x 8 = 16.\n\n- The default time limit is 1 hour. Acceptable time formats include:\n  - `minutes` (e.g. `60` => 60 minutes)\n  - `hours:minutes:seconds` (e.g. `1:00:00` => 60 minutes)\n  - `days-hours` (e.g. `2-12` => two-and-a-half days)\n\n- For single-node jobs, up to 168 hours (7 days) can be requested.\n  If that's not enough, you can request access to the `longserial` queue\n  for jobs of up to 336 hours (14 days).\n\n- OSC bills you for the time your job actually used, not what you reserved.\n  But jobs asking for more time may be queued longer before they start.\n\nAn example, asking for 2 hours in the \"minute-format\":\n\n```bash\n#!/bin/bash\n#SBATCH --time=120\n```\n\nOr for 12 hours in the \"hour-format\":\n\n```bash\n#!/bin/bash\n#SBATCH --time=12:00:00\n```\n\n:::{.callout-note}\n## When in doubt, reserve more time\nIt is common to be uncertain about how much time your job will take\n(i.e., how long it will take for your script to finish).\nWhenever this happens, ask for more, perhaps much more, time than what you\nthink/guesstimate you will need.\nIt is really annoying to have a job run out of time after several hours,\nwhile the increase in queueing time for jobs asking for more time is often quite\nminimal at OSC.\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n::: exercise\n#### {{< fa user-edit >}} Exercise: exceed the time limit\n\nModify the `sleep.sh` script to reserve only 1 minute for the job while making the\nscript run for longer than that.\n\nIf you succeed in exceeding the time limit, an error message will be printed.\nWhere do you think this error message will be printed: to the screen, in the Slurm log\nfile, or in `sleep.txt`? After waiting for the job to be killed after 60 seconds,\ncheck if you were correct and what the error message is.\n\n<details><summary>Click for the solution</summary>\nThis script would do the trick,\nwhere we request 1 minute of wall-time while we let the script sleep for 80 seconds:\n\n```bash\n#!/bin/bash\n#SBATCH --account=PAS2700\n#SBATCH --time=1\n\necho \"I will sleep for 80 seconds\" > sleep.txt\nsleep 80s\necho \"I'm awake! Done with script sleep.sh\"\n```\n\nSubmit it as usual:\n\n```bash\nsbatch scripts/sleep.sh\n```\n```bash-out\nSubmitted batch job 23641567\n```\n\nThis would result in the following type of error,\nwhich will be printed in the Slurm log file:\n\n```bash-out-solo\nslurmstepd: error: *** JOB 23641567 ON p0133 CANCELLED AT 2024-04-04T14:55:24 DUE TO TIME LIMIT ***\n```\n</details>\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Cores (& nodes and tasks)\n\nThere are several options to specify the number of nodes (â‰ˆ computers), cores, or \"tasks\" (processes). \nThese are separate but related options, and this is where things can get confusing!\nSome background:\n\n- Note that Slurm mostly uses the terms \"core\" and \"CPU\" interchangeably[^3].\n  More generally with bioinformatics tools,\n  \"thread\" is also commonly used interchangeably with core/CPU[^4].\n  Therefore, as also mentioned in the session on OSC, for our purposes,\n  you can **think of core, CPU, and thread as synonyms** that refer to the\n  sub-parts/components of a node that you can reserve and use separately.\n\n[^3]: Even though technically, one CPU often contains multiple cores.\n[^4]: Even though technically, one core often contains multiple threads.\n\n- Running a program with multiple threads/cores/CPUs (\"multi-threading\")\n  is very common, and this can make the running time of such programs much shorter.\n  While the specifics depend on the program, using 8-12 cores is often a sweet spot,\n  whereas asking for even more cores can lead to rapidly **diminishing returns**. \n\n- Running multiple processes (tasks) or needing multiple nodes\n  in a single batch job is not common.\n\nIn practice, my recommendations are to basically always:\n\n- Specify the number of threads/cores/CPUs to Slurm with **`--cpus-per-task=n`**\n  (the short notation is `-c`).\n\n- Keep the number of tasks and nodes to their defaults of 1\n  (in which case the above `-c` option specifies the number of cores, period).\n\n- Tell the program that you're running about the number of available cores ---\n  most bioinformatics tools have an option like `--cores` or `--threads`.\n  **You should set this to the same value `n` as the `--cpus-per-task`.**\n\nAn example, where we ask for 8 CPUs/cores/threads:\n\n```bash\n#!/bin/bash\n#SBATCH --cpus-per-task=8\n\n# And we tell a fictional program about that number of cores:\ncool_program.py --cores 8 sampleA_R1.fastq.gz\n```\n\n:::{.callout-note collapse=\"true\"}\n## Rare cases: multiple nodes or tasks _(Click to expand)_\n\n- You can specify the number of nodes with `--nodes` and the number of tasks\n  with `--ntasks` and/or `--ntasks-per-node`; all have defaults of 1\n  (see the table below).\n  \n- Only ask for **more than one node** when a program is parallelized with\n  e.g. \"MPI\", which is rare in bioinformatics.\n\n- For jobs with multiple processes (tasks),\n  you can use `--ntasks=n` or `--ntasks-per-node=n` --- this is also quite rare!\n  However, note in practice, specifying the number of tasks `n` with one of these\n  options is equivalent to using `--cpus-per-task=n`,\n  in the sense that both ask for `n` cores that can subsequently be used by a program\n  in your script.\n  Therefore, some people use `tasks` as opposed to `cpus` for multi-threading,\n  and you can see this usage in the OSC documentation too.\n  Yes, this is confusing!\n\nHere is an overview of the options related to cores, tasks, and nodes:\n\n| Resource/use                  | short    | long                    | default\n|-------------------------------|----------|-------------------------|:--------:| \n| Nr. of cores/CPUs/threads (per task)    | `-c 1`   | `--cpus-per-task=1`     | 1\n| Nr. of \"tasks\" (processes) | `-n 1`   | `--ntasks=1`            | 1\n| Nr. of tasks per node      | -        | `--ntasks-per-node=1`   | 1\n| Nr. of nodes               | `-N 1`   | `--nodes=1`             | 1\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### `--mem`: RAM memory\n\nUse the `--mem` option to specify the maximum amount of RAM (Random Access Memory)\nthat your job can use:\n\n- Each core on a node has 4 GB of memory \"on it\", and therefore,\n  the default amount of memory you will get is 4 GB is per reserved core.\n  For example, if you specify `--cpus-per-task=4`, you will have 16 GB of memory.\n  And the default number of cores is 1, so the default amount of memory is 4 GB.\n\n- Because it is common to ask for multiple cores and due to the above-mentioned\n  adjustment of the memory based on the number of cores,\n  you will usually end up having enough memory automatically ---\n  therefore, it is common to omit the `--mem` option.\n\n- The default `--mem` unit is MB (MegaBytes); append `G` for GB\n  (i.e. `100` means 100 MB, `10G` means 10 GB).\n\n- Like with the time limit, your job gets killed by Slurm when it hits the memory limit.\n\n- The maximum amount of memory you can request on regular Pitzer compute nodes is 177 GB\n  (and 117 GB on Owens).\n  If you need more than that, you will need one of the specialized `largemem` or\n  `hugemem` nodes --- switching to such a node can happen automatically based on\n  your requested amount, though with caveats:\n  see [this OSC page for details on Pitzer](https://www.osc.edu/resources/technical_support/supercomputers/pitzer/batch_limit_rules),\n  and [this page for details on Owens](https://www.osc.edu/resources/technical_support/supercomputers/owens/batch_limit_rules).\n\nFor example, to request 20 GB of RAM:\n\n```sh\n#!/bin/bash\n#SBATCH --mem=20G\n```\n\n::: {.callout-warning collapse=\"true\"}\n#### It is not always clear what happened when your job ran out of memory _(Click to expand)_\nWhereas you get a very clear Slurm error message when you hit the time limit\n(as seen in the exercise above), hitting the memory limit can result in a variety of errors.\n\nBut look for keywords such as \"Killed\", \"Out of Memory\" / \"OOM\",\nand \"Core Dumped\", as well as actual \"dumped cores\" in your working dir\n(large files with names like `core.<number>`, these can be deleted).\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n::: exercise\n#### {{< fa user-edit >}} Exercise: Adjusting cores and memory\n\nThink about submitting a shell script that runs a bioinformatics tool like FastQC\nas a batch job, in the following two scenarios:\n\n1. The program has an option `--threads`, and you want to set that to 8.\n   The program also says you'll need 25 GB of memory.\n   What `#SBATCH` options related to this will you use?\n\n<details><summary>Click for the solution</summary>\n\nYou should only need the following, since this will give you 8 * 4 = 32 GB of memory.\nThere is no point in \"downgrading\" the amount of memory.\n\n```bash\n#SBATCH --cpus-per-task=8\n```\n\n</details>\n\n2. The program has an option `--cores`, and you want to set that to 12.\n   The program also says you'll need 60 GB of memory.\n   What `#SBATCH` options will you use?\n\n<details><summary>Click for the solution</summary>\n\nHere, it will make sense to ask for `--mem` separately.\n\n```bash\n#SBATCH --cpus-per-task=12\n#SBATCH --mem=60G\n```\n\nAlternatively, you could ask for 15 cores, but then instruct the program to use\nonly 12. Or you could reason that since you'll need 15 cores anyway due to the\namount of memory you'll need, you might as well instruct the program to use all\n15, since this may well speed things up a little more.\n</details>\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### `--output`: Slurm log files\n\nAs we saw above, by default, all output from a script that would normally\nbe printed to screen will end up in a Slurm log file when we submit the script\nas a batch job.\nThis file will be created in the directory from which you submitted the script,\nand will be called `slurm-<job-number>.out`, e.g. `slurm-12431942.out`.\n\nBut it is possible to change the name of this file.\nFor instance, it can be useful to include the **name of the bioinformatics program**\nthat the script runs, so that it's easier to recognize this file later.\nWe can do this with the `--output` option,\ne.g. `--output=slurm-fastqc.out` if we were running FastQC.\n\nBut you'll generally want to keep the batch job number in the file name too[^6].\nSince we won't know the batch job number in advance, we need a trick here ---\nand that is to use **`%j`**, which represents the batch job number:\n\n```bash\n#!/bin/bash\n#SBATCH --output=slurm-fastqc-%j.out\n```\n\n[^6]: For instance, we might be running the FastQC script multiple times,\n      and otherwise those would all have the same name and be overwritten.\n      \n:::{.callout-note collapse=\"true\"}\n## The output streams `stdout` and `stderr`, and separating them _(Click to expand)_\n\nBy default, two output streams from commands and programs called \"standard output\" (`stdout`)\nand \"standard error\" (`stderr`) are printed to screen.\nWithout discussing this in detail, we have seen this several times:\nany regular output by a command is `stdout` and any error messages we've seen were `stderr`.\nBoth of these streams by default also end up in the same Slurm log file,\nbut it is possible to separate them into different files.\n\nBecause `stderr`, as you might have guessed, often contains error messages,\nit could be useful to have those in a separate file.\nYou can make that happen with the `--error` option, e.g. `--error=slurm-fastqc-%j.err`.\n\nHowever, reality is more messy:\nsome programs print their main output not to a file but to standard out,\nand their logging output, errors and regular messages alike, to standard error.\nYet other programs use `stdout` or `stderr` for _all_ messages.\n\n**I therefore usually only specify `--output`,**\n**such that both streams end up in that file.**\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### `--mail-type`: Receive emails\n\nYou can use the `--mail-type` option to have Slurm email you for example when\na job begins, completes or fails.\nYou don't have to specify your email address:\nyou'll be automatically emailed on the email address that is linked to your OSC account. \nI tend to use:\n\n- **`FAIL` for shorter-running jobs** (roughly up to a few hours)\\\n  `FAIL` will email you upon job failure, e.g. when the scripts exits with an error\n  or times out. This is especially useful when submitting many jobs with a loop:\n  this way you know immediately whether any of the jobs failed.\n- **`END` and `FAIL` for longer-running jobs**\\\n  This is helpful because you don't want to have to keep checking in on jobs\n  that run for many hours.\n\nI would avoid having Slurm send you emails upon regular completion for shorter jobs,\nbecause you may get inundated with emails and then quickly start ignoring the emails\naltogether.\n\n```bash\n#!/bin/bash\n#SBATCH --mail-type=END,FAIL\n```\n```bash\n#!/bin/bash\n#SBATCH --mail-type=FAIL\n```\n\n::: {.callout-tip collapse=\"true\"}\n#### Get warned when your job is close to its time limit _(Click to expand)_\nYou may also find the values `TIME_LIMIT_90`, `TIME_LIMIT_80`, and `TIME_LIMIT_50`\nuseful for very long-running jobs,\nwhich will warn you when the job is at 90/80/50% of the time limit.\nFor example, it is possible to email OSC to ask for an extension on individual jobs.\nYou shouldn't do this often, but if you have a job that ran for 6 days and it\nlooks like it may time out, this may well be worth it.\n:::\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n:::{.exercise}\n#### {{< fa user-edit >}} Exercise: Submit your FastQC script as a batch job\n\nIn [this tutorial](/shell/03_scripts.qmd),\nwe created a shell script to run FastQC, and ran it as follows:\n\n```bash\nfastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\nbash scripts/fastqc.sh \"$fastq_file\" results/fastqc\n```\n\n1. Add an `#SBATCH` lines to the script to specify the OSC project `PAS2700`,\n   and submit the modified script as a batch job with the same arguments as above.\n\n<details><summary>Solution (click here)</summary>\n\n- The top of your script should read as follows:\n\n  ```bash\n  #!/bin/bash\n  #SBATCH --account=PAS2700\n  ```\n  \n- Submit the script as follows:\n\n  ```bash\n  fastq_file=../../garrigos_data/fastq/ERR10802863_R1.fastq.gz\n  sbatch scripts/fastqc.sh \"$fastq_file\" results/fastqc\n  ```\n  ```{.bash-out}\n  Submitted batch job 12431988\n  ```\n\n</details>\n\n2. Monitor your job with `squeue`.\n\n3. When it has finished, check the Slurm log file in your working dir\n   and the main FastQC output files in `results/fastqc`.\n\n4. Bonus --- add these `#SBATCH` options, then resubmit:\n   - Let the Slurm log file include 'fastqc' in the file name as well as the job ID number.\n   - Let Slurm email you both when the job completes normally and when it fails.\n     Check that you received the email.\n\n<details><summary>Solution (click here)</summary>\n\n- The top of your script should read as follows:\n\n  ```bash\n  #!/bin/bash\n  #SBATCH --account=PAS2700\n  #SBATCH --output=slurm-fastqc-%j.out\n  #SBATCH --mail-type=END,FAIL\n  ```\n</details>\n:::\n\n<br>\n\n## In closing: making sure your jobs ran successfully\n\nHere are some summarizing notes on the overall strategy to monitor your batch jobs:\n\n- To see whether your job(s) have started,\n  check the queue (with `squeue`) or check for Slurm log files (with `ls`).\n  \n- Once the jobs are no longer listed in the queue,\n  they will have finished: either successfully or because of an error.\n\n- When you've submitted many jobs that run the same script for different samples/files:\n  - Carefully read the full Slurm log file, and check other output files,\n    for at least 1 one of the jobs.\n  - Check whether no jobs have failed: via email when using `--mail-type=END`,\n    or by checking the `tail` of each log for \"Done with script\" messages[^5].\n  - Check that you have the expected number of output files and that no files\n    have size zero (run `ls -lh`).\n\n[^5]: The combination of using strict Bash settings (`set -euo pipefail`) and\n      printing a line that marks the end of the script (`echo \"Done with script\"`)\n      makes it easy to spot scripts that failed,\n      because they won't have that marker line at the end of the Slurm log file.\n\n<br>\n\n## Self-study material\n\n#### Slurm environment variables\n\nInside a shell script that will be submitted as a batch job,\nyou can use a number of Slurm environment variables that will automatically\nbe available, such as:\n\n+------------------------+--------------------------+--------------------------------------------------------+\n| Variable               | Corresponding option     | Description                                            |\n+========================+==========================+========================================================+\n| `$SLURM_JOB_ID`        | *N/A*                    | Job ID assigned by Slurm                               |\n+------------------------+--------------------------+--------------------------------------------------------+\n| `$SLURM_JOB_NAME`      | `--job-name`             | Job name                                               |\n+------------------------+--------------------------+--------------------------------------------------------+\n| `$SLURM_CPUS_PER_TASK` | `-c` / `--cpus-per-task` | Number of CPUs (\\~ cores/threads) available            |\n+------------------------+--------------------------+--------------------------------------------------------+\n| `$SLURM_MEM_PER_NODE`  | `--mem`                  | Amount of memory available (per node)                  |\n+------------------------+--------------------------+--------------------------------------------------------+\n| `$TMPDIR`              | *N/A*                    | Path to the Compute storage available during the job   |\n+------------------------+--------------------------+--------------------------------------------------------+\n| `$SLURM_SUBMIT_DIR`    | *N/A*                    | Path to dir from which job was submitted.              |\n+------------------------+--------------------------+--------------------------------------------------------+\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\nAs an example of how these environment variables can be useful,\nthe command below uses `$SLURM_CPUS_PER_TASK` in its call to the program STAR\ninside the script:\n\n``` {.sh}\nSTAR --runThreadN \"$SLURM_CPUS_PER_TASK\" --genomeDir ...\n```\n\nWith this strategy, you will automatically use the correct (requested) number of\ncores, and don't risk having a mismatch.\nAlso, if you need to change the number of cores, you'll only have to modify it\nin one place: in the resource request to Slurm.\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Interactive shell jobs\n\nInteractive shell jobs will grant you interactive shell access on a compute node.\nWe've been working in a shell in VS Code Server, which means that we already\nhave interactive shell access on a compute node!\n\nHowever, we only have access to 1 core and 4 GB of memory in this VS Code shell,\nand there is no way of changing this.\nIf you want an interactive shell job with more resources,\nyou'll have to start one with Slurm commands.\n\nA couple of different commands can be used to start an interactive shell job.\nI prefer the general `srun` command[^1],\nwhich we can use with `--pty /bin/bash` added to get an interactive Bash shell.\n\n[^1]: Other options: `salloc` works almost identically to `srun`,\n      whereas `sinteractive` is an OSC convenience wrapper but with more limited options.\n     \n```bash\nsrun --account=PAS2700 --pty /bin/bash\n```\n```bash-out\nsrun: job 12431932 queued and waiting for resources  \nsrun: job 12431932 has been allocated resources\n\n[...regular login info, such as quota, not shown...]\n\n[jelmer@p0133 PAS2700]$\n```\n\nThere we go! First some Slurm scheduling info was printed to screen:\ninitially, the job was queued, and then it was \"allocated resources\":\nthat is, computing resources such as a compute node were reserved for the job.\nAfter that:\n\n- The job starts and because we've reserved an _interactive_ shell job,\n  a new Bash shell is initiated:\n  for that reason, we get to see our regular login info once again.\n\n- We have now moved to the **compute node** at which our interactive job is running,\n  so you should have a different `p` number in your prompt.\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\n### Table with `sbatch` options\n\nFirst, here are the options we discussed above:\n\n| Resource/use                  | short      | long                 | default\n|-------------------------|------------|----------------------|:---------:|\n| Project to be billed          | `-A PAS2700` | `--account=PAS2700`    | _N/A_\n| Time limit                    | `-t 4:00:00` | `--time=4:00:00`      | 1:00:00\n| Nr of nodes                   | `-N 1`       | `--nodes=1`            | 1\n| Nr of cores                   | `-c 1`       | `--cpus-per-task=1`    | 1\n| Nr of \"tasks\" (processes)     | `-n 1`      | `--ntasks=1`           | 1\n| Nr of tasks per node          | -          | `--ntasks-per-node`   | 1\n| Memory limit per node         | -          | `--mem=4G`             | *(4G)*\n| Log output file               | `-o`       |  `--output=slurm-fastqc-%j.out`\n| Error output (*stderr*)       | `-e`       | `--error=slurm-fastqc-%j.err`\n| Get email when job starts, ends,<br>fails, or all of the above | -        | `--mail-type=START` <br> `--mail-type=END` <br> `--mail-type=FAIL` <br> `--mail-type=ALL`\n\n<hr style=\"height:1pt; visibility:hidden;\" />\n\nAnd a couple of additional ones:\n\n| Resource/use            | option\n|-----------|------------|\n| Job name (displayed in the queue)    | `--job-name=fastqc`\n| Partition (=queue type)                      | `--partition=longserial` <br> `--partition=hugemem`\n| Let job begin only after a specific time | `--begin=2024-04-05T12:00:00`\n| Let job begin only after another job is done | `--dependency=afterany:123456`\n\n<br>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}